---
title: An Introduction to Bayesian Inference
last_modified_at: 2019-11-30 9:50:00 +0000
categories:
  - study
tags:
  - math
---

So far on this blog, we have looked the mathematics behind distributions, most notably binomial, [Poisson], and [Gamma], with a little bit of exponential. These distributions are interesting in and of themselves, but their true beauty shines through when we analyze them under the light of Bayesian inference. In today's post, we first develop an intuition for conditional probabilities to derive Bayes' theorem. From there, we  motivate the method of Bayesian inference as a means of understanding probability. 

# Conditional Probability

Suppose a man believes he may have been affected with a flu after days of fever and coughing. At the nearest hospital, he is offered to undergo a clinical examination that is known to have an accuracy of 90 percent, *i.e.* it will return positive results to positive cases 90 percent of the time. However, it is also known that the test produces false positives 50 percent of the time. In other words, a healthy, unaffected individual will test positive with a probability of 50 percent. 

<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

In cases like these, [conditional probability] is a great way to package and represent information. Conditional probability refers to a measure of the probability of an event occurring, given that another event has occurred. Mathematically, we can define the conditional probability of event $$A$$ given $$B$$ as follows:

$$P(A \mid B) = \frac{P(A \cap B)}{P(B)}$$

This equation simple states that the conditional probability of $$A$$ given $$B$$ is the fraction of the marginal probability $$P(B)$$ and the area of intersection between those two events, $$P(A \cap B)$$. This is a highly intuitive restatement of the definition of conditional probability introduced above: given that event $$B$$ has already occurred, conditional probability tells us the probability that event $$A$$ occurs, which is then synonymous to that statement that $$A \cap B$$ has occurred. 

By the same token, we can also define the reverse conditional probability of $$B$$ given $$A$$ through symmetry and substitution. Notice that the numerator stays unchanged since the operation of intersection is commutative. 

$$P(B \mid A) = \frac{P(A \cap B)}{P(A)}$$

Now let's develop an intuition for conditional probabilities by applying it to our example of clinical trials and the potentially affected patient. The purported accuracy of the clinical test is 90 percent, which we might express as follows, using the conditional probability notation:

$$P(test + \mid sick) = 0.9$$

By the same token, we can also express the information on false positives as shown below. This conditional probability statement espouses that, given an individual who is not sick, the test returns a false positive 50 percent of the time. 

$$P(test + \ ¬ sick) = 0.5$$

Conditional probability provides us with an interesting way to analyze given information. For instance, let $$R$$ be the event that it rains tomorrow, and $$C$$ be the event that it is cloudy at the present moment. Although we are no experts in climatology and weather forecast, common sense tells us that 

$$P(R \mid C) > P(R)$$

since with the additional piece of information that current weather conditions are cloudy, we are inclined to believe that it will likely rain tomorrow, or in the near future. Like this, conditional probability allows us to update our beliefs on uncertainty given new information, and we will see in the later sections that this is the core idea behind Bayesian inference. 

# Bayes' Theorem

Let's return back to the example of the potential patient with a flu. Shortly afterwards at the hospital, the the man was convinced by the doctor and decided to take the clinical test, the result of which was positive. We cannot assume that the man is sick, however, since the test has a rather high rate of false positives as we saw earlier. In this situation, the parameter that is of interest to us can be expressed as

$$P(sick \mid test +)$$

In other words, given a positive test result, what is the probability that the man is actually sick? However, we have no means as of yet to directly answer this question; the two pieces of information we have are that $$P(test + \mid sick) = 0.9$$, and that $$P(test + \ ¬ sick) = 0.5$$. To calculate the value of $$P(sick \mid test +)$$, we need Bayes's theorem to do its trick. 

Let's quickly derive Bayes' theorem using the definition of conditional probabilities delineated earlier. Recall that

$$P(A \mid B) = \frac{P(A \cap B)}{P(B)} \tag{1}$$

$$P(B \mid A) = \frac{P(A \cap B)}{P(A)} \tag{2}$$

Multiply $$P(B)$$ and $$P(A)$$ on both sides of (1) and (2) respectively to obtain the following result:

$$P(A \mid B) P(B) = P(A \cap B)$$

$$P(B \mid A) P(A) = P(A \cap B)$$

Notice that the two equations describe the same quantity, namely $$P(A \cap B)$$. We can use equivalence to put these two equations together in the following form. 

$$P(A \mid B) P(B) = P(B \mid A) P(A) \tag{3}$$

Equation (3) can be manipulated in the following manner to finally produce a simple form of Bayes' theorem:

$$P(A \mid B) P(B) = P(B \mid A) P(A) \tag{3}$$

Baye's theorem is useful because it allows us to calculate the 





[Poisson]: https://jaketae.github.io/study/poisson/
[Gamma]: https://jaketae.github.io/study/gamma/
[conditional probability]: https://en.wikipedia.org/wiki/Conditional_probability



