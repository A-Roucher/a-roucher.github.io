---
title: An Introduction to Bayesian Inference
last_modified_at: 2019-11-30 9:50:00 +0000
categories:
  - study
tags:
  - math
---

So far on this blog, we have looked the mathematics behind distributions, most notably binomial, [Poisson], and [Gamma], with a little bit of exponential. These distributions are interesting in and of themselves, but their true beauty shines through when we analyze them under the light of Bayesian inference. In today's post, we first develop an intuition for conditional probabilities to derive Bayes' theorem. From there, we  motivate the method of Bayesian inference as a means of understanding probability. 

# Conditional Probability

Suppose a man believes he may have been affected with a flu after days of fever and coughing. At the nearest hospital, he is offered to undergo a clinical examination that is known to have an accuracy of 90 percent, *i.e.* it will return positive results to positive cases 90 percent of the time. However, it is also known that the test produces false positives 50 percent of the time. In other words, a healthy, unaffected individual will test positive with a probability of 50 percent. 

<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

In cases like these, [conditional probability] is a great way to package and represent information. Conditional probability refers to a measure of the probability of an event occurring, given that another event has occurred. Mathematically, we can define the conditional probability of event $$A$$ given $$B$$ as follows:

$$P(A \mid B) = \frac{P(A \cap B)}{P(B)}$$

This equation simple states that the conditional probability of $$A$$ given $$B$$ is the fraction of the marginal probability $$P(B)$$ and the area of intersection between those two events, $$P(A \cap B)$$. This is a highly intuitive restatement of the definition of conditional probability introduced above: given that event $$B$$ has already occurred, conditional probability tells us the probability that event $$A$$ occurs, which is then synonymous to that statement that $$A \cap B$$ has occurred. 

By the same token, we can also define the reverse conditional probability of $$B$$ given $$A$$ through symmetry and substitution. Notice that the numerator stays unchanged since the operation of intersection is commutative. 

$$P(B \mid A) = \frac{P(A \cap B)}{P(A)}$$

Now let's develop an intuition for conditional probabilities by applying it to our example of clinical trials and the potentially affected patient. The purported accuracy of the clinical test is 90 percent, which we might express as follows, using the conditional probability notation:

$$P(\text{test +} \mid \text{sick}) = 0.9$$

By the same token, we can also express the information on false positives as shown below. This conditional probability statement espouses that, given an individual who is not sick, the test returns a false positive 50 percent of the time. 

$$P(\text{test +} \mid \text{¬sick}) = 0.5$$

Conditional probability provides us with an interesting way to analyze given information. For instance, let $$R$$ be the event that it rains tomorrow, and $$C$$ be the event that it is cloudy at the present moment. Although we are no experts in climatology and weather forecast, common sense tells us that 

$$P(R \mid C) > P(R)$$

since with the additional piece of information that current weather conditions are cloudy, we are inclined to believe that it will likely rain tomorrow, or in the near future. Like this, conditional probability allows us to update our beliefs on uncertainty given new information, and we will see in the later sections that this is the core idea behind Bayesian inference. 

# Bayes' Theorem

Let's return back to the example of the potential patient with a flu. Shortly afterwards at the hospital, the the man was convinced by the doctor and decided to take the clinical test, the result of which was positive. We cannot assume that the man is sick, however, since the test has a rather high rate of false positives as we saw earlier. In this situation, the parameter that is of interest to us can be expressed as

$$P(\text{sick} \mid \text{test +})$$

In other words, given a positive test result, what is the probability that the man is actually sick? However, we have no means as of yet to directly answer this question; the two pieces of information we have are that $$P(text{test +} \mid \text{sick}) = 0.9$$, and that $$P(text{test +} \mid \text{¬sick}) = 0.5$$. To calculate the value of $$P(\text{sick} \mid text{test +})$$, we need Bayes's theorem to do its trick. 

Let's quickly derive Bayes' theorem using the definition of conditional probabilities delineated earlier. Recall that

$$P(A \mid B) = \frac{P(A \cap B)}{P(B)} \tag{1}$$

$$P(B \mid A) = \frac{P(A \cap B)}{P(A)} \tag{2}$$

Multiply $$P(B)$$ and $$P(A)$$ on both sides of (1) and (2) respectively to obtain the following result:

$$P(A \mid B) P(B) = P(A \cap B)$$

$$P(B \mid A) P(A) = P(A \cap B)$$

Notice that the two equations describe the same quantity, namely $$P(A \cap B)$$. We can use equivalence to put these two equations together in the following form. 

$$P(A \mid B) P(B) = P(B \mid A) P(A) \tag{3}$$

Equation (3) can be manipulated in the following manner to finally produce a simple form of Bayes' theorem:

$$P(B \mid A) = \frac{P(A \mid B) P(B)}{P(A)} \tag{4}$$

We can motivate a more intricate version this rule by modifying the denominator. Given that $$A$$ and $$B$$ are discrete events, we can break down $$A$$ as a union of intersections between $$A$$ and $$B_i$$, where $$B_i$$ represents subsets within event $$B$$. In concrete form, we can rewrite this as

$$P(A) = \sum_{i = 1}^n P(A \cap B_i)$$

Additionally, we can rewrite the conditional probability $$P(A \cap B_i)$$ in terms of $$P(B_i)$$ and $$P(A \mid B_i)$$ according to the definition of conditional probability we observed earlier. Applying these alterations to (4) to rewrite $$P(A)$$ produces equation (5):

$$P(B_k \mid A) = \frac{P(A \mid B_k) P(B_k)}{\sum_{i = 1}^n P(A \mid B_i) P(B_i)} \tag{5}$$

This is the equation of Bayes' theorem. In simple language, Bayes' theorem tells us that the conditional probability of some subset $$B_k$$ given $$A$$ is equal to its relevant fraction within a weighted summation of the conditional probabilities $$A$$ given $$B_i$$. Although this equation may seem complicated at a glance, we can develop an intuition for this formula by reminding ourselves of the definition of conditional probabilities, as well as the fact that independent events can be expressed as a union of intersections. 

At the end of the day, Bayes' theorem provides a powerful tool through which we can calculate a conditional probability in terms of its reverse, *i.e.* calculate $$P(B \mid A)$$ by utilizing $$P(A \mid B)$$. Why is this important at all? Let's return back to our example of the potential patient. Recall that the conditional probability of our interest was 

$$P(\text{sick} \mid \text{test +})$$

while the pieces of information we were provided were

$$P(\text{test +} \mid \text{sick}) = 0.9, P(\text{test +} \mid \text{¬sick}) = 0.5$$

This is where Bayes' theorem comes in handy. 

$$P(\text{sick} \mid \text{test +})  = \frac{P(\text{test +} \mid \text{sick}) P(\text{sick})}{P(test +)} = \frac{P(\text{test +} \mid \text{sick}) P(\text{sick})}{P(\text{test +} \mid \text{sick}) P(\text{sick}) + P(\text{test +} \mid \text{¬sick}) P(\text{¬sick})}$$

Notice that we have expressed $$P(\text{sick} \mid \text{test +})$$ in terms of $$P(\text{test +} \mid \text{sick})$$ and $$P(\text{test +} \mid \text{¬sick}) P(\text{¬sick})$$. From a statistics point of view, all we have to do now is conduct a random survey of the population to see the percentage of the demographic infected with the flu. Let's say that 15 percent of the population has been affected with this flu. Plugging in the relevant value yields

$$P(\text{sick} \mid \text{test +}) = \frac{0.9 \cdot 0.15}{0.9 \cdot 0.15 + 0.5 \cdot 0.85} \approx 0.241$$

Using Bayes' theorem, we are able to conclude that there is roughly a 24 percent chance that the man who tests positive on this examination is affected by the flu. That seems pretty low given the 90 percent accuracy of the test, doesn't it? This ostensible discrepancy originates from the fact that the test has a substantial false positive of 50 percent, and also that the vast majority of the population is unaffected by the disease. This means that, if the entire population were to conduct this test, there would be more false positives than there would be true positives; hence the distortion in the value of the conditional probability. 

But what if the man were to take the same test again? Intuition tells us that the more test he takes, the more confident we can be on whether the man is or is not affected by the disease. For instance, if the man repeats the exam once and receives a positive report, the conditional probability that he is sick given two consecutive positive test results should be higher than the 24 percent we calculated above. We can see this in practice by reapplying Bayes' theorem with updated information, as shown below:

$$P(\text{sick} \mid \text{test +}) = \frac{P(\text{test +} \mid \text{sick}) P(\text{sick})}{P(\text{test +} \mid \text{sick}) P(\text{sick}) + P(\text{test +} \mid \text{¬sick}) P(\text{¬sick})} = \frac{0.9 \cdot 0.241}{0.9 \cdot 0.241 + 0.5 \cdot 0.759} \approx 0.364$$

We see that the value of the conditional probability has indeed increased, lending credence to the idea that the man is sick. Like this, Bayes' theorem is a powerful tool that can be used to calculate conditional probabilities and to update them continuously through repeated trials, allowing statisticians to generate new insight that may have been otherwise overlooked from available data. Pay close attention to the "updating" part of the story---as we will see in the next section, the continuous applicability of Bayes' theorem is key to understanding Bayesian inference. 

# Bayesian Inference













[Poisson]: https://jaketae.github.io/study/poisson/
[Gamma]: https://jaketae.github.io/study/gamma/
[conditional probability]: https://en.wikipedia.org/wiki/Conditional_probability



