this is a very quick post in which i familiarize myself with basic tensor operations in pytorch while also documenting and clarifying details that initially confused me. as you may realize, some of these points of confusion are rather minute details, while others concern important core operations that are commonly used. this document may grow as i start to use pytorch more extensively for training or model implementation. let's get started. there appear to be two ways of specifying the size of a tensor. using as an example, let's consider the difference between tensor and tensor it confused me how the two yielded identical results. indeed, we can even verify that the two tensors are identical via true i thought different behaviors would be expected if i passed in more dimensions, plus some additional arguments like , but this was not true. tensor tensor the conclusion of this analysis is that the two ways of specifying the size of a tensor are exactly identical. however, one note of caution is that numpy is more opinionated than pytorch and exclusively favors the tuple approach over the unpacked one. array typeerror traceback in 1 np.ones ~/opt/anaconda3/envs/pytorch/lib/python3.7/site packages/numpy/core/numeric.py in ones 190 191 """ 192 a = empty 193 multiarray.copyto 194 return a typeerror: cannot interpret '3' as a data type the conclusion of this analysis is that either approach is fine; it is perhaps a good idea to stick to one convention and stay consistent with that coding style throughout. resizing or reshaping a tensor is an incredibly important tensor operation that is used all the time. the interesting thing is that there seems to be many ways of achieving the same behavior. as someone who prefers a more opinionated guideline, this was rather confusing at first. however, here is what i have gathered while sifting through stack overflow and pytorch discussion forums. let's first start with a dummy random tensor. tensor the operation returns a new tensor whose dimensions match those that have been passed into the function as arguments. for example, the snippet below shows how we can reshape into a tensor. tensor one very important detail, however, is that this operation is not in place. in other words, if we check the size of again, you will realize that it is still a tensor, as was originally initialized. torch.size to change itself, we could do or even better, we can use , which is an in place operation by design. tensor notice that, unlike when we called , changes the tensor itself, in place. torch.size in older versions of pytorch, existed as a non in place operator. however, in newer versions of pytorch, this is no longer the case, and pytorch will complain with an informative deprecation error message. note that is not an in place operator, meaning its behavior will largely be identical to that of . /users/jaketae/opt/anaconda3/envs/pytorch/lib/python3.7/site packages/torch/tensor.py:358: userwarning: non inplace resize is deprecated warnings.warn tensor pytorch keeps an internal convention when it comes to differentiating between in place and copy operations. namely, functions that end with a are in place operators. for example, one can add a number to a tensor in place via , as opposed to the normal , which does not happen in place. tensor observe that the addition is not reflected in , indicating that no operations happened in place. tensor , however, achieves the result without copying and creating a new tensor into memory. tensor is another common function that is used to resize tensors. it has been part of the pytorch api for quite a long time before was introduced. without getting into too much technical detail, we can roughly understand view as being similar to in that it is not an in place operation. however, there are some notable differences. for example, this stack overflow post introduces an interesting example: runtimeerror traceback in 1 z = torch.zeros 2 y = z.t 3 y.view runtimeerror: view size is not compatible with input tensor's size and stride . use .reshape instead. on the other hand, does not run into this error. tensor the difference between the two functions is that, whereas can only be used on contiguous tensors. this so thread gives a nice explanation of what it means for tensors to be contiguous; the bottom line is that, some operations, such , do not create a completely new tensor, but returns a tensor that shares the data with the original tensor while having different index locations for each element. these tensors do not exist contiguously in memory. this is why calling after a transpose operation raises an error. , on the other hand, does not have this contiguity requirement. this felt somewhat overly technical, and i doubt i will personally ever use over , but i thought it is an interesting detail to take note of nonetheless. another point of confusion for me was the fact that there appeared to be two different ways of initializing tensors: and . not only do the two functions look similar, they also practically do the same thing. tensor tensor upon more observation, however, i realized that there were some differences, the most notable of which was the . seemed to be unable to infer the data type from the input given. torch.float32 on the other hand, was sable to infer the data type from the given input, which was a list of integers. torch.int64 sure enough, is generally non configurable, especially when it comes to data types. typeerror traceback in 1 torch.tensor typeerror: new received an invalid combination of arguments got , but expected one of: didn't match because some of the keywords were incorrect: dtype can accept as a valid argument. tensor the conclusion of this analysis is clear: use instead of . indeed, this so post also confirms the fact that should generally be used, as is more of a super class from which other classes inherit. as it is an abstract super class, using it directly does not seem to make much sense. in pytorch, there are two ways of checking the dimension of a tensor: and . note that the former is a function call, whereas the later is a property. despite this difference, they essentially achieve the same functionality. tensor torch.size torch.size to access one of the elements, we need appropriate indexing. in the case of , it suffices to consider the size as a list, meaning that square bracket syntax can be used. 2 in the case of , indices can directly be passed into as an argument to index individual elements in the size tensor. 2 these past few days, i've spent a fair amount of time using pytorch for basic modeling. one of the main takeaways from that experience is that an intuition on dimensionality and tensor operations in general is a huge plus. this gets especially important for things like batching. one very basic thing i learned admittedly perhaps too belatedly is the difference between and as dimensions. here is a concrete example. tensor this creates a one dimensional tensor, which is effectively a list. we can check the dimensions of this tensor by calling , which is very similar to how numpy works. 1 on the other hand, specifying the size as results in a two dimensional tensor. tensor the simple, barely passing answer to the question of why is two dimension would be that it has double layered brackets. more exactly speaking, having an additional layer means that it is capable of storing another tensor within it; hence, is living in a dimension that is one above that of . 2 as mentioned earlier, batch dimension is something that becomes very important later on. some pytorch layers, most notably rnns, even have an argument , which accepts a boolean value. if , pytorch expects the first dimension of the input to be the batch dimension. if , which is the case by default, pytorch assumes that the first dimension would be the sequence length dimension. a common operation that is used when dealing with inputs is , or its inverse, . before explaining what these operations perform, let's just take a look at an example. let's start with , the random tensor of size initialized above. tensor if we apply to , we essentially add a new dimension to the 0 th position of 's shape. tensor as you can see, now there is an additional batch dimension, thus resulting in a tensor whose shape is as opposed to the original . however, of course this operation is not performed in place, meaning that will still remain unchanged. there are in place versions of both and though, and that is simply adding a to the end of the function. for example, tensor equivalently, calling will remove the th dimension of the tensor. by default, is 0. tensor squeezing and unsqueezing can get handy when dealing with single images, or just single inputs in general. concatenation and stacking are very commonly used in deep learning. yet they are also operations that i often had trouble imagining in my head, largely because concatenation can happen along many axes or dimensions. in this section, let's solidify our understanding of what concatenation really achieves with some dummy examples. tensor tensor with a basic example, we can quickly verify that each tensor is a three dimensional tensor whose individual elements are two dimensional tensors of shape . tensor now, let's perform the first concatenation along the 0 th dimension, or the batch dimension. tensor we can verify that the concatenation occurred along the 0 th dimension by checking the shape of the resulting tensor. torch.size since we concatenated two tensors each of shape , we would expect the resulting tensor to have the shape of , which is indeed what we got. more generally speaking, we can think that concatenation effectively brought the two elements of each tensor together to form a larger tensor of four elements. i found concatenation along the first and second dimensions to be more difficult to imagine right away. the trick is to mentally draw a connection between the dimension of concatenation and the location of the opening and closing brackets that we should focus on. in the case of the example above, the opening and closing brackets were the outer most ones. in the example below in which we concatenate along the first dimension, the brackets are those that form the boundary of the inner two dimensional 3 by 4 tensor. let's take a look. tensor notice that the rows of were essentially appended to those of , thus resulting in a tensor whose shape is . torch.size for the sake of completeness, let's also take a look at the very last case, where we concatenate along the last dimension. here, the brackets of focus are the innermost ones that form the individual one dimensional rows of each tensor. therefore, we end up with a "long" tensor whose one dimensional rows have a total of 8 elements as opposed to the original 4. tensor in this post, we took a look at some useful tensor manipulation operations and techniques. although i do have some experience using keras and tensorflow, i never felt confident in my ability to deal with tensors, as that felt more low level. pytorch, on the other hand, provides a nice combination of high level and low level features. tensor operation is definitely more on the low level side, but i like this part of pytorch because it forces me to think more about things like input and the model architecture. i will be posting a series of pytorch notebooks in the coming days. i hope you've enjoyed this post, and stay tuned for more!