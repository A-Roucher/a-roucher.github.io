google is the most popular search engine in the world. it is so popular that the word "google" has been added to the oxford english dictionary as a proper verb, denoting the act of searching on google. while google's success as an internet search engine might be attributed to a plethora of factors, the company's famous pagerank algorithm is undoubtedly a contributing factor behind the stage. the pagerank algorithm is a method by which google ranks different pages on the world wide web, displaying the most relevant and important pages on the top of the search result when a user inputs an entry. simply put, pagerank determines which websites are most likely to contain the information the user is looking for and returns the most optimal search result. while the nuts and bolts of this algorithm may appear complicated and indeed they are the underlying concept is surprisingly intuitive: the relevance or importance of a page is determined by the number of hyperlinks going to and from the website. let's hash out this proposition by creating a miniature version of the internet. in our microcosm, there are only five websites, represented as nodes on a network graph. below is a simple representation created using python and the networkx package. running this block results in the following graph: figure 1: representation of a miniature world wide web how is this a model of the internet? well, as simple as it seems, the network graph contains all the pertinent information necessary for our preliminary analysis: namely, hyperlinks going from one page to another. let's take node d as an example. the pointed edges indicate that page e contains a link to page d, and that page d contains another link that redirects the user to page a. interpreted in this fashion, the graph indicates which pages have a lot of incoming and outgoing reference links. but all this aside, why are hyperlinks important for the pagerank algorithm in the first place? a useful intuition might be that websites with a lot of incoming references are likely to be influential sources, often written by prominent individuals. this analysis is certainly the case in the field of academics, where works of literature that are frequently cited quickly gain clout and attain an established position in the given discipline. another reasoning is that hyperlinks tell us where a user is most likely to wound up in after browsing through returned search results. take the extreme example of an isolated node, where there are zero outgoing and ingoing links to the website. it is unlikely that a user will end up on that webpage, as opposed to a popular site with a spiderweb of edges on a network graph. then, it would make sense for the pagerank algorithm to display that website on top; the isolated node, the bottom. suppose we want to know where a user is most likely to end up in after a given search. this process is often referred to as a random walk because, as the name suggests, it describes a path after a succession of random steps on some mathematical space. while it is highly unlikely that a user visits a website, randomly selects one of the hyperlinks on the given page, and repeats the two steps above repeatedly, the assumption on randomness is what allows us to simulate a user's navigation of the internet from the point of view of markov chains, a stochastic model that describes a sequence of possible events, or states, in which the probability of each event is contingent only upon the previous state attained in the previous event. one good example of a markov chain is the famous chutes and ladders game, in which the player's next position is dependent only upon their present position on the game board. for this reason, markov chains are said to be memoryless: in the chutes and ladders game, whether the player ended up in their current position by taking a ladder or a normal dice roll is irrelevant to the progress of the game. figure 2: chutes and ladders game a salient characteristic of a markov chain is that the probabilities of each event can be represented and calculated by simple matrix multiplication. the specifics of this mechanism will be a topic for another post, but intuitively speaking, there would be some stochastic matrix that represents probabilities, and some vector that denotes the th state in the markov chain. then, multiplying this state vector by the stochastic matrix would yield , where the new vector denotes the probability distribution in the th state in the markov chain. the beauty behind the markov chain is that the result of this multiplication operation, when iterated many times, converges to a stationary distribution vector regardless of where we started from, i.e. . to make all of this more concrete, let's return back to our example of the internet microcosm and the five websites. in order to apply a stochastic analysis on our model, it is first necessary to translate the network graph presented above into a stochastic matrix whose individual entries are nonnegative real numbers that denote some probability of change from one state to another. here is the matrix representation of the network graph in our example: the column vector can be defined as where denotes the probability that the user is browsing website x at state . for instance, would be an appropriate vector representation of a state distribution in a markov chain where a user began their random walk at website a. this will be our example. from these, how can we learn more more about ? would give us the answer: notice that the result is just the first column of the stochastic matrix, with all entries 0 except for the second one! why is this the case? besides the algebraic argument that matrix multiplications can be performed on a column by entry basis, the network graph contains the most intuitive answer to our question: there is only one link from page a to page b, which is why takes the absolute probability of 1. simply put, the user clicks on the one and only link on page a to move to page b, as the highlighted path shows. figure 3: network graph with path highlight once the user reaches page b, however, they now have two choices instead of one: either go back to page a or visit page c. this increase in uncertainty is reflected in the entries of the next vector, : as our intuition suggests, , and since . although no formal proof has been presented, it is now fairly clear that performing this calculation times would yield the vector that contains information about the probability of the user being at websites a, b, c, d, and e, respectively. let's quickly calculate the values of this state vector over some loops. the output of this program is . as the markov process rightly predicts, these numbers each converge to certain values with more repetitions. also notice that this result is independent of the initial vector we started out with! suppose a new initial vector xppppp\lim\limits_{n \to \infty} x_n 29\%39\%22\%2\%7\%p is the smallest out of the five entries, which aligns with the fact that node d held the most insular position in the network graph. all in all, analyzed from the dimension of time, the notion of stationary distribution is coherent with our intuition that, no matter where the user starts, the average time they spend on each website should be the same given the memoryless nature of the markov chain. so we commenced from the seemingly simple question of what pagerank entails. the markov chain madness may have appeared a bit like a rabbit hole, but it is highly germane to the clockwork behind google's search algorithm. although we used only one parameter hyperlinks as the basis of our analysis, in reality pagerank performs batch calculations on a much larger sum of data to ultimately derive the equivalent of our stationary distribution vector. the website that the user is most likely to spend the most time on, i.e. the website that is most likely important and relevant to the user's search entry, is placed on the top of the list. other entries follow in sorted order. so there you have it: the pagerank algorithm demystified. now the question is, will google place this post on the top of the search result when a user types "pagerank"? probably not given the lack of active hyperlinks to and from this webpage. but we'll see. added to the oxford english dictionary: https://www.theatlantic.com/technology/archive/2014/06/the first use of the verb to google on television buffy the vampire slayer/373599/ the pagerank algorithm: https://en.wikipedia.org/wiki/pagerank networkx package: https://networkx.github.io random walk: https://en.wikipedia.org/wiki/random_walk memoryless: https://en.wikipedia.org/wiki/markov_property