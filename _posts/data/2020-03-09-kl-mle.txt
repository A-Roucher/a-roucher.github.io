these days, i've been spending some time trying to read published research papers on neural networks to gain a more solid understanding of the math behind deep learning. this is a rewarding yet also a very challenging endeavor, mostly because i have not studied enough math to really understand all of what is going on. while reading the groundbreaking research paper wasserstein gan by martin arjovsky, i came across this phrase: ... asymptotically, maximum likelihood estimation amounts to minimizing the kullback leibler divergence... i was particularly interested in the last portion of this sentence, that mle amounts to minimizing kl divergence. we discussed mle multiple time on this blog, including this introductory post and a related post on map. neither is kl divergence an entirely unfamiliar topic. however, i had not thought about these two concepts together in one setting. in this post, let's try to hash out what the quote from the paper means. let's start with a very quick review of what mle and kl divergence each are. after all, it's been a while since i've written the linked posts, and for a fruitful, substantive discussion on this topic, it's necessary to make sure that we have a solid grasp of what mle and kl divergence are. mle is a technique used to find the optimal parameter of a distribution that best describes a set of data. to cut to the chase, this statement can be expressed as follows: from here, we can start making assumptions, such as that observations in are i.i.d, which is the assumption that we make to build models such as na√Øve bayes, and so on. for now, it suffices to clarify that the goal of maximum likelihood estimation is to find the optimal parameter of a distribution that best captures some given data. kl divergence is a concept that arises from the field of information theory that is also heavily applied in statistics and machine learning. kl divergence is particularly useful because it can be used to measure the dissimilarity between to probability distributions. the familiar equation for kl divergence goes as follows: in bayesian terms, kl divergence might be used to compare the prior and the posterior distribution, where represents the posterior and , the prior. in machine learning, is often the true distribution which we seek to model, and is the approximation of that true distribution, which is also the prediction generated by the model. note that kl divergence is not a true measure of distance, since it is asymmetric. in other words, the focus of this post is obviously not on distance metrics, and i plan on writing a separate post devoted to this topic. but as a preview of what is to come, here is an appetizer to get you interested. an alternative to kl divergence that satisfies the condition of symmetry is the jensen shannon divergence, which is defined as follows: where one can intuit jsd as being a measurement that somewhat averages the two asymmetric quantities of kl divergence. we will revisit jsd in the future when we discuss the mathematics behind gans. but for now, it suffices to know what kl divergence is and what it measures. now that we have reviewed the essential concepts that we need, let's get down to the proof. let's start with the statement of the parameter that minimizes the kl divergence between the two distribution and the approximate distribution : not a lot has happened in this step, except for substituting the expression with its definition as per . observe that in the last derived expression in , the term does not affect the argument of the minima, which is why it can safely be omitted to yield the following simplified expression: we can change the argument of the minima operator to the maxima given the negative sign in the expression for the expected value. to proceed further, it is necessary to resort to the law of large numbers, or lln for short. the law states that the average of samples obtained from a large number of repeated trials should be close to the expected value of that random variable. in other words, the average will approximate the expected value as more trials are performed. more formally, lln might be stated in the following fashion. suppose we perform an experiment involving the random variable and repeat it times. then, we would obtain a set of indecent and identically distributed samples as shown below: then, lln states that a more precise statement of the law uses chebyshev's inequality: for the curious, here is the general formulation of chebyshev's inequality outside the context of lln: for the purpose of this post, it is not necessary to go into how chebyshev's inequality is derived or what it means. however, it isn't difficult to see how one might reformulate to derive to prove the law of large numbers. all that the inequality is saying is that no more than a certain fraction of samples can fall outside more than a certain distance away from the mean of the distribution. with this understanding in mind, let's return to the original problem and wrap up the proof. let's apply the law of large numbers to modify the expected value expression sitting in : voila! we have shown that minimizing the kl divergence amounts to finding the maximum likelihood estimate of . this was not the shortest of journeys, but it is interesting to see how the two concepts are related. indeed, it sort of makes intuitive sense to think that minimizing the distance between the true and approximated distribution is best done through maximum likelihood estimation, which is a technique used to find the parameter of the distribution that best describes given data. i personally find little derivations and proofs like these to be quite interesting, which is why i plan on doing more posts on the mathematics of deep learning and its related concepts in the future. thanks for reading, and catch you up in the next one.