in this post, we'll take a look at rnns, or recurrent neural networks, and attempt to implement parts of it in scratch through pytorch. yes, it's not entirely from scratch in the sense that we're still relying on pytorch autograd to compute gradients and implement backprop, but i still think there are valuable insights we can glean from this implementation as well. for a brief introductory overview of rnns, i recommend that you check out this previous post, where we explored not only what rnns are and how they work, but also how one can go about implementing an rnn model using keras. this time, we will be using pytorch, but take a more hands on approach to build a simple rnn from scratch. full disclaimer that this post was largely adapted from this pytorch tutorial this pytorch tutorial. i modified and changed some of the steps involved in preprocessing and training. i still recommend that you check it out as a supplementary material. with that in mind, let's get started. the task is to build a simple classification model that can correctly determine the nationality of a person given their name. put more simply, we want to be able to tell where a particular name is from. we will be using some labeled data from the pytorch tutorial. we can download it simply by typing this command will download and unzip the files into the current directory, under the folder name of . now that we have downloaded the data we need, let's take a look at the data in more detail. first, here are the dependencies we will need. we first specify a directory, then try to print out all the labels there are. we can then construct a dictionary that maps a language to a numerical label. we see that there are a total of 18 languages. i wrapped each label as a tensor so that we can use them directly during training. {'czech': tensor, 'german': tensor, 'arabic': tensor, 'japanese': tensor, 'chinese': tensor, 'vietnamese': tensor, 'russian': tensor, 'french': tensor, 'irish': tensor, 'english': tensor, 'spanish': tensor, 'greek': tensor, 'italian': tensor, 'portuguese': tensor, 'scottish': tensor, 'dutch': tensor, 'korean': tensor, 'polish': tensor} let's store the number of languages in some variable so that we can use it later in our model declaration, specifically when we specify the size of the final output layer. now, let's preprocess the names. we first want to use to standardize all names and remove any acute symbols or the likes. for example, 'slusarski' once we have a decoded string, we then need to convert it to a tensor so that the model can process it. this can first be done by constructing a mapping, as shown below. 59 we see that there are a total of 59 tokens in our character vocabulary. this includes spaces and punctuations, such as . this also means that each name will now be expressed as a tensor of size ; in other words, each character will be a tensor of size . we can now build a function that accomplishes this task, as shown below: if you read the code carefully, you'll realize that the output tensor is of size , which is different from the explanation above. well, the reason for that extra dimension is that we are using a batch size of 1 in this case. in pytorch, rnn layers expect the input tensor to be of size . since every name is going to have a different length, we don't batch the inputs for simplicity purposes and simply use each input as a single batch. for a more detailed discussion, check out this forum discussion. let's quickly verify the output of the function with a dummy input. tensor now we need to build a our dataset with all the preprocessing steps. let's collect all the decoded and converted tensors in a list, with accompanying labels. the labels can be obtained easily from the file name, for example . we could wrap this in a pytorch class, but for simplicity sake let's just use a good old loop to feed this data into our model. since we are dealing with normal lists, we can easily use 's to separate the training data from the testing data. let's see how many training and testing data we have. note that we used a of 0.1. train: 18063 test: 2007 we will be building two models: a simple rnn, which is going to be built from scratch, and a gru based model using pytorch's layers. now we can build our model. this is a very simple rnn that takes a single character tensor representation as input and produces some prediction and a hidden state, which can be used in the next iteration. notice that it is just some fully connected layers with a sigmoid non linearity applied during the hidden state computation. we call at the start of every new batch. for easier training and learning, i decided to use to initialize these hidden states. we can now build our model and start training it. i realized that training this model is very unstable, and as you can see the loss jumps up and down quite a bit. nonetheless, i didn't want to cook my 13 inch macbook pro so i decided to stop at two epochs. epoch 1/2, step 3000/18063, loss: 0.0390 epoch 1/2, step 6000/18063, loss: 1.0368 epoch 1/2, step 9000/18063, loss: 0.6718 epoch 1/2, step 12000/18063, loss: 0.0003 epoch 1/2, step 15000/18063, loss: 1.0658 epoch 1/2, step 18000/18063, loss: 1.0021 epoch 2/2, step 3000/18063, loss: 0.0021 epoch 2/2, step 6000/18063, loss: 0.0131 epoch 2/2, step 9000/18063, loss: 0.3842 epoch 2/2, step 12000/18063, loss: 0.0002 epoch 2/2, step 15000/18063, loss: 2.5420 epoch 2/2, step 18000/18063, loss: 0.0172 now we can test our model. we could look at other metrics, but accuracy is by far the simplest, so let's go with that. accuracy: 72.2471% the model records a 72 percent accuracy rate. this is very bad, but given how simple the models is and the fact that we only trained the model for two epochs, we can lay back and indulge in momentary happiness knowing that the simple rnn model was at least able to learn something. let's see how well our model does with some concrete examples. below is a function that accepts a string as input and outputs a decoded prediction. i don't know if any of these names were actually in the training or testing set; these are just some random names i came up with that i thought would be pretty reasonable. and voila, the results are promising. 'english' 'chinese' 'russian' the model seems to have classified all the names into correct categories! this is cool and all, and i could probably stop here, but i wanted to see how this custom model fares in comparison to, say, a model using pytorch layers. gru is probably not fair game for our simple rnn, but let's see how well it does. let's declare the model and an optimizer to go with it. notice that we are using a two layer gru, which is already one more than our current rnn implementation. epoch 1/2, step 3000/18063, loss: 1.8497 epoch 1/2, step 6000/18063, loss: 0.4908 epoch 1/2, step 9000/18063, loss: 1.0299 epoch 1/2, step 12000/18063, loss: 0.0855 epoch 1/2, step 15000/18063, loss: 0.0053 epoch 1/2, step 18000/18063, loss: 2.6417 epoch 2/2, step 3000/18063, loss: 0.0004 epoch 2/2, step 6000/18063, loss: 0.0008 epoch 2/2, step 9000/18063, loss: 0.1446 epoch 2/2, step 12000/18063, loss: 0.2125 epoch 2/2, step 15000/18063, loss: 3.7883 epoch 2/2, step 18000/18063, loss: 0.4862 the training appeared somewhat more stable at first, but we do see a weird jump near the end of the second epoch. this is partially because i didn't use gradient clipping for this gru model, and we might see better results with clipping applied. let's see the accuracy of this model. accuracy: 81.4150% and we get an accuracy of around 80 percent for this model. this is better than our simple rnn model, which is somewhat expected given that it had one additional layer and was using a more complicated rnn cell model. let's see how this model predicts given some raw name string. 'english' 'chinese' 'spanish' 'russian' the last one is interesting, because it is the name of a close turkish friend of mine. the model obviously isn't able to tell us that the name is turkish since it didn't see any data points that were labeled as turkish, but it tells us what nationality the name might fall under among the 18 labels it has been trained on. it's obviously wrong, but perhaps not too far off in some regards; at least it didn't say japanese, for instance. it's also not entirely fair game for the model since there are many names that might be described as multi national: perhaps there is a russian person with the name of demirkan. i learned quite a bit about rnns by implementing this rnn. it is admittedly simple, and it is somewhat different from the pytorch layer based approach in that it requires us to loop through each character manually, but the low level nature of it forced me to think more about tensor dimensions and the purpose of having a division between the hidden state and output. it was also a healthy reminder of how rnns can be difficult to train. in the coming posts, we will be looking at sequence to sequence models, or seq2seq for short. ever since i heard about seq2seq, i was fascinated by tthe power of transforming one form of data to another. although these models cannot be realistically trained on a cpu given the constraints of my local machine, i think implementing them themselves will be an exciting challenge. catch you up in the next one!