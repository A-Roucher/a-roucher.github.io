in today's post, we will take yet another look at an interesting application of a neural network: autoencoders. there are many types of autoencoders, but the one we will be looking at today is the simplest variant, the vanilla autoencoder. despite its simplicity, however, there is a lot of insight to glean from this example in fact, it is precisely the simplicity that allows us to better understand how autoencoders work, and potentially extend that understanding to to analyze other flavors of autoencoders, such as variational autoencoder networks which we might see in a future post. without further ado, let's get started. we begin by importing all modules and configurations necessary for this tutorial. how do autoencoders work? there are entire books dedicated to this topic, and this post in no way claims to introduce and explore all the fascinating complexities of this model. however, one intuitive way to understand autoencoders is to consider them as, lo and behold, encoders that map complex data points into vectors living in some latent dimension. for example, a 28 by 28 pixel rgb channel image might be compressed into a five dimensional latent vector. the five numbers composing this vector somehow encodes the core information needed to then decode this vector back into the original 28 by 28 pixel rgb channel image. of course, some information is inevitably going to be lost after all, how can five numbers describe the entirety of an image? however, what's important and fascinating about autoencoders is that, with appropriate training and configuration, they manage to find ways to best compress input data into latent vectors that can be decoded to regenerate a close approximation of the input data. for the purposes of this demonstration, let's configure the latent dimension of the encoder to be 128 dimensions in other words, each 28 by 28, single channel image will be encoded into vectors living in 128 dimensional space. it's time to build the autoencoder model. in summary, an autoencoder is composed of two components: an encoder and a decoder. the encoder transfers input data into the latent dimension, and the decoder performs the exact reverse: it takes vectors in the latent space and rearranges it to bring it back into its original dimension, which is, in this case, a 28 by 28, single channel image. the followign code snippet implements this logic using the functional api. let's declare the encoder and autoencoder model by invoking the function with the specified image shape and the dimensionality of the latent space. just to get a sense of what operations are taking place dimensionality wise, here is a look at the output shapes of the autoencoder model. notice that the input is of shape , and that the final output is also of the same shape , as expected. model: "model" _________________________________________________________________ ================================================================= input_1 0 _________________________________________________________________ conv2d 160 _________________________________________________________________ batch_normalization 64 _________________________________________________________________ max_pooling2d 0 _________________________________________________________________ conv2d_1 4640 _________________________________________________________________ batch_normalization_1 128 _________________________________________________________________ max_pooling2d_1 0 _________________________________________________________________ conv2d_2 9248 _________________________________________________________________ flatten 0 _________________________________________________________________ dense 200832 _________________________________________________________________ dense_1 202272 _________________________________________________________________ reshape 0 _________________________________________________________________ conv2d_transpose 9248 _________________________________________________________________ batch_normalization_2 128 _________________________________________________________________ up_sampling2d 0 _________________________________________________________________ conv2d_transpose_1 9248 _________________________________________________________________ batch_normalization_3 128 _________________________________________________________________ up_sampling2d_1 0 _________________________________________________________________ conv2d_transpose_2 4624 _________________________________________________________________ conv2d_3 145 ================================================================= total params: 440,865 trainable params: 440,641 non trainable params: 224 _________________________________________________________________ here's the image of the model for the fancy bells and whistles. now that the autoencoder model is fully ready, it's time to see what it can do! although autoencoders present countless exciting possibilities for application, we will look at a relatively simple use of an autoencoder in this post: denoising. there might be times when the photos we take or image data we use are tarnished by noise undesired dots or lines that undermine image quality. an autoencoder can be trained to remove these noises fairly easily as we will see in thi post. first, let's import the mnist data set for this tutorial. nothing much exciting is happening below, except for the fact that we are rearranging and preprocessing the dataset so as to maximize training efficiency. next, we will add noise to the data. note that the mnist dataset does not contain noise by default: we will have to artificially and intentionally tarnish the dataset to produce a noisy training set for the autoencoder model. the function precisely performs this function. using the function, we can create a noisy sample. note that was set to 0.5, although i'd imagine other values within reasonable range would work equally well as well. training the model is very simple: the training data is , the noisy dataset, and the predicted label is . through this configuration, we essentially expect the autoencoder to be able to see noisy images, after which encoding and decoding is performed via a transformation to a latent dimension to ultimately reproduce a pristine image devoid of any noise. for experimental puposes, i tried using the callback on google colab. is a platform that gives developers full view of what happens during and after the training process. it makes observing metrics like loss and accuracy a breeze. i highly recommend that you check out this tutorial on how to use and configure this functionality on your notebook. train on 54000 samples, validate on 6000 samples epoch 1/35 54000/54000 ============================== 9s 170us/sample loss: 0.1358 val_loss: 0.1091 epoch 2/35 54000/54000 ============================== 6s 117us/sample loss: 0.1046 val_loss: 0.1041 epoch 3/35 54000/54000 ============================== 6s 118us/sample loss: 0.1004 val_loss: 0.1001 epoch 4/35 54000/54000 ============================== 6s 118us/sample loss: 0.0982 val_loss: 0.1001 epoch 5/35 54000/54000 ============================== 6s 116us/sample loss: 0.0966 val_loss: 0.0995 epoch 6/35 54000/54000 ============================== 6s 117us/sample loss: 0.0956 val_loss: 0.0991 epoch 7/35 54000/54000 ============================== 6s 117us/sample loss: 0.0946 val_loss: 0.0969 epoch 8/35 54000/54000 ============================== 6s 117us/sample loss: 0.0939 val_loss: 0.0971 epoch 9/35 54000/54000 ============================== 6s 117us/sample loss: 0.0932 val_loss: 0.0966 epoch 10/35 54000/54000 ============================== 6s 117us/sample loss: 0.0928 val_loss: 0.0959 epoch 11/35 54000/54000 ============================== 6s 116us/sample loss: 0.0922 val_loss: 0.0966 epoch 12/35 54000/54000 ============================== 6s 117us/sample loss: 0.0917 val_loss: 0.0958 epoch 13/35 54000/54000 ============================== 6s 116us/sample loss: 0.0914 val_loss: 0.0958 epoch 14/35 54000/54000 ============================== 6s 116us/sample loss: 0.0910 val_loss: 0.0970 epoch 15/35 54000/54000 ============================== 6s 116us/sample loss: 0.0907 val_loss: 0.0961 epoch 16/35 54000/54000 ============================== 6s 116us/sample loss: 0.0903 val_loss: 0.0983 epoch 17/35 54000/54000 ============================== 6s 118us/sample loss: 0.0900 val_loss: 0.0987 epoch 18/35 54000/54000 ============================== 7s 121us/sample loss: 0.0898 val_loss: 0.0963 epoch 19/35 54000/54000 ============================== 6s 116us/sample loss: 0.0895 val_loss: 0.0953 epoch 20/35 54000/54000 ============================== 6s 116us/sample loss: 0.0893 val_loss: 0.0959 epoch 21/35 54000/54000 ============================== 6s 117us/sample loss: 0.0890 val_loss: 0.0954 epoch 22/35 54000/54000 ============================== 6s 116us/sample loss: 0.0888 val_loss: 0.0953 epoch 23/35 54000/54000 ============================== 6s 116us/sample loss: 0.0887 val_loss: 0.0954 epoch 24/35 54000/54000 ============================== 6s 117us/sample loss: 0.0885 val_loss: 0.0958 epoch 25/35 54000/54000 ============================== 6s 117us/sample loss: 0.0882 val_loss: 0.0958 epoch 26/35 54000/54000 ============================== 6s 116us/sample loss: 0.0880 val_loss: 0.0966 epoch 27/35 54000/54000 ============================== 6s 117us/sample loss: 0.0879 val_loss: 0.0956 epoch 28/35 54000/54000 ============================== 6s 116us/sample loss: 0.0877 val_loss: 0.0956 epoch 29/35 54000/54000 ============================== 6s 116us/sample loss: 0.0876 val_loss: 0.0954 epoch 30/35 54000/54000 ============================== 6s 117us/sample loss: 0.0874 val_loss: 0.0959 epoch 31/35 54000/54000 ============================== 6s 118us/sample loss: 0.0873 val_loss: 0.0959 epoch 32/35 54000/54000 ============================== 6s 116us/sample loss: 0.0872 val_loss: 0.0960 epoch 33/35 54000/54000 ============================== 6s 116us/sample loss: 0.0871 val_loss: 0.0958 epoch 34/35 54000/54000 ============================== 6s 117us/sample loss: 0.0869 val_loss: 0.0980 epoch 35/35 54000/54000 ============================== 6s 116us/sample loss: 0.0867 val_loss: 0.0981 now that the training is over, what can we do with this autoencoder? well, let's see if the autoencoder is now capable of removing noise from tainted image files. but before we jump right into that, let's first build a simple function that displays images for our convenience. using the function, we can now display 25 test images that we will feed into the autoencoder. let's add noise to the data. finally, the time has come! the autoencoder will try to "denoise" the contaminated images. let's see if it does a good job. lo and behold, the autoencoder produces pristine images, almost reverting them back to their original state! i find autoencoders interesting for two reasons. first, they can be used to compress images into lower dimensions. our original image was of size 28 by 28, summing up to a total of 784 pixels. somehow, the autoencoder finds ways to decompress this image into vectors living in the predefined 128 dimensions. this is interesting in and of itself, since it presents ways that we might be able to compress large files with minimal loss of information. but more importantly, as we have seen in this tutorial, autoencoders can be used to perform certain tasks, such as removing noise from data, and many more. in the next post, we will take a look at a variant of this vanilla autoencoder model, known as variational autoencoders. variataional autoencoders are a lot more powerful and fascinating because they can actually be used to generate data instead of merely processing them. i hope you enjoyed reading this post. stay tuned for more!