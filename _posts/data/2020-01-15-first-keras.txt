lately, i have been on a datacamp spree after unlocking a two month free unlimited trial through microsoft's visual studio dev essentials program. if you haven't already, make sure to check it out, as it offers a plethora of tools, journal subscriptions, and software packages for developers. anyhow, one of the courses i decided to check out on datacamp was titled "introduction to deep learning with python," which covered basic concepts in deep learning such as forward and backward propagation. the latter half of the tutorial was devoted to the introduction of the keras api and the implementation of neural networks. i created this notebook immediately after finishing the tutorial for memory retention and self review purposes. first, we begin by importing the library as well as other affiliated functions in the module. note that keras uses tensorflow as backend by default. the warning in the code block below appears because this notebook was written on google colab, which informs users that the platform will be switching over to tensorflow 2 in the future. as you might be able to guess from one of the imported modules, the objective of the neural network will be to classify hand written digits. in doing so, we will be dealing with a classic in machine learning literature known as the the mnist data set, which contains images of hand written digits from 0 to 9, each hand labeled by researchers. the variable denotes the total number of class labels available in the classification task, which is 10. specifies the number of iterations the gradient descent algorithm will run for. let's begin by loading the data from . downloading data from https://s3.amazonaws.com/img datasets/mnist.npz 11493376/11490434 ============================== 1s 0us/step now we have to slightly modify the loaded data so that its dimensions and values are made suitable to be fed into a neural network. changing the dimensionality of data can be achieved through the function, which takes in the number of rows and columns as its argument. we convert the numbes into type , then normalize it so that its values are all between 0 and 1. although we won't get into too much detail as to why normalization is important, an elementary intuition we might develop is that normalization effectively squishes all values into the same bound, making data much more processable. we also implement one hot encoding on through the function. let's quickly check if the necessary adjustments were made by looking up the dimensions of and , respectively. looks like the data has been reshaped successfully. now, it's finally time to get into the nuts and bolts of a neural network. the simplest neural network is the model, which means that every neuron in one layer is connected to all other neurons in the previous layer. building a simple neural network is extremely easy in a high level api like keras. the model below has 784 input nodes. the input layer is then connected to a hidden layer with 512 neurons, which is then connected to a second hidden layer with also 512 neurons. note that the hidden layer uses the function as its activation function. the dropout layers ensure that our model does not overfit to the data. the last output layer has 10 neurons, each corresponding to digits from 0 to 9. the activation fuction of this last layer is the softmax function, which allows us to interpret the final results as a categorical distribution. let's double check that the layers have been formed correctly as per our intended design. model: "sequential_3" _________________________________________________________________ ================================================================= dense_6 401920 _________________________________________________________________ dropout_5 0 _________________________________________________________________ dense_7 262656 _________________________________________________________________ dropout_6 0 _________________________________________________________________ dense_8 5130 ================================================================= total params: 669,706 trainable params: 669,706 non trainable params: 0 _________________________________________________________________ everything looks good, which means we are now ready to compile and train our model. before we do that, however, it is always a good idea to use the module to ensure that gradient descent stops when no substantial weight adjustments are being made to our model. in other words, when the model successfully finds the local minimum , the will kick in and stop gradient descent from proceeding with further epochs. we are now ready to go! let's compile the model by making some configurations, namely the , , and . simply put, an specifies which flavor of the gradient descent algorithm we want to choose. the simplest version is known as , or the stochastic gradient descent. can be considered an improved version of the stochastic gradient descent in that its learning rate changes depending on the slope of the loss function, defined here as cross entropy. if you recall, cross entropy is basically a measurement of the pseudo distance between two distributions, i.e. how different two distributions are. but because cross entropy is often not easy to intuitively wrap our minds around, let's pass the metric to the function, as shown below. it's time to train the neural network with the training data, and , over a specified number of epochs. as promised, we will use the to stop graident descent from making unnecessary computations down the road. we also specify that and are components of the validation set. train on 60000 samples, validate on 10000 samples epoch 1/15 60000/60000 ============================== 8s 129us/step loss: 0.2149 acc: 0.9349 val_loss: 0.0992 val_acc: 0.9688 epoch 2/15 60000/60000 ============================== 7s 124us/step loss: 0.1048 acc: 0.9676 val_loss: 0.0815 val_acc: 0.9750 epoch 3/15 60000/60000 ============================== 7s 122us/step loss: 0.0820 acc: 0.9743 val_loss: 0.0907 val_acc: 0.9726 epoch 4/15 60000/60000 ============================== 7s 123us/step loss: 0.0669 acc: 0.9794 val_loss: 0.0795 val_acc: 0.9782 epoch 5/15 60000/60000 ============================== 7s 122us/step loss: 0.0576 acc: 0.9823 val_loss: 0.0883 val_acc: 0.9767 epoch 6/15 60000/60000 ============================== 7s 123us/step loss: 0.0498 acc: 0.9843 val_loss: 0.0704 val_acc: 0.9799 epoch 7/15 60000/60000 ============================== 7s 123us/step loss: 0.0470 acc: 0.9851 val_loss: 0.0752 val_acc: 0.9814 epoch 8/15 60000/60000 ============================== 7s 122us/step loss: 0.0425 acc: 0.9867 val_loss: 0.0857 val_acc: 0.9800 keras shows us how much our neural network improves over each epoch. this is convenient, but can we do better? the answer is a sure yes. let's quickly plot a graph to see how model accuracy improves over time, while cross entropy loss decreases with more epochs. as the last step, we might want to save our trained model. this can be achieved with a single line of code. we can load pre saved models as well. that's it for today! obviously there are a lot more we can do with , such as building deeper neural networks or non sequential models such as cnn or gan, but these are topics we might look at a later date when i grow more proficient with the keras api and deep learning in general. for now, consider this to be a gentle introduction to neural networks with keras. thanks for reading! catch you up in the next one. datacamp: http://datacamp.com visual studio dev essentials program: https://visualstudio.microsoft.com/dev essentials/ gradient descent algorithm: https://en.wikipedia.org/wiki/gradient_descent cross entropy: https://jaketae.github.io/study/information entropy/ keras api: https://keras.io google colab: http://colab.research.google.com