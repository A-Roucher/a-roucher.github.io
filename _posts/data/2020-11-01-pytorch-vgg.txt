in today's post, we will be taking a quick look at the vgg model and how to implement one using pytorch. this is going to be a short post since the vgg architecture itself isn't too complicated: it's just a heavily stacked cnn. nonetheless, i thought it would be an interesting challenge. full disclosure that i wrote the code after having gone through aladdin persson's wonderful tutorial video. he also has a host of other pytorch related vidoes that i found really helpful and informative. having said that, let's jump right in. we first import the necessary modules. let's first take a look at what the vgg architecture looks like. shown below is a table from the vgg paper. we see that there are a number of different configurations. these configurations typically go by the name of vgg 11, vgg 13, vgg 16, and vgg 19, where the suffix numbers come from the number of layers. each value of the dictionary below encodes the architecture information for each model. the integer elements represents the out channel of each layer. represents a max pool layer. you will quickly see that the dictionary is just a simple representation of the tabular information above. now it's time to build the class that, given some architecture encoding as shown above, can produce a pytorch model. the basic idea behind this is that we can make use of iteration to loop through each element of the model architecture in list encoding and stack convolutional layers to form a sub unit of the network. whenever we encounter , we would append a max pool layer to that stack. this is probably the longest code block i've written on this blog, but as you can see, the meat of the code lies in two methods, and . these methods are where all the fun stacking and appending described above takes place. i actually added a little bit of customization to make this model a little more broadly applicable. first, i added batch normalization, which wasn't in the original paper. batch normalization is known to stabilize training and improve performance; it wasn't in the original vgg paper because the batch norm technique hadn't been introduced back when the paper was published. also, the model above can actually handle rectangular images, not just square ones. of course, there still is a constraint, which is that the and parameters must be multiples of 32. valueerror traceback in 3 in_height=200, 4 in_width=150, 5 architecture=vgg_types"vgg16" 6 ) in __init__ 16 self.num_classes = num_classes 17 self.convs = self.init_convs 18 self.fcs = self.init_fcs 19 20 def forward: in init_fcs 29 if + != 0: 30 raise valueerror 33 out_height = self.in_height // factor valueerror: and must be multiples of 32 let's roll out the model architecture by taking a look at vgg19, which is the deepest architecture within the vgg family. if we print the model, we can see the deep structure of convolutions, batch norms, and max pool layers. vgg: sequential: conv2d, stride=, padding=) : batchnorm2d : relu : conv2d, stride=, padding=) : batchnorm2d : relu : maxpool2d, stride=, padding=0, dilation=1, ceil_mode=false) : conv2d, stride=, padding=) : batchnorm2d : relu : conv2d, stride=, padding=) : batchnorm2d : relu : maxpool2d, stride=, padding=0, dilation=1, ceil_mode=false) : conv2d, stride=, padding=) : batchnorm2d : relu : conv2d, stride=, padding=) : batchnorm2d : relu : conv2d, stride=, padding=) : batchnorm2d : relu : conv2d, stride=, padding=) : batchnorm2d : relu : maxpool2d, stride=, padding=0, dilation=1, ceil_mode=false) : conv2d, stride=, padding=) : batchnorm2d : relu : conv2d, stride=, padding=) : batchnorm2d : relu : conv2d, stride=, padding=) : batchnorm2d : relu : conv2d, stride=, padding=) : batchnorm2d : relu : maxpool2d, stride=, padding=0, dilation=1, ceil_mode=false) : conv2d, stride=, padding=) : batchnorm2d : relu : conv2d, stride=, padding=) : batchnorm2d : relu : conv2d, stride=, padding=) : batchnorm2d : relu : conv2d, stride=, padding=) : batchnorm2d : relu : maxpool2d, stride=, padding=0, dilation=1, ceil_mode=false) ) : sequential: linear : relu : dropout : linear : relu : dropout : linear ) ) we can clearly see the two submodules of the network: the convolutional portion and the fully connected portion. now let's see if all the dimensions and tensor sizes match up. this quick sanity check can be done by passing in a dummy input. this input represents a 3 channel 224 by 224 image. passing in this dummy input and checking its shape, we can verify that forward propagation works as intended. torch.size and indeed, we get a batched output of size , which is expected given that the input was a batch containing two images. just for the fun of it, let's define and see if it is capable of processing rectangular images. again, we can pass in a dummy input. this time, each image is of size . and we see that the model is able to correctly output what would be a probability distribution after a softmax. torch.size