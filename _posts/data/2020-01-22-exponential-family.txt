normal, binomial, exponential, gamma, beta, poisson... these are just some of the many probability distributions that show up on just about any statistics textbook. until now, i knew that there existed some connections between these distributions, such as the fact that a binomial distribution simulates multiple bernoulli trials, or that the continuous random variable equivalent of the geometric distribution is the exponential. however, reading about the concept of the exponential family of distributions has lent me new insight, and i wish to share that renewed understanding on probability distributions through this post. in this section, we will take a look at what the exponential family of distributions is all about. we will begin by laying out a few mathematical definitions, then proceed to see examples of probability distributions that belong to the exponential family. to cut to the chase, the exponential family simply denotes a group of probability distributions that satisfy a certain condition, namely that they can be factorized and parametrized into a specific form, as show below: here, is a log noramlizing constant that ensures that the probability distribution integrates to 1. there are other alternative forms that express the same factorization. one such variant that i prefer and find more intuitive uses a simple fractional approach for normalization instead of adding complications to the exponential term. for notational convenience, i will follow the fractional normalization approach shown below throughout this post. before we proceed any further, it is probably a good idea to clarify the setup of the equations above. first, denotes a dimensional random variable of interest; , a dimensional parameter that defines the probability distribution. is known as the sufficient statistic function. below is a brief summary concerning the mappings of these different functions. you will notice that i used and instead of and as shown in equation . this is because assumes vectorization of these functions as follows. we could have expressed without vectorization, but doing so would be rather verbose. so we instead adhere to the vectorized convention in throughout this post. as i hinted earlier, the exponential family covers a wide range of probability distributions, most pdfs and pmfs. in fact, most probability distributions that force themselves onto the page of statistics textbooks belong to this powerful family. below is a non comprehensive list of distributions that belong to the exponential family. probability density functions exponential gaussian beta gamma chi squared probability mass functions bernoulli binomial poisson geometric multinomial of course, there are examples of common distributions that do not fall under this category, such as the uniform distribution or the student distribution. this point notwithstanding, the sheer coverage of the exponential family makes it worthy of exploration and analysis. also, notion of an exponential family itself is significant in that it allows us to frame problems in meaningful ways, such as through the notion of conjugate priors: if you haven't noticed, the distributions outlined above all have conjugate priors that also belong to the exponential family. in this sense, the exponential family is particularly of paramount importance in the field of bayesian inference, as we have seen many times in previous posts. let's concretize our understanding of the exponential family by applying factorization to actual probability distributions. the easiest example, as you might have guessed, is the exponential distribution. recall that the formula for the exponential distribution is where the indicator function, denoted as , takes the following form: the indicator function is a simple modification applied to ensure that the function is well defined across the entire real number domain. normally, we omit the indicator function since it is self apparent, but for the sake of robustness in our analysis, i have added it here. how can we coerce equation to look more like , the archetypal form that defines the exponential family? well, now it's just a matter of drag and match: by paying close attention to the variables, parameters, and the output of each function, we can reconstruct to take the form of . the easeist starting point is to observe the exponent to identify and , after which the rest of the surrounding functions can be inferred. the end result is presented below: after substituting each function with their prescribed value in , it isn't difficult to see that the exponential distribution can indeed by factorized according to the form outlined in . although this is by no means a rigorous proof, we see not only the evident fact that the exponential distribution indeed belongs to the exponential family, but also that the factorization formula in isn't just a complete soup of equations and variables. we can do the same for the bernoulli distribution, which also falls under the exponential family. the formula for the bernoulli distribution goes as follows: again, i have added a very simple indicator function to ensure that the the probability mass function is well defined across the entire real number line. again, the indicator function is a simple boolean gate function that checks whether is an element within a set of zero and one: factorizing the bernoulli is slightly more difficult than doing the same for the exponential distribution, largely because it is not apparent from how factorization can be achieved. for example, we do not see any exponential term embedded in as we did in the case of the exponential distributions. therefore, a simple one to one correspondence cannot be identified. the trick to get around this problem is to introduce a log transformation, then reapplying an exponential. in other words, by applying this manipulation, we can artificially create an exponential term to more easily coerce into the factorization mold. specifically, observe that the power of the exponent can be expressed as a dot product between two vectors, each parameterized by and , respectively. this was the hard part: now, all that is left is to configure the rest of the functions to complete the factorization. one possible answer is presented below: by now, it should be sufficienty clear that the definition of the exponential family is robust enough to encompass at least the two probability distributions: the exponential and the bernoulli. although we do not go over other examples in this article, the exponential family is a well defined set of probability distributions that, at thei core, are defined by a common structure. and as we will see in the next section, this underlying similarity makes certain calculations surprisingly convenient. in a previous post, we explorerd the notion of maximum likelihood estimation, and contrasted it with maximum a posteriori estimation. the fundamental question that maximum likelihood estimation seems to answer is: given some data, what parameter of a distribution best explains that observation? this is an interesting question that merits exploration in and of itself, but the discussion becomes a lot more interesting and pertinent in the context of the exponential family. before diving into mle, let's define what is known as the canonical form of the exponential family. despite its grandiose nomenclature, the canonical form simply refers to a specific flavor of factorization scheme where in which case simplifies to we will assume some arbitrary distribution in the exponential family following this canonical form to perform maxmimum likelihood estimation. much like in the previous post on maximum likelihood estimation, we begin with some data set of independent and identically distributed observations. this is going to be the setup of the mle problem. given this dataset, the objective of maximum likelihood estimation is to identify some parameter that maximizes the likelihood, i.e. the probability of observing these data points under a probability distribution defined by . in other words, how do we identify this parameter? well, the go to equipment in a mathematician's arsenal for an optimization problem like this one is calculus. recall that our goal is to maximize the likelihood function, which can be calculated as follows: the first equality stands due to the assumption that all data are independent and identically distributed. maximizing is a complicated task, especially because we are dealing with a large product. products aren't bad, but we typically prefer sums because they are easier to work with. a simple hack that we almost always use when dealing with maximum likelihood, therefore, is to apply a log transformation to calculate the log likelihood, since the logarithm is a monotonically increasing function. in other words, what does the log likelihood look like? well, all we have to do is to apply a log function to , which yields the following result. maximizing the log liklihood can be achieved by setting the gradient to zero, as the gods of calculus would tell us. as you might recall from a previous post on some very basic matrix calculus, the gradient is simply a way of packaging derivatives in a multivariate context, typically involving vectors. if any of this sounds unfamilar, i highly recommend that you check out the linked post. we can compute the partial derivative of the log likelihood function with respect to as shown below. observe that the last term in is eliminated because it is a constant with respect to . this is a good starting point, but we still have no idea how to derive the log of . to go about this problem, we have to derive an expression for . recall from the definition of the exponential family that is a normalizing constant that exists to ensure that the probability function integrates to one. in other words, this necessarily implies that now that we have an expression for to work with, let's try to compute the derivative term we left unsolved in . the first and second equalities stand due to the chain rule, and the third equality is a simple algebraic manipulation that recreates the probability function within the integral, allowing us to ultimately express the partial derivative as an expected value of for the random variable . this is a surprising result, and a convenient one indeed, because we can now use this observation to conclude that the gradient of the log likelihood function is simply the expected value of the sufficient statistic. therefore, starting again from , we can continue our calculation of the gradient and set the quantity equal to zero to calculate the mle estimate of the parameter. it then follows that how do we interpret the final result in equation ? it looks nice, simple, and concise, but what does it mean to say that the expected value of the sufficient statistic is the average of the sufficient statistic for each observed individual data points? to remove abstractness, let's employ a simple example, the exponential distribution, and attempt to derive a clearer understanding of the final picture. recall that the probability density function of the exponential distribution takes the following form according to the factorizations outlined below: computing the derivative of the log of the normalizing term as we did in , because we know that the resulting quantity is the expected value of the sufficient statistic, we know that and indeed, this is true: the expected value of the random variable characterized by an exponential distribution is simply the inverse of the parameter defining that distribution. note that the parameter for the exponential distribution is most often denoted as , in which case the expected value of the distribution would simply be written as . this is all great, but there is still an unanswered question lingering in the air: what is the mle estimate of the parameter ? this moment is precisely when equation comes in handy. recall that therefore, finally, we have arrived at our destination: we finally know how to calculate the parameter under which the likelihood of observing given data is maximized. the beauty of this approach is that it applies to all probability distributions that belong to the exponential family because our analysis does not depend on which distribution is in question; we started from the canonical form of the exponential family to derive a set of generic equations. this is the convenience of dealing with the exponential family: because they are all defined by the same underlying structure, the mle equations hold general applicability. in this post, we explored the exponential family of distributions, which i flippantly ascribed the title "the medici of probability distributions." this is obviously my poor attempt at an intellectual joke, to which many of you might cringe, but i personally think it somewhat captures the idea that many probability distributions that we see on the textbook are, in fact, surprisingly more related than we might think. at least to me, it wasn't obvious from the beginning that the exponential and the bernoulli distributions shared the same structure, not to mention the wealth of other distributions that belong to the exponential family. also, the convenient factorization is what allowed us to perform an mle estimation, which is an important concept in statistics with wide ranging applications. this post in no way claims to give a full, detailed view of the exponential family, but hopefully it gave you some understanding of what it is and why it is useful. in the next post, we will take a look at maximum a posteriori estimation and how it relates to the concept of convex combinations. stay tuned for more.