neural networks are powerful models that can be used to identify complex hidden patterns in data. there are many types of neural networks, two of which we have seen already on this blog: the vanilla, feed forward neural network and convolutional neural networks, often abbreviated as convnets. today, we will add a third kind to this exciting mix: recurrent neural networks, or rnns. let's take a brief conceptual look at how recurrent neural networks work, then implement a toy rnn to see how it compares to other models on the imdb movie reviews dataset. i heavily borrowed my examples from deep learning with python by fran√ßois chollet and the tutorial on text classification available from the official tensorflow documentation. recurrent neural networks, as the name implies, refer to neural network models that contain some sort of internal looping structure that simulates a flow of information. a good way to conceptualize this loop is to think of something like loops, where a certain operation is performed repeatedly for a specified number of cycles. given these pieces of information, we might ask ourselves two questions. firstly, what does this looping operation involve? secondly, what is the purpose of having this loop in the first place? let's try to answer both questions in the following subsections. one of the most salient features of a recurrent neural network is that it is capable of emulating some primitive form of memory. why might we want to do this? well, take the example of reading a text. when we read, we don't process a given text at once in its totality; instead, we break them down into pieces, such as a word or a bag of words, and build our understanding based on information obtained from the previous sequence of text. in other words, processing information through reading is at best understood as a process of continuously receiving new information while retaining information obtained from the previous sequence. this is why recurrent neural network models are frequently employed in the context of natural language processing. but the applications of rnns extends beyond the domain of nlp. for example, say we are given a dataset of temperature recording of a city district. obviously, the structural integrity of that dataset is very important, i.e. we should not shuffle the datasets because making weather predictions requires us to understand temporal patterns. in predicting the weather 24 hours from today, data points pertaining to the last two days will be obviously much more important that those recorded a month ago. in such instances of time series analysis, recurrent neural networks perform better than other models we have looked at thus far. to better understand how rnns work, let's try to build a very simple recurrent neural network from scratch with . we will only implement forward propagation for the sake of simplicity, but with some matrix calculus, one can imagine how deriving the formula for back propagation will be possible. let's cut to the chase: rnns emulate memory by using the output from the previous sequence as an input to the next. perhaps writing this down in matrix notation might give you a better idea of what the statement above means. here is one way we might implement a very simple recurrent neural network. if the word "recursion" pops up into your mind, then you are on the right track. notice that in calculating , the output of the current sequence, the output of the previous sequence, is used. by using the output of the previous sequence, the recurrent neural network is able to "remember" some of the information that was produced by the previous input. granted, this is not exactly what memory is or how it works in the strictest sense, but we can see how some information from the previous input is trickling down to affect the computation of the current sequence of input data. note that i used for the example, but we can just about use any other activation function. here is one way we might implement this in python. although not necessary, i decided to opt for a class based implementation to make things look tight and nicer. let's create a class object to see that everything works properly. 6400 there is really not much point in seeing the output because the calculations are going to be based off of randomly generated data and randomly created weights, but perhaps there is something to be learned from the dimensionality of input and output data. first, note that our model accepts input of size . the dimensionality of the output is a little more tricky because of the option. what concatenate does is that it basically flattens all number of outputs into a single list. in this case, because we set the option to , we get a flattened list containing , or 6400 elements. the main takeaway is that recurrent neural networks can be used to implement some sort of memory functionality, which is useful when dealing with datasets where there exists some sort of sequential structure. one way to implement memory is by using the output of the previous sequence to define a variable, which is used to compute the next output as we have done above. now let's get down to business with the api. implementing a recurrent neural network is not so much different from building a simple feed forward or convolutional neural network: we simply import a rnn specific layer and arrange these layers to construct a working model. before we proceed any further, let's first import all necessary dependencies for this tutorial. as with any tutorial, we need to start by loading and preprocessing data. luckily, there isn't going to be much preprocessing involved since we will be using a dataset available from the library, the imbd movie reviews dataset. the dataset contains 25,000 movie reviews from imdb, labeled as either positive or negative. each review is encoded as a sequence of integers according to a consistent encoding scheme. in other words, each integer corresponds to a unique word in the vocabulary of the dataset. more specifically, the integer to which a word is mapped corresponds to the frequency with which the word appears, i.e. the word encoded as 10 corresponds to the 10th most frequent word in the data. we will apply some very basic preprocessing on the dataset so that we can feed it into our model. specifically, we will preprocess the dataset such that only a number of most frequently occurring words are considered. this step will weed out words that occur very infrequently, thus decreasing the amount of noise from the network's perspective. next, we will apply padding to the dataset so that all reviews are of length . this means that longer reviews will be truncated, whereas shorter reviews will be padded with zero entries. below is a sample code implementation of this process. let's load the data using the function after specifying necessary parameters. now that the data is here and ready to go, it's time to build our neural network. to spice things up a bit, let's create four different models and compare their performance. before we jump into that, however, we first need to understand what an embedding layer is, as it is key to natural language processing. simply put, an embedding layer is a layer that transforms words or integers that encode words into dense vectors. this is a necessary transformation since neural networks are incapable of dealing with non quantitative variables. why dense vectors, then? can't we simply use one hot encoding? that is a valid point, since one hot encoding is how we mostly deal with categorical variables in a dataset. however, one downside of this approach is that we end up with many sparse vectors. in other words, a lot of resources are wasted because the model now has to process vectors of thousands or millions of dimensions, depending on the vocabulary size. instead of using sparse vectors to represent each word, we can simply use a denser vector of smaller dimensions to encode our data. another advantage of this approach is that dense vectors can be used to encode semantic information. you might have heard of the famous example that "king minus male equals queen minus female." if we were to represent the words king, queen, male, and female as vectors, we can add and subtract vectors to represent and distill meaningful information. this vector based computation is the key to natural language processing with deep neural networks: by back propagating and adjusting the weights of our embedding layer, our model can eventually be trained to "understand" the meaning of words and their relationship with other words in the form of dense vectors. enough talking, let's use the embedding layer to build our neural networks, starting with the simple feed forward model. the feed forward neural network model will first have an embedding layer that processes input. then, the output of this embedding layer will be flattened to be passed onto a dense layer with one output transformed by the sigmoid activation function. we use the sigmoid function since we want to conduct a sentiment analysis of determining whether a given movie review is positive or negative. that wasn't so difficult. let's initialize our model by defining the model parameters , , and , then plot the model to see the structure of the network alongside the input and output dimensions of each layer. note that we defined to be 16, which means that each word is transformed into dense vectors living in 16 dimensions. by plotting the model, we can get a better idea of the layers that compose the model. the next model we will build is a simple recurrent neural network. this neural network is going to have an embedding layer, just like the previous model. however, instead of a dense layer, it will have two consecutive layers stacked on top of each other. the layer is essentially the implementation of the model we built earlier. let's take a look. we instantiate the model and take plot the network, just as we have done above. the model we built is, as the name shamelessly puts out, pretty simple. there are a lot more advanced recurrent neural networks that have complicated internal cell structures to better emulate human memory, in a sense. the biggest difference between a simple recurrent neural network and an lstm is that lstms have a unique parameter known as the carrier that encodes an additional layer of information about the state of the cell. i might write a separate post devoted to the intricacies of the lstm, but if you're an avid reader who's itching to know more about it right away, i highly recommend this excellent post by christiopher olah. for now, let's just say that lstms represent a huge improvement over conventional rnns, and that we can implement them in by simply calling the layer as shown below: because lstm layers take a lot longer to train than others, and because the representational capacity of a single lstm layer is higher than that of others, i decided to use only one lstm layer instead of two. let's initialize this model to take a better look. the last model we will create is a convnet, which we explored on this previous post on image classification. convolutional neural networks are great at identifying spatial patterns in data, which is why they also perform reasonably well in natural language processing. another huge advantage of convents over recurrent networks is that they took a lot lesser time and resources to train. this is why it is often a good idea to build a convent to establish a baseline performance metric. let's initialize the model with identical parameters and take a look at its internal structure. let's train all four models using the training data. for control our experiment, we will train all four models over the same , , and . there isn't much exciting here to look at it terms of code; it's just a matter of patience, waiting for the models to hopefully converge to a global minimum. for future reference, all training history is dumped in the object where corresponds to the model number. train on 20000 samples, validate on 5000 samples epoch 1/10 20000/20000 ============================== 2s 119us/sample loss: 0.2706 acc: 0.8922 val_loss: 0.3372 val_acc: 0.8562 epoch 2/10 20000/20000 ============================== 2s 121us/sample loss: 0.2434 acc: 0.9073 val_loss: 0.3483 val_acc: 0.8476 epoch 3/10 20000/20000 ============================== 2s 118us/sample loss: 0.2173 acc: 0.9212 val_loss: 0.3585 val_acc: 0.8454 epoch 4/10 20000/20000 ============================== 2s 116us/sample loss: 0.1920 acc: 0.9364 val_loss: 0.3904 val_acc: 0.8320 epoch 5/10 20000/20000 ============================== 2s 112us/sample loss: 0.1689 acc: 0.9477 val_loss: 0.3901 val_acc: 0.8308 epoch 6/10 20000/20000 ============================== 2s 109us/sample loss: 0.1472 acc: 0.9589 val_loss: 0.4091 val_acc: 0.8268 epoch 7/10 20000/20000 ============================== 2s 115us/sample loss: 0.1272 acc: 0.9687 val_loss: 0.4306 val_acc: 0.8214 epoch 8/10 20000/20000 ============================== 2s 114us/sample loss: 0.1098 acc: 0.9771 val_loss: 0.4550 val_acc: 0.8220 epoch 9/10 20000/20000 ============================== 2s 111us/sample loss: 0.0943 acc: 0.9829 val_loss: 0.4867 val_acc: 0.8150 epoch 10/10 20000/20000 ============================== 2s 113us/sample loss: 0.0799 acc: 0.9890 val_loss: 0.5074 val_acc: 0.8134 train on 20000 samples, validate on 5000 samples epoch 1/10 20000/20000 ============================== 53s 3ms/sample loss: 0.6252 acc: 0.6412 val_loss: 0.6472 val_acc: 0.6328 epoch 2/10 20000/20000 ============================== 53s 3ms/sample loss: 0.6286 acc: 0.6467 val_loss: 0.6108 val_acc: 0.6628 epoch 3/10 20000/20000 ============================== 52s 3ms/sample loss: 0.5302 acc: 0.7362 val_loss: 0.4725 val_acc: 0.7834 epoch 4/10 20000/20000 ============================== 52s 3ms/sample loss: 0.4340 acc: 0.8036 val_loss: 0.4519 val_acc: 0.7890 epoch 5/10 20000/20000 ============================== 52s 3ms/sample loss: 0.4045 acc: 0.8242 val_loss: 0.4529 val_acc: 0.8108 epoch 6/10 20000/20000 ============================== 53s 3ms/sample loss: 0.3885 acc: 0.8338 val_loss: 0.4481 val_acc: 0.7916 epoch 7/10 20000/20000 ============================== 52s 3ms/sample loss: 0.3751 acc: 0.8381 val_loss: 0.4470 val_acc: 0.7882 epoch 8/10 20000/20000 ============================== 52s 3ms/sample loss: 0.3443 acc: 0.8566 val_loss: 0.4582 val_acc: 0.8054 epoch 9/10 20000/20000 ============================== 52s 3ms/sample loss: 0.3310 acc: 0.8615 val_loss: 0.4757 val_acc: 0.8048 epoch 10/10 20000/20000 ============================== 52s 3ms/sample loss: 0.3146 acc: 0.8703 val_loss: 0.5022 val_acc: 0.7892 train on 20000 samples, validate on 5000 samples epoch 1/10 20000/20000 ============================== 85s 4ms/sample loss: 0.4972 acc: 0.7459 val_loss: 0.4147 val_acc: 0.8120 epoch 2/10 20000/20000 ============================== 85s 4ms/sample loss: 0.3874 acc: 0.8345 val_loss: 0.3996 val_acc: 0.8206 epoch 3/10 20000/20000 ============================== 85s 4ms/sample loss: 0.3639 acc: 0.8399 val_loss: 0.3948 val_acc: 0.8164 epoch 4/10 20000/20000 ============================== 84s 4ms/sample loss: 0.3483 acc: 0.8497 val_loss: 0.3963 val_acc: 0.8160 epoch 5/10 20000/20000 ============================== 84s 4ms/sample loss: 0.3371 acc: 0.8551 val_loss: 0.3870 val_acc: 0.8190 epoch 6/10 20000/20000 ============================== 85s 4ms/sample loss: 0.3271 acc: 0.8566 val_loss: 0.4170 val_acc: 0.8064 epoch 7/10 20000/20000 ============================== 84s 4ms/sample loss: 0.3173 acc: 0.8607 val_loss: 0.3956 val_acc: 0.8126 epoch 8/10 20000/20000 ============================== 84s 4ms/sample loss: 0.3049 acc: 0.8689 val_loss: 0.4180 val_acc: 0.8250 epoch 9/10 20000/20000 ============================== 85s 4ms/sample loss: 0.2996 acc: 0.8698 val_loss: 0.4207 val_acc: 0.8186 epoch 10/10 20000/20000 ============================== 84s 4ms/sample loss: 0.2892 acc: 0.8764 val_loss: 0.4177 val_acc: 0.8238 train on 20000 samples, validate on 5000 samples epoch 1/10 20000/20000 ============================== 8s 418us/sample loss: 0.6473 acc: 0.6470 val_loss: 0.5372 val_acc: 0.7466 epoch 2/10 20000/20000 ============================== 2s 106us/sample loss: 0.5667 acc: 0.7542 val_loss: 0.7262 val_acc: 0.5218 epoch 3/10 20000/20000 ============================== 2s 108us/sample loss: 0.5669 acc: 0.7245 val_loss: 0.5634 val_acc: 0.7370 epoch 4/10 20000/20000 ============================== 2s 107us/sample loss: 0.5552 acc: 0.7383 val_loss: 0.5574 val_acc: 0.7634 epoch 5/10 20000/20000 ============================== 2s 106us/sample loss: 0.4822 acc: 0.7847 val_loss: 0.5535 val_acc: 0.7614 epoch 6/10 20000/20000 ============================== 2s 115us/sample loss: 0.4619 acc: 0.8069 val_loss: 0.5387 val_acc: 0.7730 epoch 7/10 20000/20000 ============================== 2s 106us/sample loss: 0.4385 acc: 0.8191 val_loss: 0.5487 val_acc: 0.7762 epoch 8/10 20000/20000 ============================== 2s 110us/sample loss: 0.4413 acc: 0.8173 val_loss: 0.5513 val_acc: 0.7748 epoch 9/10 20000/20000 ============================== 2s 107us/sample loss: 0.4005 acc: 0.8329 val_loss: 0.6434 val_acc: 0.7672 epoch 10/10 20000/20000 ============================== 2s 107us/sample loss: 0.3908 acc: 0.8400 val_loss: 0.6809 val_acc: 0.7844 after a long time of waiting, the training is finally complete! if you are following this tutorial on your local workstation, please note that the time required for training may vary depending on your hardware configurations or the specification of our instance if you are using a cloud based platform like aws. none of our models reached the threshold of ninety percent accuracy, but they all managed to converge to some reasonable number, hovering around the high seventies to low eighties. let's test the performance of our models by using the and data, both of which none of our models have seen before. 25000/25000 ============================== 1s 48us/sample loss: 0.5968 acc: 0.7931 0.5968295060539246, 0.79312 25000/25000 ============================== 14s 562us/sample loss: 0.4948 acc: 0.7911 0.4948168795013428, 0.79112 25000/25000 ============================== 24s 946us/sample loss: 0.4092 acc: 0.8321 0.4091824066162109, 0.83212 25000/25000 ============================== 1s 42us/sample loss: 0.6494 acc: 0.7915 0.6493830096054077, 0.79152 based on the results, it looks like the lstm model performed best, beating other models by a small margin. at this point, we cannot conclude as to whether or not this marginal boost in performance is significant. judging this would not only depend on the context, but also most likely require us to have a larger test dataset that captures the statistics of the population data. this point notwithstanding, it is certainly beneficial to know that lstm networks are good at detecting sequential patterns in data. last but not least, let's visualize the training scheme of all four models to take a identify any possible signs of convergence and overfitting, if any. to do that, we will be using the function shown below. the dense feed forward network seems to have a very linear pattern. one immediate pattern we see is that the model seems to be overfitting right away, since the testing accuracy decreases with each epoch while the training accuracy increases. this is certainly not a good sign; in the best case scenario, we want to see that training and testing labels moving in the same direction. perhaps this is the biggest indication that a simple feed forward network is a suboptimal model choice in the context of this problem. the graphs for the model seems a lot better. at the very least, we see the training and test labels moving in unison: the accuracy increases with each epoch, while the loss slowly decreases. however, we do see some overfitting happening at the last two epochs or so. specifically, note that cross entropy loss for the testing data seems to pick up an incrementing pattern past the seventh epoch. this observation suggests that we need to configure the model differently, presumably by decreasing the number of tunable parameters. next comes the winner of the day, the lstm network. an interesting point to note about the learning curve of this model is that the test data accuracy and loss seem to stay roughly stagnant despite the progression of epochs. to better understand this phenomena, we probably have to run more trials with more data over longer iterations than we have done in this tutorial. this point notwithstanding, it is interesting to see how a single layer lstm network can outperform a stacked rnn network. the last up on this list is the one dimensional convolutional neural network. the convent produced very remarkable results in this experiment, especially given its extremely short training time. recurrent neural networks typically take a lot of time to train even when they are not stacked because each neuron is defined by a rather complicated operation involving many parameters, such as states, carriage, and so on. convents, on the other hand, are relatively simpler, and thus take noticeably shorter to train and deploy. this tutorial demonstrates that convents can perform as well as simple recurrent networks to establish a baseline performance metric. in this post, we briefly introduced and explored the concept of recurrent neural networks, how they work, and how to build them using the functional api. recurrent neural networks are one of the hottest topics in the contemporary deep learning academia because it presents numerous possibilities for applications. hopefully this post gave you a better understanding of what all the hype is about, why rnns are effective at what they do, and how they can be used in the context of basic natural language processing. in the next post, we will take a look at another interesting natural language processing task. peace! my deepest condolences to those affected by the wuhan corona virus, as well as the families and fans of kobe bryant.