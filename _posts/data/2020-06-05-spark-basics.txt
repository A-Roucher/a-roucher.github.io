i've stumbled across the word "apache spark" on the internet so many times, yet i never had the chance to really get to know what it was. for one thing, it seemed rather intimidating, full of buzzwords like "cloud computing", "data streaming," or "scalability," just to name a few among many others. however, a few days ago, i decided to give it a shot and try to at least get a glimpse of what it was all about. so here i report my findings after binge watching online tutorials on apache spark. if you're into data science or even just software development, you might have heard some other apache variants like apache kafka, apache cassandra, and many more. when i first heard about these, i began wondering: is apache some sort of umbrella software, with spark, kafka, and other variants being different spinoffs from this parent entity? i was slightly more confused because the apache i had heard of, at least as far as i recalled, had to do with web servers and hosting. turns out that there is an organization called the apache software foundation, which is the world's largest open source foundation. this foundation, of course, has to do with the apache http server project, which was the web server side of things that i had ever so faintly heard about. then what is apache spark? spark was originally developed at uc berkeley at the amp lab. later, its code base was open sourced and eventually donated to the apache software foundation; hence its current name, apache spark. for this tutorial, we will be loading apache spark on jupypter notebook. there are many tutorials on how to install apache spark, and they are easy to follow along. however, i'll also share a quick synopsis of my own just for reference. installing apache spark is pretty straight forward if you are comfortable dealing with on macos or on linux. the executive summary is that you need to add apache spark binaries to the variable of your system. what is a ? basically, the variable is where all your little unix programs live. for example, when we run simple commands like or , we are essentially invoking built in mini programs in our posix system. the variable tells the computer where these mini programs reside in, namely , which is by default part of the variable. can the variable be modified? the answer is a huge yes. say we have our own little mini program, and we want to be able to run it from the command line prompt. then, we would simply modify so that the computer knows where our custom mini program is located and know what to do whenever we type some command in the terminal. this is why we enter the python shell in interactive mode when we type on the terminal. here is the little setup i have on my own : here, i prepended to the default then appended at the end. appending and prepending result in different behaviors: by default, the computer searches for commands in the variable in order. in other words, in the current setup, the computer will first search the directory, then search the default directory, and look at the very last, at least in my current setup. note that spark has specific java requirements that may or may not align with the default java installation on your workstation. in my case, i had to install a different version of java and apply certain configurations. the path variable is a result of this minor modification. the contents in the directory simply contains the result of unzipping the file available for download on the apache spark website. once the variable has been configured, run , and you should be ready to run apache spark on your local workstation! to see if the installation and configuration has been done correctly, type on the terminal: to use jupyter with spark, we need to do a little more work. there are two ways to do this, but i will introduce the method that i found not only fairly simple, but also more applicable and generalizable. all we need is to install package via . then, on jupyter, we can do: then simply import apache spark via that is literally all we need! we can of course still use apache spark on the terminal simply by typing if we want, but it's always good to have more options on the table. the rdd api is the most basic way of dealing with data in spark. rdd stands for "resilient distributed dataset." although more abstracted, higher level apis such as spark sql or spark dataframes are becoming increasingly popular, thus challenging rdd's standing as a means of accessing and transforming data, it is a useful structure to learn nonetheless. one salient feature of rdds is that computation in an rdd is parallelized across the cluster. to run spark, we need to initialize a spark context. a spark context is the entry point to spark that is needed to deal with rdds. we can initialize one simply as follows: strictly speaking, the more proper way to do this would be to follow the syntax guideline on the official website. however, we use a more simplified approach without initializing different apps for each example, simply for convenience purposes. let's begin with a simple dummy example. here, we turn a normal python list into an rdd, then print out its contents after applying a squaring function. we use to turn the rdd into an iterable, specifically a list. 1 4 9 16 another useful function is and . as you might have easily guessed, these functions are literally used to count the number of elements itself or their number of occurrences. this is perhaps best demonstrated by an example. 7 works in a similar fashion, but it creates a dictionary of key value pairs, where the key is an element and the value is the count of that element in the input list. defaultdict as the name implies, is a way of reducing a rdd into something like a single value. in the example below, we reduce an rdd created with a list of numbers into a product of all the numbers in that original input list. rdd: 1, 2, 3, 4 24 this was a pretty simple example. let's take a look at a marginally more realistic of an example, albeit still extremely simple. i'm using the files from this spark tutorial on github. after cloning the repo, we establish a base file path and retrieve the file we will be using in this example. let's see what this rdd looks like. we can do this via , much like in pandas. '1,"goroka","goroka","papua new guinea","gka","ayga", 6.081689,145.391881,5282,10,"u","pacific/port_moresby"', '2,"madang","madang","papua new guinea","mag","aymd", 5.207083,145.7887,20,10,"u","pacific/port_moresby"', '3,"mount hagen","mount hagen","papua new guinea","hgu","aymh", 5.826789,144.295861,5388,10,"u","pacific/port_moresby"', '4,"nadzab","nadzab","papua new guinea","lae","aynz", 6.569828,146.726242,239,10,"u","pacific/port_moresby"', '5,"port moresby jacksons intl","port moresby","papua new guinea","pom","aypy", 9.443383,147.22005,146,10,"u","pacific/port_moresby"' if you look carefully, you will realize that each element is a long string, not multiple elements separated by a comma as we would like. let's define a helper function to split up each elements as we would like. let's test out this function with the first element in the rdd. '1', 'goroka', 'goroka', 'papua new guinea', 'gka', 'ayga', ' 6.081689', '145.391881', '5282', '10', 'u', 'pacific/port_moresby' great! it worked as expected. now let's say that we want to retrieve only those rows whose entries deal with airports in the united states. specifically, we want the city and the name of the airport. how would we go about this task? well, one simple idea would be to filter the data for airports in the united states, then only displaying the relevant information, namely the name of the airport and the city in which it is located. let's begin by defining the function. and we test it on the first element to verify that it works as expected: 'goroka, goroka' as stated earlier, we first filter the data set so that we only have entries that pertain to airports in the united states. then, we the rdd using the function we defined above. this will transform all elements into the form we want: the name of the airport and the city. we actually used above when we were dealing with square numbers. it's pretty similar to how map works in python or other functional programming languages. 'putnam county airport, greencastle', 'dowagiac municipal airport, dowagiac', 'cambridge municipal airport, cambridge', 'door county cherryland airport, sturgeon bay', 'shoestring aviation airfield, stewartstown' now let's take a look at another commonly used operation: . for this example, we load a text file containing prime numbers and create a rdd. ' 2\t 3\t 5\t 7\t 11\t 13\t 17\t 19\t 23\t 29', ' 31\t 37\t 41\t 43\t 47\t 53\t 59\t 61\t 67\t 71', ' 73\t 79\t 83\t 89\t 97\t101\t103\t107\t109\t113', '127\t131\t137\t139\t149\t151\t157\t163\t167\t173', '179\t181\t191\t193\t197\t199\t211\t223\t227\t229' , as the name implies, maps a certain operation over the elements of a rdd. the difference between and is that the latter flattens the output. in this case, we split the numbers along . normally, this would create a separate list for each line. however, since we also flatten that output, there is no distinction between one line and another. 2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71 just for a simple recap, let's try adding all the numbers in that rdd using . 24133 we can also consider rdds to be likes sets, in the pythonic or the mathematical sense, whichever you conceptually prefer. the idea is that we can use set operators, such as intersection or unions, to extract data we want from the rdd to create a new rdd. below is an example using nasa records, each from july and august of 1995. 'host\tlogname\ttime\tmethod\turl\tresponse\tbytes', '199.72.81.55\t \t804571201\tget\t/history/apollo/\t200\t6245\t\t', 'unicomp6.unicomp.net\t \t804571206\tget\t/shuttle/countdown/\t200\t3985\t\t', '199.120.110.21\t \t804571209\tget\t/shuttle/missions/sts 73/mission sts 73.html\t200\t4085\t\t', 'burger.letters.com\t \t804571211\tget\t/shuttle/countdown/liftoff.html\t304\t0\t\t' the task is to obtain hosts that are both in the july and august logs. we might want to break this task up into several discrete components. the first step would be to extract the host information from the original logs. we can do this simply by splitting and obtaining the first element of each resulting list. 'host', 'in24.inetnebr.com', 'uplherc.upl.com', 'uplherc.upl.com', 'uplherc.upl.com' then, all we have to do is to apply the intersection operation, then filter out the first column header . 'www a1.proxy.aol.com', 'www d3.proxy.aol.com', 'piweba1y.prodigy.com', 'www d4.proxy.aol.com', 'piweba2y.prodigy.com' lastly, if we wanted to save the rdd to some output file, we would use the function. note that the output file would be split up into multiple files, since computation is distributed in spark. also, it is generally considered standard good practice to split up huge datasets into separate files, rather than coalescing all of them into a single one. so far, we have looked at rdds. now, let's turn our attention to another type of widely used rdds: pair rdds. pair rdds are widely used because they are, in a way, like dictionaries with key value pairs. this key value structure is very useful, since we can imagine there being a lot of operations where, for instance, a value is reduced according to their keys, or some elements are grouped by their keys, and et cetera. let's take a look at what we can do with pair rdds. both and operations work the same way as you would expect with normal rdds. let's first create a toy example using the rdd we looked at earlier. to remind ourselves of what this data looked like, we list the first five elements in the rdd. '1,"goroka","goroka","papua new guinea","gka","ayga", 6.081689,145.391881,5282,10,"u","pacific/port_moresby"', '2,"madang","madang","papua new guinea","mag","aymd", 5.207083,145.7887,20,10,"u","pacific/port_moresby"', '3,"mount hagen","mount hagen","papua new guinea","hgu","aymh", 5.826789,144.295861,5388,10,"u","pacific/port_moresby"', '4,"nadzab","nadzab","papua new guinea","lae","aynz", 6.569828,146.726242,239,10,"u","pacific/port_moresby"', '5,"port moresby jacksons intl","port moresby","papua new guinea","pom","aypy", 9.443383,147.22005,146,10,"u","pacific/port_moresby"' next, we define a function with which we will map the rdd. the biggest difference between this function and the ones we have defined previously is that this tuple returns a tuple instead of a single value. therefore, this tuple functionally takes the structure of a pair. in this case, we have the name of the airport and the country in which it is located in. for demonstration and sanity check purposes, let's apply the function to an example: works as expected! if we map the entire rdd with , then we would end up with an rdd containing tuples as each of its elements. in a nutshell, this is what a pair rdd looks like. , , , , if we want to use filter, we can simply access keys or values as appropriate using list indexing with brackets. for example, if we want to obtain a list of airports in the united states, we might execute the following statement. , , , , as stated earlier, one of the advantages of using pair rdds is the ability to perform key or value specific operations. for example, we might want to apply some map function on the values of the rdd while leaving the keys unchanged. let;s say we want to change country names to uppercase. this might be achieved as follows: , , , , however, this statement is rather verbose, since it requires us to specify that we want to leave the key unchanged by declaring the tuple as . instead, we can use to achieve the same result with much less boilerplate. , , , , note that we didn't have to tell spark what to do with the keys: it already knew that the keys should be left unchanged, and that mapping should only be applied to the values of each pair element in the rdd. earlier, we took a look at the operation, which was used to calculate things like sums or products. the equivalent for pair rdds is . let's take a look at an example of a simple word frequency counting using a dummy text file. "the history of new york begins around 10,000 bc, when the first native americans arrived. by 1100 ad, new york's main native cultures, the iroquoian and algonquian, had developed. european discovery of new york was led by the french in 1524 and the first land claim came in 1609 by the dutch. as part of new netherland, the colony was important in the fur trade and eventually became an agricultural resource thanks to the patroon system. in 1626 the dutch bought the island of manhattan from native americans.1 in 1664, england renamed the colony new york, after the duke of york new york city gained prominence in the 18th century as a major trading port in the thirteen colonies.", '' to count the occurrences of words, we first need to split the strings into words. note that we want to use since we don't want to establish a distinction between different sentences; instead, we want to flatten the output. 'the', 'history', 'of', 'new', 'york' a hacky way to go about this task is to first transform the rdd into a pair rdd where each key is a word and the value is 1. then, we can add up the values according to each key. let's accomplish this step by step. , , , , as you might have guessed, this is where comes into play. applying the simple addition lambda function with produces a pair rdd that contains the total count for each word. , , , , one natural extension we might want to go from the word counting example is sorting. one can easily imagine situations where we might want to sort a pair rdd in some ascending or descending order according to value. this can be achieved via the function. here, we set so that the most frequent words would come at the top. , , , , another common operation with pair rdds is . however, before we get into the details, it should be noted that this method is strongly discouraged for performance reasons, especially on large datasets. for more information, i highly recommend that you take a look at this notebook by databricks although it is written in scala, you will understand most of what is going on based on your knowledge of pyspark. with this caveat out of the way, let's take a look at what we can do with . we first use the rdd we've used before in other examples. '1,"goroka","goroka","papua new guinea","gka","ayga", 6.081689,145.391881,5282,10,"u","pacific/port_moresby"', '2,"madang","madang","papua new guinea","mag","aymd", 5.207083,145.7887,20,10,"u","pacific/port_moresby"', '3,"mount hagen","mount hagen","papua new guinea","hgu","aymh", 5.826789,144.295861,5388,10,"u","pacific/port_moresby"', '4,"nadzab","nadzab","papua new guinea","lae","aynz", 6.569828,146.726242,239,10,"u","pacific/port_moresby"', '5,"port moresby jacksons intl","port moresby","papua new guinea","pom","aypy", 9.443383,147.22005,146,10,"u","pacific/port_moresby"' this time, we use a mapping function that returns a key value pair in the form of . as you may have guessed, we want to group by country keys to build a new pair rdd. first, we check that the function works as expected, thus producing a pair rdd of country airport pair elements. , , , , if we apply to this rdd, we get a pair rdd whose values are objects in pyspark speak. this is somewhat like a list but offered through the spark interface and arguably less tractable than normal python lists in that they can't simply be indexed with brackets. , , , , to get a sneak peak into what objects look like, we can convert them into a list. note that we normally wouldn't enforce list conversion on large datasets. iceland 'akureyri', 'egilsstadir', 'hornafjordur', 'husavik', 'isafjordur', 'keflavik international airport', 'patreksfjordur', 'reykjavik', 'siglufjordur', 'vestmannaeyjar', 'reykjahlid airport', 'bakki airport', 'vopnafjörður airport', 'thorshofn airport', 'grímsey airport', 'bildudalur airport', 'gjogur airport', 'saudarkrokur', 'selfoss airport', 'reykjahlid', 'seydisfjordur', 'nordfjordur airport' join, which comes from relational algebra, is a very common operation that comes from relational algebra. it is commonly used in sql to bring two or more tables into the same picture. for a quick visual representation of what joins are, here is an image that might be of help. we can perform joins on pair rdds as well. we can consider pair rdds to be somewhat like sql tables with just a primary key and a single column to go with it. let's quickly create a toy dataset to illustrate the join operators in pyspark. here, we have a list of names, ages, and their countries of origin. to best demonstrate the join operation, we intentionally create a mismatch of keys in the and . here is a little helper function to help us take a look at what the keys and values are in a pair rdd. key: sarah, value: 16 key: john, value: 20 key: tom, value: 16 key: clara, value: 15 key: tom, value: uk key: clara, value: fr key: ellie, value: prc key: jake, value: can key: demir, value: bel as we can see, the and each have some overlapping keys, but not all keys are in both rdds. for instance, tom and clara are in both rdds, but john is only in the . this intentional mismatch is going to be useful later when we discuss the difference between left and right joins. first, let's take a look at , which in pyspark refers to an inner join. since this an inner join, we only get results pertaining to keys that are present in both rdds, namely clara and tom. key: clara, value: key: tom, value: note that if we flip the order of joins, the order of elements in the values of each key value pairs also changes. key: clara, value: key: tom, value: things get slightly more interesting with other joins like . i personally find it intuitive to image two van diagrams, with the left one being completely filled in the case of a left join. in other words, we keep all the keys of the left table while joining with the table on the right. in this case, since there exists a key mismatch, only those keys that are present in both tables will end up with a full joined tuple; others are s in their joined values. key: john, value: key: clara, value: key: tom, value: key: sarah, value: the same happens with . this is exactly identical to what the left join is, except with the very obvious caveat that the right diagram is filled, not the left. key: ellie, value: key: clara, value: key: tom, value: key: jake, value: key: demir, value: lastly, let's take a look at . this is a combination of the left and right join we fill up the entire van diagram, both left and right. notice that this is exactly what happens when we add the results from and , with duplicates removed. key: john, value: key: ellie, value: key: clara, value: key: tom, value: key: jake, value: key: sarah, value: key: demir, value: in this post, we explored the various aspects of apache spark: what it is, how to set it up, and what we can do with it via the rdd api. there is a lot more to spark that we haven't discussed, such as spark sql or mllib. i will most definitely be writing a post on these as i become more familiar with the various apis and functionalities that spark has to offer. i doubt i'll be using spark for any personal project, since spark is used for processing large datasets across different clusters, not on a single computer as we have done here. however, it was an interesting journey and one that was definitely worth the time and effort, since i feel like i've at least gained some glimpse of what all the hype behind the spark keyword is. i hope you've enjoyed reading this post. see you in the next one!