taylor series is used in countless areas of mathematics and sciences. it is a handy little tool in the mathematicians arsenal that allows us to decompose any function into a series of polynomials, which are fairly easy to work with. today, we are going to take a brief look at another type of series expansion, known as fourier series. note that these concepts are my annotations of professor gilbert strang's amazing lecture, available on youtube. the biggest difference between taylor series and fourier series is that, unlike taylor series, whose basic fundamental unit is a polynomial term, the building block of a fourier series is a trigonometric function, namely one of either sine or cosine. concretely, a generic formula of a fourier expansion looks as follows: personally, i found this formula to be more difficult to intuit than the taylor series. however, once you understand the underlying mechanics, it’s fascinating to see how periodic wave functions can be decomposed as such. first, let’s begin with an analysis of orthogonality. commonly, we define to vectors and as being orthogonal if that is, if their dot product yields zero. this follows from the definition of a dot product, which has to do with cosines. with a stretch of imagination, we can extend this definition of orthogonality to the context of functions, not just vectors. for vectors, a dot product entails summing the element wise products of each component. functions don’t quite have a clearly defined, discrete component. therefore, instead of simply adding, we integrate over a given domain. for example, the same applies to cosines and sines: where and can be any integer. in other words, cosine functions of different frequencies are orthogonal to each other, as are cosines are with sines! now, why is orthogonality relevant at all for understanding the fourier series? it’s time to sit back and let the magic unfold when we multiply to and integrate the entire expression. if we divide both sides of by , you will realize that we have derived an expression for the constant corresponding to the expansion term: the key takeaway here is this: by exploiting orthogonality, we can knock out every term but one, the very term that we multiplied to the expansion. by the same token, therefore, we can deduce that we can do the same for the sine terms: the only small caveat is that the case is a bit more specific for . when , reduces to a constant of one, which is why we end up with instead of . in other words, hence, we end up with this exceptional term has a very intuitive interpretation: it is the average of the function over the domain of integration. indeed, if we were to perform some expansion, it makes intuitive sense that we start from an average. one observation to make about fourier expansion is the fact that it is a combination of sines and cosines and we have seen those before with, lo and behold, euler’s formula. recall that euler’s formula is a piece of magic that connects all the dots of mathematics. here is the familiar equation: using euler’s formula, we can formulate an alternative representation of fourier series: let’s unequivocally drive this concept home with a simple example involving the dirac delta function. the delta function is interesting function that looks like this: the delta function has two nice properties that make it great to work with. first, it integrates to one if the domain includes . this is the point where the graph peaks in the diagram. second, the delta function is even. this automatically tells us that when we perform a fourier expansion, we will have no sine functions sine functions are by nature odd. with this understanding in mind, let’s derive the fourier series of the dirac delta by starting with . the equality is due to the first property of the delta function outlined in the previous paragraph. the derivation of the rest of the constants can be done in a similar fashion. the trick is to use the fact that the delta function is zero in all domains but . therefore, the oscillations of will be nullified by the delta function in all but that one point, where is just one. therefore, simply reduces to integrating the delta function itself, which is also one! to sum up, we have the following: i find it fascinating to see how a function so singular and unusual as the dirac delta can be reduced to a summation of cosines, which are curvy, oscillating harmonics. this is the beauty of expansion techniques like the fourier and taylor: as counterintuitive as it may seem, these tools tell us that any function can be approximated through an infinite summation, even if the original function may not resemble the building block of the expansion technique at all at a glance.