i've always been a fan of tensorflow, specifically , for its simplicity and ease of use in implementing algorithms and building models. today, i decided to give pytorch a try. it is my understanding that tensorflow is more often used in coporate production environments, whereas pytorch is favored by academics, especially those in the field of nlp. i thought it would be an interesting idea to give it a try, so here is my first go at it. note that the majority of the code shown here are either borrowed from or are adaptations of those available on the pytorch website, which is full of rich content and tutorials for beginners. of course, basic knowledge of dl and python would be helpful, but otherwise, it is a great place to start. let's dive right in! like tensorflow, pytorch is a scientific computing library that makes use of gpu computing power to acceleration calculations. and of course, it can be used to create neural networks. in this section, we will take a look at how automatic differentiation works in pytorch. note that differentiation is at the core of backpropagation, which is why demonstrating what might seem like a relatively low level portion of the api is valuable. let's begin our discussion by first importing the pytorch module. it isn't difficult to see that is a scientific computing library, much like . for instance, we can easily create a matrice of ones as follows: tensor the is a parameter we pass into the function to tell pytorch that this is something we want to keep track of later for something like backpropagation using gradient computation. in other words, it "tags" the object for pytorch. let's make up some dummy operations to see how this tagging and gradient calculation works. y = tensor z = tensor out = 27.0 note that performs element wise multiplication, otherwise known as the dot product for vectors and the hadamard product for matrics and tensors. let's look at how autograd works. to initiate gradient computation, we need to first call on the final result, in which case . then, we can simply call to tell pytorch to calculate the gradient. note that this works only because we "tagged" with the parameter. if we try to call on any of the other intermediate variables, such as or , pytorch will complain. tensor let's try to understand the result of this computation. let denote the final tensor. since we called , and since has a total of four elements, we can write out our dummy calculations mathematically in the following fashion: using partial differentiation to obtain the gradients, since , since is just an arbitrary, non specific index out of a total of four, we can easily see that the same applies for all other indices, and hence we will end up with a matrix whose all four entries take the value of 4.5, as pytorch has rightly computed. we can go even a step farther and declare custom operations. for example, here's a dummy implementation of the relu function. let's talk about the method first. note that it takes in two argument parameters: and . as you might have guessed, is simply the value that the function will be provided with. the can simply be thought of as a cache where we can store vectors or matrices to be used during backpropagation. in this case, we store the by calling method. during the backward pass, we compute the gradient. here, we need to retrieve the variable which was stored in the context. this is because the relu function takes the following form: thus, its derivative is during backpropagation, this means that gradients will flow down to the next layer only for those indices whose input elements to te relu function were greater than 0. thus, we need the input vector for reference purposes, and this is done via stashing it in the variable. we will see how we can incorporate into the model in the next section. in this example, we'll take a look at an extremely simple model to gain a better understanding of how everything comes into play in a more practical example. this is the method that i've mostly been using when implementing simple dense fully connected models in numpy. the idea is that we would mathematically derive the formula for the gradients ourselves, then backpropagate these values during the optimization process. of course, this can be done with pytorch. to build our simple model, let's first write out some variables to use, starting with the configuration of our model and its dimensions. we will also need some input and output tensors to be fed into the model for trainining and optimization. next, here are the weight matrices we will use. for now, we assume a simple two layered dense feed forward network. last but not least, let's define a simple squared error loss function to use during the training step. with this entire setup, we can now hash out what the entire training iteration is going to look like. wrapped in a loop, we perform one forward pass, then perform backpropagation to adjust the weights. epoch 99: 714.8318481445312 epoch 199: 3.586176633834839 epoch 299: 0.03582914546132088 epoch 399: 0.0007462671492248774 epoch 499: 8.23544614831917e 05 great! we see that the loss drops as more epochs elapse. while there is no problem with this approach, things can get a lot more unwieldy once we start building out more complicated models. in these cases, we will want to use the auto differentiation functionality we reviewed earlier. let's see this in action. also, let's make this more pytorch y by making use of classes. we will revisit why class based implementations are important in the next section. loss: 321.8311767578125 loss: 0.6376868486404419 loss: 0.0022135386243462563 loss: 7.520567305618897e 05 loss: 1.823174170567654e 05 notice we didn't have to explicitly specify the backpropagation formula with matrix derivatives: by simply calling properties for each of the weights matrices, we were able to perform gradient descent. one detail to note is that, unlike in the case above where we had to explicitly call in order to obtain the loss value which would be of type we leave the computed loss to remain as a tensor in order to call . we also make sure to reset the gradients per epoch by calling . we can also improve our implementation by making use of the class that we implemented earlier. this is simple as doing this might be a better way to implement the function for reasons of simplicity and readability. although ing works, it's more arguably cleaner to write a relu this way. also, this is a dummy example, and we can imagine a lot of situations where we might want to write custom functions to carry out specific tasks. much like tensorflow, pytorch offers to ways of declaring models: function based and class based methods. although i have just started getting into pytorch, my impression is that the later is more preferred by pytorch developers, whereas this is not necessarily the case with keras or . of course, this is a matter of preference and development setting, so perhaps such first impression generalizations do not carry much weight. nonetheless, in this section, we will take a look at both ways of building models. let's start with the function based method. the function based method reminds me a lot of keras's sequential method. let's remind ourselves of kera's basic sequential model api: now let's compare this method with pytorch's way of declaring sequential models: this model declaration is in fact exactly identical to the simple model we have declared above. you can easily see how similar this code snippet is to the keras example. the only difference is that the activation function is declared independently of the layer itself in pytorch, whereas keras combines them into one via argument. of course, you don't have to specify this argument, and we can import the relu function from tensorflow to make it explicit like the pytorch example. the point, however, is that the sequential model api for both libraries are pretty similar. another way to build models is by subclassing . the submodule in pytorch is the one that deals with neural networks; hence the . this subclassing might look as follows: this model is no different from the we defined earlier. the only notable difference is that we didn't define a separate type function. for the most part, the overall idea boils down to the weights are definited in the function the function deals with forward pass now let's take a look at what the training code looks like. epoch 99: 2.7017245292663574 epoch 199: 0.050356119871139526 epoch 299: 0.001569570624269545 epoch 399: 5.8641602663556114e 05 epoch 499: 2.4734026737860404e 06 although things might look a bit different, there's not much going on in this process, other than the fact that some of the functions and logic we wrote before are now abstracted away by pytorch. for example, we see , which is effectively the mean squared error loss, similar to how we defined above. another difference we see is , which, as the variable name makes apparent, is the optimizer that we use for backpropagation. in this specific instance, we use sgd. each backpropagation step is then performed simply via . in this tutorial, we took a very brief look at the pytorch model. this is by no means a comprehensive guide, and i could not even tell anyone that i "know" how to use pytorch. nonetheless, i'm glad that i was able to gain some exposure to the famed pytorch module. also, working with django has somewhat helped me grasp the idea of classes more easily, which certainly helped me take in class based concepts in pytorch more easily. i distinctively remember people saying that pytorch is more object oriented compared to tensorflow, and i might express agreement to that statement after having gone through the extreme basics of pytorch. in the upcoming articles, i hope to use pytorch to build more realistic models, preferrably in the domain of nlp, as that seems to be where pytorch's comparative advantage stands out the most compared to tensorflow. of course, this is not to say that i don't like tensorflow anymore, or that pytorch is not an appropriate module to use in non nlp contexts: i think each of them are powerful libraries of their own that provide a unique set of functionalities for the user. and being bilinguial or even a polyglot, if you can use things like caffe perhaps in the dl module landscape will certainly not hurt at all. i hope you've enjoyed this article. catch you up in the next one!