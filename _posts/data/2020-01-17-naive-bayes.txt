welcome to part three of the "from scratch" series where we implement machine learning models from the ground up. the model we will implement today, called the naive bayes classifier, is an interesting model that nicely builds on top of the bayesian mindset we developed in the previous post on markov chain monte carlo. much like the logistic regression model, naive bayes can be used to solve classification tasks, as opposed to regression in which case the goal is to predict a continuous variable. the main difference between logistic regression and naive bayes is that naive bayes is built on a probabilistic model instead of an optimization model such as graident descent. hence, implementing naive bayes is somewhat easier from a programming point of view. enough of the prologue, let's cut to the chase. to understand naive bayes, we need not look further than bayes's theorem, which is probably the single most referenced theorem on this blog so far. i won't explain this much since we have already seen it so many times, but presented below is the familar formula for reference and readability's sake. very standard, perhaps with the exception of some minor notation. here, refers to a single instance, represented as a vector with entries; , the corresponding label or class for that instance. note that is not a feature matrix, but a single instance. more concretely, of course, is just a scalar value. this characterization is very apt in the context of machine learning. the underlying idea is that, given some data with feature columns, we can derive a probability distribution for the label for that intance. the naive assumption that the naive bayes classifier makes now you can guess where that name comes from is that each of the variables in the instance vector are independent of one another. in other words, knowing a value for one of the features does not provide us with any information about the values for the other feature columns. combining this assumption of independence with bayes' theorem, we can now restate as follows: pretty straight forward. we know that the demominator, which often goes by the name "evidence" in bayesian inference, is merely a normalizing factor to ensure that the posterior distribution integrates to 1. so we can discard this piece of information and distill down even farther: equation tells us that it is possible to calculate the probability of instance belonging to class systematically. why is this important? the simple answer is that we can use to train the naive bayes classifier. say we know that for a particular instance , the label is . then, we have to find the distribution for each feature such that we can maximize . does this ring any bells? yes it is maximum a posteriori estimation! in other words, our goal would be to maximize the posterior distribution for each training instance so that we can eventually build a model that would output the most likely label that the testing instance belongs to. in other words, our training scheme can be summarized as: but this is all to abstract. let's get into the details by implementing the naive bayes classifer from scratch. before we proceed, however, i must tell you that there are many variations of the naive bayes classifer. the variant that we will implement today is called the gaussian naive bayes classifer, because we assume that the distribution of the feature variables, denoted as , is normal. for a corresponding explanation of this model on , refer to this documentation. let's jump right into it. as per convention, we start by importing necessary modules for this tutorial. for reproducability, we specify a . the magic commands are for the configuration of this jupyter notebook. let's begin by building some toy data. to make things simple, we will recycle the toy data set we used in the previous post on logistic regression and k nearest neighbors. the advantage of using this data set is that we can easily visualize our data since all instances live in . in other words, we only have two axes: and . for convenience, we preprocess our toy data set and labels into arrays. the first step is to separate the data set by class values, since our goal is to find the distributions for each class that best describe the given data through map. to achieve this objective, we can create a function that returns a dictionary, where the key represents the class and the values contain the entries of the data set. one way to implement this process is represented below in the function. let's see if the function works properly by passing as into its argument. {0: array, 1: array} great! as expected, is a dictionary whose keys represent the class and values contain entries corresponding to that class. now that we have successfully separated out the data by class, its' time to write a function that will find the mean and standard deviation of each class data. this process is legitimate only because we assumed the data to be normally distributed hence the name "gaussian naive bayes." let's quickly see this in code. the function receives a data set as input and returns a nested list that contains the mean and standard deviation of each column of the data set. for example, if we pass the toy data set into , the returned list will contain two lists: the first list element corresponding to the mean and standard deviation of , and the second list element, . 4.8327021465, 2.6051838419314794, 2.4053928934, 1.1629554833322375 we can combine both and functions to create a new wrapper function that returns the mean and standard deviation of each column for each class. this is a crucial step that will allow us to perform a map approximation for the distribution of variables for each class. testing out the function on created earlier yields the desired result. notice that the returned dictionary contains information for each class, where the key corresponds to the label and the value contains the parameters calculated from . {0: 2.4190554339999997, 0.833648422388659, 2.8336963796, 0.8664248811868022, 1: 7.246348858999999, 1.1079779342778044, 1.9770894072, 1.2599012203753612} a good way to understand this data is through visualization. let's try to visualize what the distribution of and looks like for data labeled class . we can use the library to create a joint plot of the two random variables. we see that both variables are normally distributed. therefore, we can imagine data points for class to be distributed across a three dimensional gaussian distribution whose center lies at the point where the the plot has the darkest color, i.e. . i find this way of understanding data to be highly intuitive in this context. now, it's time to bake bayesian philosophy into code. recall that to perform bayesian analysis, we first need to specify a prior. although we could use an uninformed prior, in this case, we have data to work with. the way that makes the most sense would be to count the number of data points corresponding to each class to create a categorical distribution and use that as our prior, as shown below. if we use the created from , we should get a very simple prior whereby since there is an equal number of data points belonging to the two classes in the toy data set. indeed, this seems to be true. {0: 0.5, 1: 0.5} next, it's time to model the likelihood function. i won't get into the specifics of this function, but all it does is that it calculates the likelihood by using the parameters returned by the function to indicate the likelihood that a particular belongs to a certain class. as per convention of this tutorial, the returned dictionary has keys corresponding to each class and values indicating the likelihood that the belongs to that class. we can see the function in action by passing a dummy test instance. {0: 2.379134694332673e 16, 1: 0.010520187742829746} we are almost done! all that we have to do is to create a funcition that returns the predicted label of a testing instance given some labeled training data. implemenitng this process is straightforward since we have all the bayesian ingredients we need, namely the prior and the likelihood. the last step is to connect the dots with bayes' theorem by calculating the product of the prior and likelihood for each class, then return the class label with the largest posterior, as illustrated below. let's see if the works as expected by seeing if passing as argument , for which we know that its label is 1, actually returns 1. 1 the function is only able to process a single testing instance. let's complete our model construction by writing the function that takes labeled data and a testing set as its argument to return a array containing the predicted class labels for each instance in the testing set. done! let's import some data from the library. the wine set data is a classic multi class classfication data set. the data set contains three target classes, labeled as integers from 0 to 2, and thirteen feature columns, listed below: alcohol malic acid ash alcalinity of ash magnesium total phenols flavanoids nonflavanoid phenols proanthocyanins color intensity hue od280/od315 of diluted wines proline as i am not a wine afficionado, i have no idea what some of these columns represent, but that is irrelevant to the purpose of this tutorial. let's jump right in by loading the data. it always a good idea to get a sense of what the data looks like by verifying its dimension. note that we used the function we wrote in previous posts to shuffle and slice the data into training and validation sets. for convenience, the code for this function is presented below. now it's finally time to check our model by making predictions. this can simply be done by passing the training and testing data set into the function that represented our gaussian naive bayes model. array let's check if the predicted class labels match the answer key, i.e. the array. array eyeballing the results, it seems like we did reasonably well! in fact, the line below tells us that our model mislabeled only one test instance! array we can quantify the performance our model through the metric of accuracy. the function does this for us. note that instead of using the for loop approach used in previous posts, this function is more vectorized, making computation less expensive. the shorter code is also an added benefit. 0.9714285714285714 the accuracy of our from scratch model is 97 percent, which is not bad for a start. let's see if the model in outperforms our hand coded model. 0.9714285714285714 the accuray score yielded by is exactly identical to that achieved by our model! looks like the model in scikit learn does exactly what our model does, at least juding from the metric of accuracy. this is surprising, but since we basically followed the bayesian line of reasoning to buid our model, which is what naive bayes really is all about, perhaps this is not as astonishing as it seems. in this post, we built the gaussian naive bayes model from scratch. in the process, we reviewed key concepts such as bayesian inference and maximum a posteriori estimation, both of which are key statistical concepts used in many subdomains of machine learning. hopefully through this tutorial, you gained a better understanding of how gaussian mathematics and bayesian thinking can be used in the context of classification. the true power of naive bayes is not limited to the task of classificaiton, however. in fact, it is used in many fields, most notably natural language processing. perhaps we might look into the possible applications of naive bayes in the context of nlp in a future post. but for now, this level of modeling will do. thanks for reading. see you in the next post! post: https://jaketae.github.io/study/map mle/ logistic regression: https://jaketae.github.io/study/logistic regression/ k nearest neighbors: https://jaketae.github.io/study/knn/ previous post: https://jaketae.github.io/study/mcmc/