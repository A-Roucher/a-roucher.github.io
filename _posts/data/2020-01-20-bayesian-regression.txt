in today's post, we will take a look at bayesian linear regression. both bayes and linear regression should be familiar names, as we have dealt with these two topics on this blog before. the bayesian linear regression method is a type of linear regression approach that borrows heavily from bayesian principles. the biggest difference between what we might call the vanilla linear regression method and the bayesian approach is that the latter provides a probability distribution instead of a point estimate. in other words, it allows us to reflect uncertainty in our estimate, which is an additional dimension of information that can be useful in many situations. by now, hopefully you are fully convinced that bayesian linear regression is worthy of our intellectual exploration. let's take a deep dive into bayesian linear regression, then see how it works out in code using the library. in this section, we will derive the formula for bayesian linear regression step by step. if you are feeling rusty on linear algebra or bayesian analysis, i recommend that you go take a quick review of these concepts before proceeding. note that i borrowed heavily from this video for reference. for any regression problem, we first need a data set. let denote this pre provided data set, containing entries where each entry contains an dimensional vector and a corresponding scalar. concretely, where the goal of bayesian linear regression is to find the predictive posterior distribution for . this is where the difference between bayesian linear regression and the normal equation method becomes most apparent. whereas vanilla linear regression only gives us a single point estimate given an input vector, bayesian linear regression gives an entire distribution. for the purposes of our demonstration, we will define the predictive posterior to take the following form as shown below, with precision pre given. precision is simply the reciprocal of variance and is commonly used as an alternative way of parametrizing gaussian distributions. in other words, we assume the model our goal will be to derive a posterior for this distribution by performing bayesian inference on , which corresponds to the slope of the linear regression equation, where denotes noise and randomness in the data, thus affecting our final prediction. to begin bayesian inference on parameter , we need to specify a prior. our uninformed prior will look as follows. where denotes precision, the inverse of variance. note that we have a diagonal covariance matrix in place of variance, the distribution for will be a multivariate gaussian. the next ingredient we need for our recipe is the likelihood function. recall that likelihood can intuitively be understood as an estimation of how likely it is to observe the given data points provided some parameter for the true distribution of these samples. the likelihood can easily be computed by referencing back to equation above. note that the dot product of with itself yields the sum of the exponents, which is precisely the quantity we need when computing the likelihood. where is a design matrix given by and is a column vector given by before calculating the posterior, let's recall what the big picture of bayesian inference looks like. where denotes the parameter of interest for inference. in plain terms, the proposition above can be written as in other words, the posterior distribution can be obtained by calculating the product of the prior distribution and the likelihood function. in many real world cases, this process can be intractable, but because we are dealing with two gaussian distributions, the property of conjugacy ensures that this problem is not only tractable, but also that the resulting posterior would also be gaussian. although this may not be immediately apparent, observe that the exponent is a quadratic that follows the form after making appropriate substitutions therefore, we know that the posterior for is indeed gaussian, parameterized as follows: let's try to obtain the map estimate of of , i.e. simplify notice the similarity with the mle estimate, which is the solution to normal equation, which i otherwise referred to as vanilla linear regression: this is no coincidence: in a previous post on map and mle, we observed that the map and mle become identical when we have a uniform prior. in other words, the only cause behind the divergence between map and mle is the existence of a prior distribution. we can thus consider the additional term in absent in as a vestige of the prior we defined for . map versus mle is a recurring theme that appears throughout the paradigmatic shift from frequentist to bayesian, so it merits discussion. now that we have a posterior distribution for which we can work with, it's time to derive the predictive distribution. we go about this by marginalizing using the property of conditional probability, as illustrated below. this may seem like a lot, but most of it was simple calculation and distributing vectors over parentheses. it's time to use the power of conjugacy again to extract a normal distribution out of the equation soup. let's complete the square of the exponent according to the gaussian form after making the appropriate substitutions again, observing this is not a straightforward process, especially if we had no idea what the final distribution is going to look like. however, given that the resulting predictive posterior will take a gaussian form, we can backtrack using this knowledge to obtain the appropriate substitution parameters in . continuing, where the last equality stands because we can pull out terms unrelated to by considering them as constants. why do we bother to pull out the exponent? this is because the integral of a probability density function evaluates to 1, leaving us only with the exponential term outside the integral. to proceed further from here, let's take some time to zoom in on for a second. substituting , we get we can now plug this term back into as shown below. although it may seem as if we made zero progress by unpacking , this process is in fact necessary to complete the square of the exponent according to the gaussian form after making the substitutions by now, you should be comfortable with this operation of backtracking a quadratic and rearranging it to complete the square, as it is a standard operation we have used in multiple parts of this process. finally, we have derived the predictive distribution in closed form: with more simplification using the , it can be shown that and there's the grand formula for bayesian linear regression! this result tells us that, if we were to simply get the best point estimate of the predicted value , we would simply have to calculate , which is the tranpose product of the map estimate of the weights and the input vector! in other words, the answer that bayesian linear regression gives us is not so much different from vanilla linear regression, if we were to reduce the returned predictive probability distribution into a single point. but of course, doing so would defeat the purpose of performing bayesian inference, so consider this merely an intriguing food for thought. as promised, we will attempt to visualize bayesian linear regression using the library. doing so will not only be instructive from a perspective of honing probabilistic programming skills, but also help us better understand and visualize bayesian inference invovled in linear regression as explored in the context of this article. note that, being a novice in , i borrowed heavily from this resource available on the official documentation. first, let's begin by importing all necessary modules. let's randomly generate two hundred data points to serve as our toy data set for linear regression. below is a simple visualization of the generated data points alongside the true line which we will seek to approximate through regression. now is the time to use the library. in reality, all of the complicated math we combed through reduces to an extremely simple, single line command shown below. under the hood, the using variations of random sampling to produce an approximate estimate for the predictive distribution. auto assigning nuts sampler... initializing nuts using jitter+adapt_diag... multiprocess sampling nuts: sd, x, intercept sampling 2 chains: 100%|██████████| 7000/7000 00:04 we see two lines for each plot because the sampler ran over two chains by default. what do those sampled values mean for us in the context of linear regression? well, let's plot some sampled lines using the function conveniently made available through the library. we see that the gray lines, sampled by , all seem to be a good estimate of the true regression line, colored in gold. we might also notice that the sampled regression lines seem to stay below the true regression line for smaller values of . this is because we have more samples beneath the true regression line that we have above it. bayesian linear regression is able to account for such variations in data and uncertainty, which is a huge advantage over the simple mle linear regression method. the true power of bayesian linear regression might be summarized as follows: instead of returning just a single line using the mle weight estimate of data, bayesian linear regression models the entire data set to create a distribution of linear functions so to speak, allowing us to sample from that distribution to obtain sample linear regression lines. this is an approach that makes much more sense, since it allows us to take into account the uncertainty in our linear regression estimate. the reason why the normal equation method is unable to capture this uncertainty is that as you might recall from the derivation of the formula for vanilla linear regression the tools we used did not involve any probabilistic modeling. recall that we used only linear algebra and matrix calculus to derive the model for vanilla linear regression. bayesian linear regression is more complicated in that it involves computations with probability density functions, but the end result is of course more rewarding. that's it for today! i hope you enjoyed reading this post. catch you up in the next one. bayes: https://jaketae.github.io/study/bayes/ linear regression: https://jaketae.github.io/study/linear regression/ video: https://www.youtube.com/watch?v=dtkgq9tdyci&list=pld0f06aa0d2e8ffba&index=59 conjugacy: https://en.wikipedia.org/wiki/conjugate_prior map and mle: https://jaketae.github.io/study/map mle/ resource: https://docs.pymc.io/notebooks/glm linear.html multivariate gaussian: https://jaketae.github.io/study/gaussian distribution/