in this post, we will continue our journey with the r programming language. in the last post, we explored some basic plotting functions and how to use them to visualize data. in this post, we will take a break from commands relating to visualization and instead focus on some tools for statistical analysis on distributions. let’s jump right in. binomial distribution ===================== let’s consider the simple example of a fair coin toss, just because we are uncreative and like to recycle overused examples in textbooks. the function is used to calculate the probability desntiy function . for example, we might compute as follows: dbinom we can also use slicing to obtain , if that quantity is ever an interest to us. sum) notice that the function was used in order to aggregate all the values in the returned list. as you can see, this is one way one might go about calculating the cumulative mass function. cdfs cannot be computed in this fashion since slicing of integers from cannot be used for continuous random variables. there is a more direct way to calculate the cmf right away without using the command, and that is . here is a simple demonstration. pbinom as expected we get the same exact value. the argument tells r that we want values lesser or equal to 10, inclusive. quite similarly, we can also use as a quantile function, which can be considered as the inverse of in the sense that it gives us a value instead of a value. qbinom the and commands we have looked so far dealt with probability mass functions of the binomial distribution. but what if we want to sample a random variable from this distribution, say to perform some sort of monte carlo approximation? this can be achieved with the command. rbinom note that this is a simulation of the binomial random variable, not bernoulli. since we specified , we get five numbers. if we repeat this many times, it turns into a very primitive form of monte carlo simulation. hist, main='binomial distribution', las=1) normal distribution =================== one useful pattern to realize is that the designers of r were very systematic: they didn’t name functions out of arbitrary whim. instead, there is a set pattern, where it strictly adheres to the form , where is a single character, one of , , , or . here is a quick rundown of what each of these characters signify: : pdf or pmf : cdf or cmf : inverse cdf or cmf : random sampling from distribution given this piece of information, perhaps it’s unsurprising that the commands for the normal distribution are , , , and . let’s start with the first one on the list, . dnorm recall that the equation for a univariate standard normal distribution is given by if you plug in x = 0 into this equation, you will see that the value returned by the function, which is simply the normalizing constant, is indeed approximately 0.39. in short, represents the pdf of the normal distribution. the next on the list is , which we already know models the gaussian cdf. this can easily be verified by the fact that pnorm this is expected behavior, since corresponds to the exact mid point of the gaussian pdf. z_scores as a bite sized exercise, let’s try to take a look at the empirical rule of the normal distribution, namely that values within one standard deviation from the mean cover roughly 68% of the entire distribution. pnorm pnorm we could have also used the argument, which defines in which direction we calcalate the cdf. if is set to , then the function returns the integral from to infinity of the pdf of the normal distribution. 1 pnorm pnorm the is best understood as the inverse cdf function. this means that the function would receive as input the value of the area under the function, which can also be interpreted as the score. qnorm we can directly verify the fact that is an inverse of by pluggin in a value. pnorm) last but not least, the function can be used to sample values from the normal distribution with the specified parameters. let’s start by sampling 10 values from the standard normal distribution. rnorm we can also set the seed to make sure that results are replicable. the command does not return anything; it merely sets the seed for the current thread. set.seed poisson distribution ==================== recall that a poisson distribution is used to model the probability of having some number of events occuring within a window of unit time given some rate parameter λ. suppose that the phenomenon we’re modeling has an average rate of 7. if we want to know the probability , we can use the function to calculate the pmf: lambda it’s certainly not exactly a gaussian, but at least it does not unimodal and symmetric. if lln and clt is true, then we already know the mean and variance of this normal distribution: the mean is simply the rate parameter, 7, and the variance can be calculated as let’s see if these are indeed true. first, we can verify the mean of the sample means via mean that is sure enough close to 7, as we would expect. then, there’s variance: sd^2 lambda/n and indeed, it seems like the results match. note that the second calculation is based on the results in . note that in specifying , we didn’t have to specify a range by creating a or using slicing; instead, r is able to understand that is a variable. hist curve), add=t) the result seems to quite strongly vindicate clt and lln, as expected. conclusion ========== today’s post introduced some very useful functions relating to probability distributions. the more i dive into r, the more i’m amazed by how powerful r is as a statistical computing language. while i’m still trying to wrap my head around some of r’s quirky syntax , but this minor foible is quickly offset by the fact that it offers powerful vectorization, simulating, and sampling features. i love , but r just seems to do a bit better in some respects. in the next post, we will be taking a look at things like the t distribution and hypothesis testing. stay tuned for more!