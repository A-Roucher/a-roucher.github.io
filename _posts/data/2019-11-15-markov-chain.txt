in a previous post, we briefly explored the notion of markov chains and their application to google's pagerank algorithm. today, we will attempt to understand the markov process from a more mathematical standpoint by meshing it together the concept of eigenvectors. this post was inspired and in part adapted from this source. in linear algebra, an eigenvector of a linear transformation is roughly defined as follows: a nonzero vector that is mapped by a given linear transformation onto a vector that is the scalar multiple of itself this definition, while seemingly abstract and cryptic, distills down into a simple equation when written in matrix form: here, denotes the matrix representing a linear transformation; , the eignevector; , the scalar value that is multiplied onto the eigenvector. simply put, an eigenvector of a linear transformation is one that is allow me to use this term in the loosest sense to encompass positive, negative, and even imaginary scalar values "stretched" by some factor when the transformation is applied, i.e. multiplied by the matrix which maps the given linear transformation. the easiest example i like to employ to demonstrate this concept is the identity matrix . for the purpose of demonstration, let be an arbritrary vector and the three by three identity matrix. multiplying by produces the following result: the result is unsurprising, but it reveals an interesting way of understanding : identity matrices are a special case of diagonalizable matrices whose eigenvalues are 1. because the multiplying any arbitrary vector by the identity matrix returns the vector itself, all vectors in the dimensional space can be considered an eigenvector to the matrix , with = 1. a formal way to calculate eigenvectors and eigenvalues can be derived from the equation above. since is assumed as a nonzero vector, we can deduce that the matrix is a singular matrix with a nontrivial null space. in fact, the vectors in this null space are precisely the eigenvectors that we are looking for. here, it is useful to recall that the a way to determine the singularity of a matrix is by calculating its determinant. using these set of observations, we can modify the equation above to the following form: by calculating the determinant of , we can derive the characteristic polynomial, from which we can obtain the set of eigenvectors for representing some linear transformation . now that we have reviewed some underlying concepts, perhaps it is time to apply our knowledge to a concrete example. before we move on, i recommend that you check out this post i have written on the markov process, just so that you are comfortable with the material to be presented in this section. in this post, we turn our attention to the game of chutes and ladders, which is an example of a markov process which demonstrates the property of "memorylessness." this simply means that the progress of the game depends only on the players' current positions, not where they were or how they got there. a player might have ended up where they are by taking a ladder or by performing a series of regular dice rolls. in the end, however, all that matters is that the players eventually hit the hundredth cell. figure 1: chutes and ladders game to perform a markov chain analysis on the chutes and ladders game, it is first necessary to convert the information presented on the board as a stochastic matrix. how would we go about this process? let's assume that we start the game at the th cell by rolling a dice. there are six possible events, each with probability of . more specifically, we can end up at the index numbers 38, 2, 3, 14, 5, or 6. in other words, at position 0, where and denote the current and next position of the player on the game board, respectively. we can make the same deductions for other cases where . we are thus able to construct a 101 by 101 matrix representing the transition probabilities of our chutes and ladders system, where each column represents the system at a different state, i.e. the th entry of the th column vector represents the probabilities of moving from cell to cell . to make this more concrete, let's consider a program that constructs the stochastic matrix , without regards to the chutes and ladders for now. the indexing is key here: for each column, th rows were assigned the probability of . let's say that a player is in the th cell. assuming no chutes or ladders, a single roll of a dice will place him at one of the cells from to ; hence the indexing as presented above. however, this algorithm has to be modified for bigger or equal to 95. for example if , there are only three probabilities: , , and , each of values , , and respectively. the statements are additional corrective mechanisms to account for this irregularity. so now we're done with the stochastic matrix! ... or not quite. things get a bit more complicated once we throw the chutes and ladders into the mix. to achieve this, we first build a dictionary containing information on the jump from one cell to another. in this dictionary, the keys correspond to the original position; the values, the index of the cell after the jump, either through a chute or a ladder. for example, represents the first ladder on the game board, which moves the player from the first cell to the thirty eighth cell. to integrate this new piece of information into our code, we need to build a permutation matrix that essentially "shuffles up" the entries of the stochastic matrix in such a way that the probabilities can be assigned to the appropriate entries. for example, does not reflect the fact that getting a 1 on a roll of the dice will move the player up to the thirty eighth cell; it supposes that the player would stay on the first cell. the new permutation matrix would adjust for this error by reordering . for an informative read on the mechanics of permutation, refer to this explanation from wolfram alpha. let's perform a quick sanity check to verify that contains the right information on the first ladder, namely the entry in the dictionary. notice the in the th entry hidden among a haystack of 100 s! this result tells us that is indeed a permutation matrix whose multiplication with will produce the final stochastic vector that correctly enumerates the probabilities encoded into the chutes and ladders game board. here is our final product: we can visualize the stochastic matrix using the package. this produces a visualization of our stochastic matrix. figure 2: visualization of the stochastic matrix so there is our stochastic matrix! now that we have a concrete matrix to work with, let's start by identifying its eigenvectors. this step is key to understanding markov processes since the eigenvector of the stochastic matrix whose eigenvalue is 1 is the stationary distribution vector, which describes the markov chain in a state of equilibrium. for an intuitive explanation of this concept, refer to this previous post. let's begin by using the package to identify the eigenvalues and eigenvectors of the stochastic matrix. this code block produces the following output: the first entry of this array, which is the value , deserves our attention, as it is the eigenvalue which corresponds to the stationary distribution eigenvector. since the index of this value is , we can identify its eigenvector as follows: notice that this eigenvector is a representation of a situation in which the player is in the th cell of the game board! in other words, it is telling us that once the user reaches the th cell, they will stay on that cell even after more dice rolls hence the stationary distribution. on one hand, this information is impractical given that a player who reaches the end goal will not continue the game to go beyond the th cell. on the other hand, it is interesting to see that the eigenvector reveals information about the structure of the markov chain in this example. markov chains like these are referred to as absorbing markov chains because the stationary equilibrium always involves a non escapable state that "absorbs" all other states. one might visualize this system as having a loop on a network graph, where it is impossible to move onto a different state because of the circular nature of the edge on the node of the absorbing state. at this point, let's remind ourselves of the end goal. since we have successfully built a stochastic matrix, all we have to do is to set some initial starting vector and perform iterative matrix calculations. in recursive form, this statement can be expressed as follows: the math inclined thinkers in this room might consider the possibility of conducting an eigendecomposition on the stochastic matrix to simply the calculation of matrix powers. there is merit to considering this proposition, although later on we will see that this approach is inapplicable to the current case. eigendecomposition refers to a specific method of factorizing a matrix in terms of its eigenvalues and eigenvectors. let's begin the derivation: let be the matrix of interest, a matrix whose columns are eigenvectors of , and , a matrix whose diagonal entries are the corresponding eigenvalues of . let's consider the result of multiplying and . if we view multiplication as a repetition of matrix times vector operations, we yield the following result. but recall that are eigenvectors of , which necessarily implies that therefore, the result of can be rearranged and unpacked in terms of : in short, therefore, we have , which is the formula for eigendecomposition of a matrix. one of the beauties of eigendecomposition is that it allows us to compute matrix powers very easily. concretely, because and nicely cross out, all we have to compute boils down to ! this is certainly good news for us, since our end goal is to compute powers of the stochastic matrix to simulate the markov chain. however, an important assumption behind eigendecomposition is that it can only be performed on nonsingular matrices. although we won't go into the formal proofs here, having a full span of independent eigenvectors implies full rank, which is why we must check if the stochastic matrix is singular before jumping into eigendecomposition. unfortunately, the stochastic matrix is singular because , the number of columns or rows. this implies that our matrix is degenerate, and that the best alternative to eigendecomposition is the singular value decomposition. but for the sake of simplicity, let's resort to the brute force calculation method instead and jump straight into some statistical analysis. we first write a simple function that simulates the chutes and ladders game given a starting position vector . because a game starts at the th cell by default, the function includes a default argument on as shown below: calling this function will give us , which is a 101 by 1 vector whose th entry represents the probability of the player being on the th cell after a single turn. now, we can plot the probability distribution of the random variable , which represents the number of turns necessary for a player to end the game. this analysis can be performed by looking at the values of since the last entry of this vector encodes the probability of the player being at the th cell, i.e. successfully completing the game after rounds. this block produces the following figure: figure 3: game completion percentage after n turns i doubt that anyone would play chutes and ladders for this long, but after about 150 rolls of the dice, we can expect with a fair amount of certainty that the game will come to an end. the graph above presents information on cumulative fractions, but we can also look at the graph for marginal probabilities by examining its derivative: and the result: figure 3: fraction of games completed at n turns from the looks of it, the maximum of the graph seems to exist somewhere around . to be exact, . this result tells us that we will finish the game in 19 rolls of the dice more often than any other number of turns. we can also use this information to calculate the expected value of the game length. recall that or if the probability density function is continuous, in this case, we have a discrete random variable, so we adopt the first formula for our analysis. the formula can be achieved in python as follows: this result tells us that the typical length of a chutes and ladders game is approximately 36 turns. but an issue with using expected value as a metric of analysis is that long games with infinitesimal probabilities are weighted equally to short games of substantial probability of occurrence. this mistreatment can be corrected for by other ways of understanding the distribution, such as median: this function tries to find the point in the cumulative distribution where the value is closest to , i.e. the median of the distribution. the result tells us that about fifty percent of the games end after 29 turns. notice that this number is smaller than because it discredits more of the long games with small probabilities. the markov chain represents an in interesting way to analyze systems that are memoryless, such as the one in today's post, the chutes and ladders game. although it is a simple game, it is fascinating to see just how much information and data can be derived from a simple image of the game board. in a future post, we present another way to approach similar systems, known as monte carlo simulations. but that's for another time. peace! this source: https://jakevdp.github.io/blog/2017/12/18/simulating chutes and ladders/ previous post: https://jaketae.github.io/blog/math/pagerank and markov/ this post: https://jaketae.github.io/blog/math/pagerank and markov/ defined: https://en.wikipedia.org/wiki/eigenvalues_and_eigenvectors stochastic matrix: http://mathworld.wolfram.com/stochasticmatrix.html explanation from wolfram alpha: http://mathworld.wolfram.com/permutationmatrix.html memorylessness: https://en.wikipedia.org/wiki/markov_property characteristic polynomial: http://mathworld.wolfram.com/characteristicpolynomial.html eigendecomposition: https://en.wikipedia.org/wiki/eigendecomposition_of_a_matrix singular value decomposition: https://en.wikipedia.org/wiki/singular_value_decomposition absorbing markov chains: https://en.wikipedia.org/wiki/absorbing_markov_chain