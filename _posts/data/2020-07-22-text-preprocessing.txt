in today's post, we will finally start modeling the auto tagger model that i wanted to build for more blog. as you may have noticed, every blog post is classified into a few different tags, which are essentially topic keywords that i have been manually assigning thus far. i have been getting increasingly lazier these past few weeks, which is ironically what compelled me into experimenting and studying more about the basics of nlp. as mentioned in previous posts, this is why i've been posting things like tf idf vectorization and word embeddings. while there are so many sota models out there, for the purposes of this mini project, i decided to go slow. in what may or may not become a little series of its own, i aspire to achieve the following: design a basic parsing algorithm to clean blog posts in markdown vectorize the cleaned string data build a target vector corresponding to each blog post construct and train a document classifier develop a pipeline to generate and display the model's predictions this is by no means a short, easy project for me. this is also the first time that i'm dealing with real data data created by no one other than myself so there is an interesting element of meta ness to it that i enjoy. after all, this very post that i'm writing will also be a valuable addition to the training set for the model. with that said, let's jump right into it. the first challenge is cleaning the textual data we have. by cleaning, i don't mean vectorizing; instead, we need to first retrieve the data, get rid of extraneous characters, code blocks, and mathjax expressions, and so on. after all, our simple model cannot be expected to understand code blocks or latex expressions, as awesome as that sounds. there were two routes i could take with parsing. the first was web scraping; the second, using directly parsing raw markdown files. i'll detail each attempts i've made in the following sections, then explain why i chose one approach over the other. because all my published posts are available on my blog website, i could crawl the blog and extract tags to construct my training dataset. here are some of the steps i took while experimenting with this approach. for demonstration purposes, i decided to use a recent post on gaussian process regression, as it contains a nice blend of both code and mathjax expressions. now we can take a look at the first tag in the web scraped list. in this post, we will explore the gaussian process in the context of regression. this is a topic i meant to study for a long time, yet was never able to due to the seemingly intimidating mathematics involved. however, after consulting some extremely well curated resources on this topic, such as kilian’s lecture notes and ubc lecture videos by nando de freitas, i think i’m finally starting to understand what gp is. i highly recommend that you check out these resources, as they are both very beginner friendly and build up each concept from the basics. with that out of the way, let’s get started. this is not a bad starting point, but obviously there is so much more work that has to be done. for one thing, we need to remove tags that are often wrapped around tags. we also have to remove inline latex expressions, which as written as . below is a function that that i wrote to clean the data with the following considerations in mind. for demonstration purposes, let's try scraping the post on gp regression i mentioned earlier. 'ution of the predicted data at a given test point. gaussian processes are similar to bayesian linear regression in that the final result is a distribution which we can sample from. the biggest point of difference between gp and bayesian regression, however, is that gp is a fundamentally non parametric approach, whereas the latter is a parametric one. i think this is the most fascinating part about gps—as we will see later on, gps do not require us to specify any function or model to fit the data. instead, all we need to do is to identify the mean and covariance of a multivariate gaussian that defines the posterior of the gp. all of this sounds too good be true—how can a single multivariate gaussian distribution be enough for what could potentially be a high dimensional, complicated regression problem? let’s discuss some mathematical ideas that enable gp to be so powerful. gaussians are essentially a black hole of distributions: once a gaussian, always a gaussian. for example, we ' we see that the text has indeed been parsed, which is great! so we have the basic tools to parse a post given a url. so naturally, the next step would be to figure out all the urls for the blog posts i have on my website. of course, i could do this manually, but that sort of defeats the point of building an auto tagger. so after some trial and error, here is another function i wrote that scrapes all blog post urls on my website. we start from the root url, then basically extract hrefs from the elements that each represent a single blog post. 'https://jaketae.github.io/development/tinkering docker/', 'https://jaketae.github.io/study/word2vec/', 'https://jaketae.github.io/study/complex fibonacci/', 'https://jaketae.github.io/study/tf idf/', 'https://jaketae.github.io/study/gaussian process/', 'https://jaketae.github.io/study/genetic algorithm/', 'https://jaketae.github.io/study/revisiting basel/', 'https://jaketae.github.io/study/zeta prime/', 'https://jaketae.github.io/study/bfs dfs/', 'https://jaketae.github.io/study/numerical methods/', 'https://jaketae.github.io/study/gibbs sampling/', 'https://jaketae.github.io/development/sklearn sprint/', 'https://jaketae.github.io/study/spark basics/', 'https://jaketae.github.io/study/dissecting lstm/', 'https://jaketae.github.io/study/sklearn pipeline/', 'https://jaketae.github.io/study/natural gradient/', 'https://jaketae.github.io/blog/workflow cleanup/', 'https://jaketae.github.io/study/r tutorial 4/', 'https://jaketae.github.io/study/sql basics/', 'https://jaketae.github.io/study/r tutorial 3/', 'https://jaketae.github.io/development/c/', 'https://jaketae.github.io/study/leibniz rule/', 'https://jaketae.github.io/study/r tutorial 2/', 'https://jaketae.github.io/study/r tutorial 1/', 'https://jaketae.github.io/study/fisher/', 'https://jaketae.github.io/study/stieltjes/', 'https://jaketae.github.io/study/stirling/', 'https://jaketae.github.io/study/pca/', 'https://jaketae.github.io/study/fourier/', 'https://jaketae.github.io/study/gan math/', 'https://jaketae.github.io/study/kl mle/', 'https://jaketae.github.io/study/development/open source/', 'https://jaketae.github.io/study/development/flask/', 'https://jaketae.github.io/study/gan/', 'https://jaketae.github.io/study/vae/', 'https://jaketae.github.io/study/autoencoder/', 'https://jaketae.github.io/study/auto complete/', 'https://jaketae.github.io/study/rnn/', 'https://jaketae.github.io/study/neural net/', 'https://jaketae.github.io/study/cnn/', 'https://jaketae.github.io/blog/typora/', 'https://jaketae.github.io/study/map convex/', 'https://jaketae.github.io/study/exponential family/', 'https://jaketae.github.io/study/bayesian regression/', 'https://jaketae.github.io/study/naive bayes/', 'https://jaketae.github.io/study/first keras/', 'https://jaketae.github.io/study/r tutorial/', 'https://jaketae.github.io/development/anaconda/', 'https://jaketae.github.io/study/mcmc/', 'https://jaketae.github.io/study/logistic regression/', 'https://jaketae.github.io/study/map mle/', 'https://jaketae.github.io/study/knn/', 'https://jaketae.github.io/study/information entropy/', 'https://jaketae.github.io/study/moment/', 'https://jaketae.github.io/study/gaussian distribution/', 'https://jaketae.github.io/study/svd/', 'https://jaketae.github.io/study/linear regression/', 'https://jaketae.github.io/study/monte carlo/', 'https://jaketae.github.io/study/likelihood/', 'https://jaketae.github.io/blog/jupyter automation/', 'https://jaketae.github.io/study/bayes/', 'https://jaketae.github.io/blog/test/', 'https://jaketae.github.io/study/basel zeta/', 'https://jaketae.github.io/study/gamma/', 'https://jaketae.github.io/study/poisson/', 'https://jaketae.github.io/study/eulers identity/', 'https://jaketae.github.io/study/markov chain/', 'https://jaketae.github.io/tech/new mbp/', 'https://jaketae.github.io/study/pagerank and markov/', 'https://jaketae.github.io/blog/studying deep learning/' i knew that i had been writing somewhat consistently for the past few months, but looking at this full list made me realize how fast time has flown by. continuing with our discussion on cleaning data, now we have all the basic tools we need to build our training data. in fact, we can simply build our raw strings training data simply by looping over all the urls and extracting text from each: at this point, you might be wondering why i even attempted a second approach, given that these methods all work fine. the answer is that, although web scraping works okay and we could certainly continue with this approach but we would have to build a text parser anyway. think about it: although we can build the training data through web scraping, to run the actual inference, we need to parse the draft, in markdown format, that has not been published yet. in other words, we have no choice but to deal with markdown files, since we have to parse and clean our draft to feed into the model. it is after this belated realization that i started building a parser. now, the interesting part is that i tried two different approachdes going down this road as well. so really, the accurate description would be that i tried three different methods. the first sub approach was the one i first thought about, and is thus naturally the more naive method of the two. this is simply an adaptation of the algorithm used in the function, involving a boolean variable that would switch on and off as we loop through the words, switching whenever we see a delimiter like restr.replace""?\n \n_postsnltk.mdstop_wordsstem_tokenizer.ipynb.md.md` file. this shouldn't be too difficult, but nonetheless it is an important detail that would be very cool to implement once we have a finalized model. it would be even better if we could implement some mechanism to train our model with new data per each blog post upload; after all, we don't want to use an old model trained with old data. instead, we want to feed it with new data so that it is able to learn more. hopefully we'll find a way to tackle these considerations as we move forward. catch you up in the next post!