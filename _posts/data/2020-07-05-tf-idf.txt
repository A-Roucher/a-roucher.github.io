although i've been able to automate some portion of the blog workflow, there's always been a challenging part that i wanted to further automate myself using deep learning: automatic tagging and categorization. every post requires some form of yaml front matter, containing information such as the title, tag, and category of the blog post to be uploaded. although i sometimes create new tags or categories if existing ones seem unfit, i only deal with a limited number of topics on this blog, which is why i've always thought that some form of supervised learning be able to automate the process by at least generating some possible tags for me. i'm currently in the process of preparing the data and building a simple nlp document classification model for the job. however, nlp is a field that i'm admittedly not well acquainted with, not to mention the fact that i've not bee posting a lot about deep learning implementations for a while now. so in today's short post, i decided to write about tf idf vectorization, which is a very simple yet powerful technique that is often used in routine tasks like document classification, where sota models aren't really required. as always, this post is going to take a hands on approach by demonstrating a simple way of implementing tf idf vectorization from scratch. let's get started. tf idf stands for term frequency inverse document frequency. this is all there is to it in fact, the formula for tf idf can simply be expressed as where denotes a single term; , a singe document, and , a collection of documents. so simply put, tf idf is simply a product of the term frequency, denoted above as , and inverse document frequency, . all there is left, then, is to figure out what term frequency and inverse document frequency are. without much explanation, you can probably guess what term frequency is: it simply indicates how frequently a word appeared in a given document. for example, if there were a total of 3 distinct words in a document , then each of the three words would have a tf score of . put differently, the sum of the tf vector for each document should sum to one. the definition of a tf score might be thus expressed as where the denominator denotes the count of all occurrences of the term in document , and the numerator represents the total number of terms in the document. roughly speaking, inverse document frequency is simply the reciprocal of document frequency. therefore, it suffices to show what document frequency is, since idf would immediately follow from df. before getting into the formula, i think it's instructive to consider the motivation behind tf idf, and in particular what role idf plays in the final score. the motivation behind tf idf commences from a simple question: how do we determine the semantic importance of a word in a set of documents? on one hand, words the appear a lot are probably worth paying attention to. for example, in one of my posts on gaussian distributions, the word "gaussian" probably appears many times throughout the post. a keyword probably appears frequently in the document; hence the need to calculate tf. on the other hand, there might be words that appear a lot, but aren't really that important at all. for example, consider the word "denote." i know that i use this word a lot before writing down equations or formulas, just for the sake of notational clarity. however, the word itself carries little information on what the post is about. the same goes for other words, such as "example," "however," and so on. so term frequency only doesn't really tell us much; instead, we want to pay attention to words that occur frequently in a given document, but doesn't appear a lot in others such words are most likely to be unique keywords that potentially capture the gist of that document. given this analysis, it isn't difficult to see why tf idf is designed the way it is. although we give priority weight to words with high term frequency, we discount words that appear frequently across all documents by dividing tf by idf, or inverse document frequency. in short, document frequency tells us how frequently a given word appears throughout all documents; the inverse is the reciprocal of that quantity. however, this is a mere technically; the intuition we motivated earlier still applies regardless. with these ideas in mind, let's go implement tf idf vectorization in python! in this section, we will develop a simple set of methods to convert a set of raw documents to tf idf vectors, using a dummy dataset. below are four documents that we will be using throughout this tutorial. the first step is to preprocess and tokenize the data. although the specifics of preprocessing would probably differ from task to task, in this simple example, we simply remove all punctuations, change documents to lower case letters, and tokenize them by breaking down documents into a bag of words. other possible techniques not discussed here include stemming and lemmatization. the function accepts as input a set of documents and removes all the punctuation in each document. here is the result of applying our function to the dummy data. 'tom plays soccer', 'tom loves basketball', 'basketball is his hobby', 'sarah loves basketball' next, we need to tokenize the strings by splitting them into words. in this process, we will also convert all documents to lower case as well. note that works on each documents, not the entire collection. let's try calling the function with the first document in our dummy example. 'tom', 'plays', 'soccer' finally, as part of the preprocessing step, let's build the corpus. the corpus simply refers to the entire set of words in the dataset. specifically for our purposes, the corpus will be a dictionary whose keys are the words and values are an ordinal index. another way to think about the corpus in this context is to consider it as a word to index mapping. we will be using the indices to represent each word in the tf, idf, and tf idf vectors later on in the tutorial. because we have a very simple example, our corpus only contains 9 words. this also means that our tf idf vectors for each document will also be a list of length 9. thus it isn't difficult to see how tf idf vectorization can result in extremely high dimensional matrices, which is why we often apply techniques such as lemmatization or pca on the final result. also note that published modules use sparse representations to minimize computational load, as we will later see with scikit learn. {'his': 0, 'soccer': 1, 'sarah': 2, 'basketball': 3, 'plays': 4, 'hobby': 5, 'loves': 6, 'tom': 7, 'is': 8} now it's time to implement the first step: calculating term frequency. in python, this simply amounts to looping through each document, creating a tf vector per iteration, and making sure that they are normalized as frequencies at the very end. in creating tf vectors for each document, we will be referencing the word to index mapping in our corpus. let's see what we get for the four documents in our dummy example. 0.0, 0.3333333333333333, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.3333333333333333, 0.0 0.0, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0, 0.3333333333333333, 0.3333333333333333, 0.0 0.25, 0.0, 0.0, 0.25, 0.0, 0.25, 0.0, 0.0, 0.25 0.0, 0.0, 0.3333333333333333, 0.3333333333333333, 0.0, 0.0, 0.3333333333333333, 0.0, 0.0 due to floating point arithmetic, the decimals don't look the most pleasing to the eye, but it's clear that normalization has been performed as expected. also note that we get 4 vectors of length 9 each, as expected. next, it's time to implement the idf portion of the vectorization process. in order to calculate idf, we first need a total count of each number in the entire document collection. a module that is perfect for this job is , which accepts as input an iterable and outputs a dictionary like object whose values represent the count of each key. let's test in on our dummy dataset to see if we get the count of each tokenized word. counter this is precisely what we need to calculate idf. recall the formula for calculating idf as noted earlier, the intuition behind idf was that important keywords probably appear only in specific relevant documents, whereas generic words of comparatively lesser importance appear throughout all documents. we transcribe into code as follows: now, we have the idf vectors for the nine terms in the dummy dataset. 1.916290731874155, 1.916290731874155, 1.916290731874155, 1.2231435513142097, 1.916290731874155, 1.916290731874155, 1.5108256237659907, 1.5108256237659907, 1.916290731874155 at this point, all there is left to do is to multiply the term frequencies with their corresponding idf scores. this is extremely easy, since we are essentially performing a dot product of the tf and idf vectors for each document. as a final step, we normalize the result to ensure that longer documents do not overshadow shorter ones. normalizing is pretty simple, so we'll assume that we have a function that does the job for now. before we test the code, we obviously need to implement . this can simply done by obtaining the sum of the l2 norm of each vector, then dividing each element by that constant. here is an easy contrived example we can do in our heads: 0.6, 0.8 and now we're done! if let's print the tf idf vectors for each of the four documents in the dummy example. 0.0, 0.617614370975602, 0.0, 0.0, 0.617614370975602, 0.0, 0.0, 0.48693426407352264, 0.0 0.0, 0.0, 0.0, 0.496816117482646, 0.0, 0.0, 0.6136667440107332, 0.6136667440107332, 0.0 0.5417361046803605, 0.0, 0.0, 0.3457831381910465, 0.0, 0.5417361046803605, 0.0, 0.0, 0.5417361046803605 0.0, 0.0, 0.7020348194149619, 0.4480997313625987, 0.0, 0.0, 0.5534923152870045, 0.0, 0.0 it seems about right, as all the vectors appear normalized and are of the desired dimensions. however, to really verify the result, it's probably a good idea to pit our algorithm against scikit learn's implementation. in scikit learn, the does all the job. to transform the data to tf idf vectors, we need to create an instance of the and call its method, . and here are the results: 0. 0. 0. 0. 0. 0.61761437 0. 0.61761437 0.48693426 0.49681612 0. 0. 0. 0.61366674 0. 0. 0. 0.61366674 0.34578314 0.5417361 0.5417361 0.5417361 0. 0. 0. 0. 0. 0.44809973 0. 0. 0. 0.55349232 0. 0.70203482 0. 0. there are several observations to be made about this result. first, note that the default return type of is a sparse matrix. sparse matrices are a great choice since many of the entries of the matrix will be zero there is probably no document that contains every word in the corpus. therefore, sparse representations can save a lot of space and compute time. this is why we had to call on the result. second, you might be wondering why the order of elements are different. this is because the way we built ordinal indexing in corpus is probably different from how scikit learn implements it internally. this point notwithstanding, it's clear that the values of each vectors are identical, disregarding the fact that the result produced by our algorithm has more decimal points due to floating point arithmetic. this was a short introductory post on tf idf vectors. when i first heard about tf idf vectors from a friend studying computational linguistics, i was intimidated. however, now that i have a project i want to complete, namely an auto tagging and classification nlp model, i've mustered more courage and motivation to continue my study the basics of nlp. i hope you've enjoyed reading this post. catch you up in the next one!