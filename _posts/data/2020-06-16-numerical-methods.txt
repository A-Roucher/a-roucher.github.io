recently, i ran into an interesting video on youtube on numerical methods . it was a channel called numericalmethodsguy, run by a professor of mechanical engineering at the university of florida. while the videos themselves were recorded a while back in 2009 at just 240p, i found the contents of the video to be very intriguing and easily digestable. his videos did not seem to assume much mathematical knowledge beyond basic high school calculus. after watching a few of his videos, i decided to implement some numerical methods algorithms in python. specifically, this post will deal with mainly two methods of solving non linear equations: the newton raphson method and the secant method. let's dive right into it. before we move on, it's first necessary to come up with a way of representing equations in python. for the sake of simplicity, let's first just consider polynomials. the most obvious, simplest way of representing polynomials in python is to simply use functions. for example, we can express as however, a downside of this approach is the fact that it's difficult to directly derive this equation despite the fact that it is nothing more than just a simple polynomial. so instead, we will use a list index based representation. namely, the th element of a list represents the coefficient of the th power in a polynomial equation. in other words, would translate into . the is a function that returns a python function given a list that conforms to this list index representation. let's see if this works as expected. 7 , so the function passes our quick sanity test. one useful helper function that i also implemented for the sake of convenience is a array to equation parser that translates a list representation into a mathematical expression in python. this is best demonstrated than explained, so i'll defer myself to an example. '1 x 3 20' below is the full definition of the function. at this point, i also thought that it would be useful and interesting to compose a function that translates the string output of into a proper python function we can use to calculate values. below is the function that receives as input some parsed output string and returns a corresponding python function. now, we can do something like this: f = 1 x 3 20 f = 105 now that we have more than enough tools we can use relating to the list index representation we decided to use to represent polynomials, it's time to exploit the convenience that this representation affords us to calculate derivatives. calculating derivatives using the list index representation is extremely easy and convenient: in fact, it can be achieved in just a single line. let's test this function with the example we have been using previously. let's also use the function to make the final result for human readable. f = 1 x 3 20 f' = 3 x 2 seems like the derivative calculation works as expected. in the process, i got a little bit extra and also wrote a function that integrates a function in list index representation format. 27 if we integrate , we end up with , where is the integration constant. excluding the integration constant, we get a result that is consistent with the function. f = 0.25 x 4 20.0 x 1 while it's great that we can calculate derivatives and integrals, one very obvious drawback of this direct approach is that we cannot deal with non polynomial functions, such as exponentials or logarithms. moreover, the list index representation is unable to represent polynomials that include terms whose powers are not positive integers. for these reasons, we will need some other methods of calculating derivatives as well. hence the motivation for approximation methods, outlined in the section below. if you probe the deepest depths of your memory, somewhere you will recall the following equation, which i'm sure all of us saw in some high school calculus class: this equation, commonly known as the definition of a derivative, is also known as the forward divided difference formula of calculating derivatives. there is another variant, known as the backward divided difference formula: and are almost nearly identical, but the difference lies in which term is subtracted from who. in , we go an infinitesimal step forward hence the and subtract the value at the point of approximation, . in , we go backwards, which is why we get . as approaches 0, and asymptotically gives us identical results. below is a python variant of the backward divided difference formula. some tweaks have been made to the formula for use in the section that follows, but at its core, it's clear that the function uses the approximation logic we've discussed so far. another variant of the forward and backward divided difference formula is the center divided difference. by now, you might have some intuition as to what this formula is as the name implies, we somehow use the center point, instead of going forward or backwards from the point of approximation. here is the formula: heuristically, this formula also makes sense. we can imagine going both a step forward and backward, then dividing the results by the total of two steps we've taken, one in each direction. shown below is the python implementation of the center divided difference formula. according to conventional mathematical wisdom, the center divided method normally provides a more robust way of approximating first order derivatives. in this subsection, we discuss why this is the case. using taylor expansion, we can approximate the value of as follows, given that goes to 0 under the limit. notice that we can manipulate to derive the forward divided difference equation in . if we move the term to the lhs, then divide both sides by , we end up with here, we used big o notation to denote the order of magnitude of the trailing terms. the trailing terms are significant since they are directly related to the accuracy of our approximation. an error term of means that, if we halve the step size, we will also halve the error. this is best understood as a linear relationship between error and the step size. we can conduct a similar mode of analysis with backward divided difference. by symmetry, we can express as if we rearrange , we end up with . again, we see that backward divided difference yields linear error, or a trailing term of . here's where things get more interesting: in the case of center divided difference, the magnitude of the error term is , meaning that halving the step size decreases the error by four folds. this is why center divided difference yields much more accurate approximations than forward or backward divided difference. to see this, we subtract from , then move some terms, and divide both sides by . notice that subtracting these two expression results in a lot of term cancellations. dividing both sides by yields from this result, we now know why the central divided difference method provides more accurate approximations of the derivative than do forward and backward divided difference. this is somewhat related to what we will be doing in the next section, so it's a good intuition to have throughout when reading the rest of this article. now that we have these tools for differential calculus, now comes the exciting part: solving non linear equations. specifically, we will be taking a look at two numerical methods: the newton raphson method and the secant method. it's time to put the methods we developed in the preceding sections to use for solving non linear equations. specifically, we'll begin by taking look at a classic algorithm, the newton raphson method. the newton raphson method is one of the many ways of solving non linear equations. the intuition behind the newton raphson method is pretty straightforward: we can use tangent lines to approximate the x intercept, which is effectively the root of the equation . specifically, we begin on some point on the graph, then obtain the tangent line on that point. then, we obtain the intercept of that tangent line, and repeat the process we've just completed by starting on a point on the graph whose value is equal to that intercept. the following image from wikipedia illustrates this process quite well. mathematically, the newton raphson method can be expressed recursively as follows: deriving this formula is quite simple. say we start at a point on the graph, . the tangent line from that point will have a slope of . therefore, the equation of the tangent line can be expressed as then, the intercept can simpy be obtained by finding an value that which makes . let denote that point. then, we arrive at the following update rule. since we will be using as the value for the next iteration, , and now we have the update rule as delineated in . below is an implementation of the newton raphson method in python. i've added some parameters to the function for functionality and customization. is simply some small value we use to decide when to stop the update; if the change in the value of the root is so small that it is not worth the extra compute, we should stop. determines how many iterations we want to continue. if the algorithm is unable to find the root within iterations, it likely means that the function provided does not have a root, or at the very least, the root is not discoverable via the algorithm. lastly, is a flag that determines whether we return the full update history or simply the last value in the iteration as a single value. one peculiarity that deserves attention is the exception, which occurs in this case if the number of arguments passed into the function does not match. i added this block to take into account the fact that the method and other approximate derivative calculation methods such as have differing numbers of parameters. let's see if this actually works by using the example we've been reusing thus far, , or and , both of which we have already defined and initialized above. 2.7144176165949068 the root seems to be around 2.7. and indeed, if we cube it, we end up with a value extremely close to 20. in other words, we have successfully found the root to . 20.000000000000004 instead of the direct derivative, , we can also use approximation methods. in the example below, we show that using results in a very similar value . 2.7144176165949068 this result aligns with the earlier observation that center divided difference provides very accurate approximations of the first derivative. note that the advantage of using is that we can now apply newton raphson to non polynomial equations that cannot be formulated in list index representation format. for instance, let's try something like . 1.0986122886724257 to verify that this is indeed correct, we can plug back into . also, given that , we can use some heuristics to realize that the answer must be only ever so slightly larger than 1. 1.2947865002388426e 11 notice that the result is extremely close to zero, suggesting that we have found the correct root. now that we have seen the robustness of the newton raphson method, let's take a look at another similar numerical method that uses backward divided difference for derivative approximation. in this section, we will look at the secant method, which is another method for identifying the roots of non linear equations. before we get into a description of how this method works, here's a quick graphic, again from wikipedia, on how the secant method works. as the name implies, the secant function works by drawing secant lines that cross the function at each iteration. then, much like the newton raphson method, we find the intercept of that secant line, find a new point on the graph whose coordinate corresponds to that intercept, and use the point from the previous iteration to find a new secant line. this process is very intuitively outlined in this video by numericalmethodsguy. the update rule for the secant method can be expressed as we can derive simply by slightly modifying the update rule we saw for newton raphson. recall that the newton raphson update rule was written as the only modification we need to make to this update rule is to replace with an approximation using the backward divided difference formula. here, we make a slight modification to , specifically by using values from previous iterations. if we plug back into , with some algebraic simplifications, we land on , the update rule for the secant method. this is left as an exercise for the reader. now let's take a look at how we might be able to implement this numerical method in code. presented below is the method, which follows the same general structure as the function we looked at earlier. the only part that differs is the part where we use backward divided difference, which requires that we look up not only the immediately previous root approximation, but the value prior to that as well. in other words, we need both and to calculate via an iterative update. and here is an obligatory sanity check using our previous example. 2.714417616613744 2.7 is a familiar value, and indeed it is what was returned by the newton raphson method as well. we confirm that this is indeed the root of the equation. 20.00000000041639 now that we have looked at both methods, it's time to make a quick comparison. we will be comparing three different methods: newton raphson method with direct polynomial derivatives newton raphson method with center divided difference secant method with backward divided difference by setting to be , we can obtain a full list of root updates as the three methods begin their quest for the root of the function. we can then see which method converges the quickest. let's see how this little experiment turns out. we first begin by importing some dependencies to plot the history of values. then, we obtain the history for each of the three approaches and plot them as a scatter plot. the result is shown below. you might have to squint your eye to see that and almost coincide exactly at the same points. i was honestly somewhat surprised by the result: although we had verified the advantage of using center divided difference over forward or backward divided difference via some simple big o analysis with trailing error terms, i did not expect the two to coincide with such exactitude. another interesting observation is that the secant method seems to take slightly longer than the newton raphson method. this is probably due to the fact that the secant method uses backward divided difference, and also the fact that it requires two previous at each iteration instead of one. the reason why the first update seems rather ineffective is that the two initial guesses that we fed into the model was probably not such a good starting point. the topic of today's post was somewhat different from what we had previously dealt with in this blog, but it was an interesting topic for me nonetheless. i had encountered the newton raphson method previously when going down my typical wikipedia rabbit holes, but it is only today that i feel like i've finally got a grasp of the concept. i consider this post to be a start of many more posts on numerical methods to come. i hope you've enjoyed reading this post. see you in the next one.