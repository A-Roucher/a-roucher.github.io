in a previous post, we took a look at autoencoders, a type of neural network that receives some data as input, encodes them into a latent representation, and decodes this information to restore the original input. autoencoders are exciting in and of themselves, but things can get a lot more interesting if we apply a bit of twist. in this post, we will take a look at one of the many flavors of the autoencoder model, known as variational autoencoders, or vae for short. specifically, the model that we will build in this tutorial is a convolutional variational autoencoder, since we will be using convolutional layers for better image processing. the model architecture introduced in this tutorial was heavily inspired by the one outlined in fran√ßois chollet's deep learning with python, as well as that from a separate article on the keras blog. let's start by importing the modules necessary for this demonstration. the objective of today's task is to build an autoencoder model that produces mnist hand written digits. the hidden dimension, or the latent space of the model, is going to a random vector living in two dimensional space. let's specify this setup, along with some other miscellaneous configurations, before we proceed with constructing the model architecture. it's time to build our model... or not quite now. before we start stacking layers for the encoder and the decoder, we need to define a sampling function that will perform the meat of the variational inference involved in vae. let's start out by taking a look at the sampling function we will use to define one of the layers of the variational autoencoder network. simply put, the above below takes as arguments and in the form of a bundled list. as you can guess from the name of the variables, these two parameters refer to the mean and log variance of the random vector living in our predefined latent space. note that we are assuming a diagonal gaussian here: in other words, the covariance matrix of the multi dimensional gaussian is assumed to be diagonal, meaning that each elements of the vector are independent. if any of this sounds foreign to you, i recommend that you read this post on the gaussian distribution. let's continue our discussion with the sampling function. the goal here is to sample a random vector in the latent space from the distribution specified by the two parameters, mean and log variance. the sampling process can be expressed as follows: where denotes the mean, corresponding to , denotes a tensor of random numbers sampled from the standard normal distribution, and denotes the standard deviation . essentially, the goal here is to use a resampling technique such that we can sample from a standard normal distribution centered around mean 0 and a standard deviation of 1, but consequentially sample from a distribution of living in the latent space. if you are wondering how translates to the return statement, then the following equation might resolve your curiosity. this is the promised elaboration on the relationship between log variance and standard deviation: therefore, multiplying 0.5 is just a simple algebraic manipulation to morph log variance to standard deviation. the reason why we use log variance instead of just variance or standard deviation is to ensure numerical stability in computation. now that this part has been cleared, let's start stacking away layers! just like the autoencoder, vaes are composed of two discrete components: the encoder and the decoder. here, we take a look at the first piece of the puzzle, the encoder network. there are several things to note about this model. first, i decided to use a loop to simplify the process of stacking layers. instead of repeating the same code over multiple lines, i found this approach to be more succinct and concise. second, we define a custom layer at the end, shown as , that uses the function we defined earlier. this is the final key that enables us to build an encoder model that receives as input a 28 by 28 image, then output a two dimensional latent vector representation of that image to pass onto the decoder network. below is the summary of what our model looks like. note that the model outputs a total of three quantities: , , and . we need the first two parameters to later sample from the latent distribution; , of course, is needed to train the decoder. model: "encoder" __________________________________________________________________________________________________ ================================================================================================== input_1 0 __________________________________________________________________________________________________ conv2d 320 input_100 __________________________________________________________________________________________________ conv2d_1 18496 conv2d00 __________________________________________________________________________________________________ flatten 0 conv2d_100 __________________________________________________________________________________________________ dense 50192 flatten00 __________________________________________________________________________________________________ z_mean 34 dense00 __________________________________________________________________________________________________ z_log_var 34 dense00 __________________________________________________________________________________________________ z 0 z_mean00 z_log_var00 ================================================================================================== total params: 69,076 trainable params: 69,076 non trainable params: 0 __________________________________________________________________________________________________ the decoder network looks similar to the the encoder, except that much of the architecture is in reverse order. most notably, we use to undo the convolution done by the encoder. this allows us to effectively scale up the input back to its original dimension, which is what we want to do with a generative model like a vae. one subtly worth mentioning is the fact that we use a sigmoid activation in the end. this is because we want the pixel values of the output to be between 0 and 1, just as the original input was normalized before it was fed into the encoder network via division by 255. the summary of the decoder network is presented below: model: "decoder" _________________________________________________________________ ================================================================= input_2 0 _________________________________________________________________ dense_1 9408 _________________________________________________________________ reshape 0 _________________________________________________________________ conv2d_transpose 36928 _________________________________________________________________ conv2d_transpose_1 18464 _________________________________________________________________ conv2d_transpose_2 289 ================================================================= total params: 65,089 trainable params: 65,089 non trainable params: 0 _________________________________________________________________ now that we have both the encoder and the decode network fully defined, it's time to wrap them together into one autoencoder model. this can simply achieved by defining the input as the input of the encoder the normalized mnist images and defining the output as the output of the decoder when fed a latent vector. concretely, this process might look as follows: let's look a the summary of the cvae. note that the encoder and the decoder look like individual layers in the grand scheme of the vae architecture. model: "model_1" _________________________________________________________________ ================================================================= input_1 0 _________________________________________________________________ encoder , , 65089 ================================================================= total params: 134,165 trainable params: 134,165 non trainable params: 0 _________________________________________________________________ we have almost everything we need, but there is one crucial step that is missing: compiling the model with an optimizer and a loss function. normally, defining a loss function is very easy: in most cases, we use pre made loss functions that are available through the tensorflow api, such as cross entropy or mean squared error. in the case of variational autoencoders, however, this is not such an easy task: how do we judge the robustness or the effectiveness of the decoder, which is essentially a generative algorithm? of course, we could stop training once the figures it generates becomes reasonable, i.e. the mock mnist digits it creates looks compelling to the human eye. however, this is a subjective metric at best, and we can't expect there to be a ml engineer peering at the screen, looking at the outputs of the decoder per each epoch. to tackle this challenge, we need to dive into some math. let's take a look. first, let's carefully review what our goal is for this task. the motivating idea behind variational autoencoders is that we want to model a specific distribution, namely the distribution of the latent space given some input. as you recall, this latent space is a two dimensional vector modeled as a multivariate diagonal gaussian. using bayes' theorem, we can express this distribution as follows: by now, it is pretty clear what the problem its: the evidence sitting in the denominator is intractable. therefore, we cannot directly calculate or derive in its closed form; hence the need for variational inference. the best we can do is to find a distribution that best approximates . how do we find this distribution? well, we know one handy concept that measures the difference or the pseudo distance between two distributions, and that is kullback leibler divergence. as we discussed in this post on entropy, kl divergence tells us how different two distributions are. so the goal here would be find a distribution that minimizes the following expression: using the definition of conditional probability, we can simplify as follows: the trick is to notice that is a constant that can break out of the expectation calculation. let's continue by deriving an expression for the evidence term. a useful property to know about kl divergence is the fact that it is always non negative. we will get into why this is the case in a moment. for now, let's assume non negativity to be true and transform into an inequality: the term on the right of the inequality is known as the evidence lower bound, or elbo for short. why are we interested in elbo? first, note that , the evidence, is a constant. therefore, minimizing kl divergence amounts to maximizing elbo. this is the key to variational inference: instead of calculating the intractable integral in , we can find a distribution that which minimizes kl divergence by maximizing elbo, which is a tractable operation. let's prove why kl divergence is always greater or equal to zero, which is a condition we assumed to be true in the derivation of elbo above. for the sake of completeness, i present two ways of proving the same property. in the context of probability, jensen's inequality can be summarized as follows. given a convex function , we won't get into rigorous proofs here, but it's not difficult to see why this inequality stands with some basic geometric intuition. due to its bow like shape, the expected value of a convex function evaluated across a given interval will always be greater or equal to the function evaluated at the expected value of the random variable. !img how is jensen's inequality related to the non negativity of kl divergence? let's return back to the definition of kl divergence. for simplicity and to reduce notational burden, we briefly depart from conditional probabilities and return back to generic distributions and . notice that the definition of kl divergence itself is an expected value expression. also, note that is a convex function itself is concave, but the negative sign flips the concavity the other way. with these observations in mind, we can apply jensen's inequality to derive the following: therefore, we have shown that kl divergence is always greater or equal to zero, which was our end goal. there is another version of a proof that i found a lot more intuitive and easier to follow than the previous approach. this derivation was borrowed from this post. we start from the simple observation that a logarithmic function is always smaller than a linear one. in other words, this is no rocket science, and one can easily verify by simply plotting the two functions on a cartesian plane. using , we can proceed in a different direction from the definition of kl divergence. once again, we have shown that kl divergence is positive! proving this isn't really necessary in the grand scheme of exploring the mathematics behind vaes, yet i thought it would help to have this adjunctive section to better understand kl divergence and familiarize ourselves with some standard algebraic manipulations that are frequently invoked in many derivations. let's jump back into variational inference and defining the cost function with elbo. recall from the setup of our variational autoencoder model that we have defined the latent vector as living in two dimensional space following a multivariate gaussian distribution. it's time to apply the elbo equation to this specific context and derive a closed form expression of our loss function. let's recall the formula for elbo: after some rearranging, we can decompose elbo into two terms, one of which is a kl divergence: now, it's finally time for us to dive deep into math: let's unpack the closed form expression in . note that the elbo expression applies to just about any distribution, but since we chose a multivariate gaussian to be the base distribution, we will see how it unfolds specifically in this context. let's begin by assuming the distribution of our models to be gaussian. namely, because is an approximation of , we naturally assume the same model for the approximate distribution: now we can derive an expression for the negative kl divergence sitting in the elbo expression: this may seem like a lot, but it's really just plugging in the distributions into the definition of kl divergence as an expectation and using some convenient properties of logarithms to perform simple algebraic simplifications. to proceed further, observe that the first term is a constant that can escape out of the expectation: from the definition of variance and expectation, we know that therefore, we can simplify as follows: let's zoom in on the expected value term in . our goal is to use again so that we can flesh out another one half from that term. this can be achieved through some clever algebraic manipulation: but since the the expected value of is constant and that of is zero, we can now plug this simplified expression back into the calculation of kl divergence, in : since we will standardize our input such that and , we can plug these quantities into and show that we are almost done with deriving the expression for elbo. i say almost, because we still have not dealt with the trailing term in : at this point, it is extremely useful to recall the definition of cross entropy, which is generically defined as follows: therefore, we see that the trailing term in is just a cross entropy between two distributions! this was a circumlocutions journey, but that is enough math we will need for this tutorial. it's time to get back to coding. all that math was for this simple code snippet shown below: as you can see, this short code snippet shows, in essence, how we can define a compile a model with a custom loss function. in this case, refers to the reconstruction loss, which is the cross entropy term we saw earlier. , as you might expect, simply refers to kl divergence. notice how there is a multiplying factor in the expression, just like we did when we derived it in the section above. with some keen observations and comparisons, you will easily see that the code is merely a transcription of , with some minor differences given dimensionality. one important fact to note is that the gradient descent algorithm, by default, seeks to minimize the loss function. however, we discussed above how the objective of vae is to maximize elbo. therefore, we modify elbo into a loss function that is to be minimized by defining the loss function as the negative of elbo. in other words, the cost function is defined as ; hence the difference in sign. it's finally time to test the model. let's first begin with data preparation and preprocessing. now, we should have the training and test set ready to be fed into our network. next, let's define a simple callback application using the monitor so that training can be stopped when no substantial improvements are being made to our model. this was included because training a vae can take some time, and we don't want to waste computing resources seeing only submarginal increments to the model performance. training begins! train on 60000 samples, validate on 10000 samples epoch 1/30 60000/60000 ============================== 13s 210us/sample loss: 191.6765 val_loss: 170.1529 epoch 2/30 60000/60000 ============================== 11s 180us/sample loss: 163.9683 val_loss: 160.2263 epoch 3/30 60000/60000 ============================== 11s 181us/sample loss: 159.0007 val_loss: 158.0777 epoch 4/30 60000/60000 ============================== 11s 180us/sample loss: 156.8238 val_loss: 156.3414 epoch 5/30 60000/60000 ============================== 11s 181us/sample loss: 155.4041 val_loss: 154.7498 epoch 6/30 60000/60000 ============================== 11s 181us/sample loss: 154.2847 val_loss: 153.9668 epoch 7/30 60000/60000 ============================== 11s 180us/sample loss: 153.4675 val_loss: 153.8024 epoch 8/30 60000/60000 ============================== 11s 179us/sample loss: 152.7539 val_loss: 152.6393 epoch 9/30 60000/60000 ============================== 11s 181us/sample loss: 152.2562 val_loss: 152.6557 epoch 10/30 60000/60000 ============================== 11s 180us/sample loss: 151.7278 val_loss: 151.7882 epoch 11/30 60000/60000 ============================== 11s 179us/sample loss: 151.3973 val_loss: 151.6642 epoch 12/30 60000/60000 ============================== 11s 177us/sample loss: 150.9899 val_loss: 151.3316 epoch 13/30 60000/60000 ============================== 11s 177us/sample loss: 150.6191 val_loss: 152.0779 epoch 14/30 60000/60000 ============================== 11s 179us/sample loss: 150.3378 val_loss: 151.6977 after 14 epochs, training has stopped, meaning that no meaningful improvements were being made. let's visualize the representation of the latent space learned by the vae. visualizing this representation is easy in this case because we defined the latent space to be two dimensional; in other words, all points can be plotted on a cartesian plane. let's take a look: this plot shows us how each numbers are distributed across the latent space. notice that numbers that belong to the same class seem to be generally clustered around each other, although there is a messy region in the middle. this is a reasonable result: while we would expect ones to be fairly easy to distinguish from, say, eights, numbers like zeros and sixes might look very similar, and hence appear mixed as a lump in the fuzzy region in the middle. one cool thing about vaes is that we can use their learned representation to see how numbers slowly morph and vary across a specified domain. this is why vaes are considered to be generative models: if we feed the vae some two dimensional vector living in the latent space, it will spit out a digit. whether or not that digit appears convincing depends on the random vector the decoder was provided as input: if the vector is close to the learned mean, , then the result will be convincing; if not, we might see a confusing blob of black and white. let's see what exactly is going on in the fuzzy region of the image, because that is apparently where all the digits mingle together and seem indistinguishable from one another. put differently, if we vary the random vector little by little across that region, we will be able to see how the digit slowly morphs into another number. how cool is that? we were able to get a vae to show us how one digit can shift across a certain domain of the latent space. this is one of the many cool things we can do with a generative model like a variational autoencoder. in this post, we took a deep dive into the math behind variational autoencoders. it was a long journey, but definitely worth it because it exposed us to many core concepts in deep learning and statistics. at the same time, i found it fascinating to see how a model could learn from a representation to generate numbers, as we saw in the very last figure. in a future post, we will look at generative adversarial networks, or gans, which might be considered as the pinnacle of generative models and a successor to autoencoders. gans resemble autoencoders in that it is also composed of two models. one core difference, however, is that in gans, the two models are in a competing relationship, whereas in autoencoders, the encoder and the decoder play distinct, complementary roles. if any of this sounds exciting, make sure to check out the next post. i hope you enjoyed reading. catch you up in the next one!