in a previous post, we discussed how we can use tf idf vectorization to encode documents into vectors. while probing more into this topic and geting a taste of what nlp is like, i decided to take a jab at another closely related, classic topic in nlp: word2vec. word2vec is a technique introduced by google engineers in 2013, popularized by statements such as "king man + woman = queen." the gist of it, as you may know, is that we can express words as vectors that encode their semantics in a meaningful way. when i was just getting starting to learn tensorflow, i came across the embedding layer, which performed exactly this operation: transforming words into vectors. while i thought this process was extremely interesting, i didn't know about the internals of this structure until today, particularly after reading this wonderful tutorial by chris mccornick. in this post, we will be implementing word2vec, a popular embedding technique, from scratch with numpy. let's get started! instead of going over the concepts and implementations separately, let's jump straight into the whole implementation process and elaborate on what is necessary along the way. in order to create word embeddings, we need some sort of data. here is a text on machine learning from wikipedia. i've removed some parentheses and citation brackets to make things slightly easier. since we can't feed raw string texts into our model, we will need to preprocess this text. the first step, as is the approach taken in many nlp tasks, is to tokenize the text, i.e. splitting the text up into smaller units like words, getting rid of punctuations, and so on. here is a function that does this trick using regular expressions. let's create tokens using the wikipedia excerpt shown above. the returned object will be a list containing all the tokens in . another useful operation is to create a map between tokens and indices, and vice versa. in a sense, we are creating a lookup table that allows us to easily convert from words to indices, and indices to words. this will be particularly useful later on when we perform operations such as one hot encoding. let's check if the word to index and index to word maps have successfully been created. {'it': 0, 'wide': 1, 'variety': 2, 'build': 3, 'improve': 4, 'computer': 5, 'a': 6, 'make': 7, 'decisions': 8, 'difficult': 9, 'on': 10, 'applications': 11, 'based': 12, 'filtering': 13, 'explicitly': 14, 'email': 15, 'study': 16, 'without': 17, 'learning': 18, 'of': 19, 'vision': 20, 'perform': 21, 'machine': 22, 'known': 23, 'or': 24, 'automatically': 25, 'so': 26, 'seen': 27, 'training': 28, 'sample': 29, 'artificial': 30, 'in': 31, 'to': 32, 'the': 33, 'being': 34, 'where': 35, 'tasks': 36, 'conventional': 37, 'do': 38, 'predictions': 39, 'such': 40, 'mathematical': 41, 'model': 42, 'used': 43, 'and': 44, 'through': 45, 'programmed': 46, 'develop': 47, 'are': 48, 'needed': 49, 'data': 50, 'subset': 51, 'order': 52, 'as': 53, 'intelligence': 54, 'that': 55, 'algorithms': 56, 'is': 57, 'experience': 58, 'infeasible': 59} as we can see, the lookup table is a dictionary object containing the relationship between words and ids. note that each entry in this lookup table is a token created using the function we defined earlier. now that we have tokenized the text and created lookup tables, we can now proceed to generating the actual training data, which are going to take the form of matrices. since tokens are still in the form of strings, we need to encode them numerically using one hot vectorization. we also need to generate a bundle of input and target values, as this is a supervised learning technique. this then begs the question of what the input and target values are going to look like. what is the value that we are trying to approximate, and what sort of input will we be feeding into the model to generate predictions? the answer to these questions and how they tie into word2vec is at the heart of understanding word embeddings as you may be able to tell, word2vec is not some sort of blackbox magic, but a result of careful training with input and output values, just like any other machine learning task. so here comes the crux of word2vec: we loop through each word in the sentence. in each loop, we look at words to the left and right of the input word, as shown below. this illustration was taken from this article by ramzi karam. in the particular example as shown above, we would generate the following input and prediction pairs part of the training data. note that the window size is two, which is why we look up to two words to the left and right of the input word. so in a way, we can understand this as forcing the model to understand a rough sense of context the ability to see which words tend to stick together. in our own example, for instance, we would see a lot of , meaning that the model should be able to capture the close contextual affinity between these two words. below is the code that generates training data using the algorithm described above. we basically iterate over the tokenized data and generate pairs. one technicality here is that, for the first and last few tokens, it may not be possible to obtain words to the left or right of that input token. in those cases, we simply don't consider these word pairs and look at only what is feasible without causing s. also note that we create and separately instead of putting them in tuple form as demonstrated above. this is just for convenience with other matrix operations later on in the post. below is the definition for , an auxiliary function we used above to combine two objects. also, here is the code we use to one hot vectorize tokens. this process is necessary in order to represent each token as a vector, which can then be stacked to create the matrices and . finally, let's generate some training data with a window size of two. let's quickly check the dimensionality of the data to get a sense of what matrices we are working with. this intuition will become important in particular when training and writing equations for backpropagation in the next section. both and are matrices with 330 rows and 60 columns. here, 330 is the number of training examples we have. we would expect this number to have been larger had we used a larger window. 60 is the size of our corpus, or the number of unique tokens we have in the original text. since we have one hot encoded both the input and output as 60 dimensional sparse vectors, this is expected. now, we are finally ready to build and train our embedding network. at this point, you might be wondering how it is that training a neural network that predicts some nearby context word given an input token can be used to embed words into vectors. after all, the output of the network is going to be some probability vector that passed through a softmax layer, not an embedding vector. this is entirely correct, and this is a question that came to my mind as well. however, this is the part that gets the most interesting: the rows of the intermediate weight matrix is the embedding we are looking for! this becomes much more apparent once we consider the dimensions of the weight matrices that compose the model. for simplicity purposes, say we have a total of 5 words in the corpus, and that we want to embed these words as three dimensional vectors. more specifically, here is the first weight layer of the model: a crucial observation to make is that, because the input is a sparse vector containing one hot encoded vectors, the weight matrix effectively acts as a lookup table that moves one hot encoded vectors to dense vectors in a different dimension more precisely, the row space of the weight matrix. in this particular example, the weight matrix was a transformation of . this is exactly what we want to achieve with embedding: representing words as dense vectors, a step up from simple one hot encoding. this process is exactly what embedding is: as we start training this model with the training data generated above, we would expect the row space of this weight matrix to encode meaningful semantic information from the training data. continuing onwards, here is the second layer that receives as input the embeddings, then uses them to generate a set of outputs. we are almost done. all we now need in the last layer is a softmax layer. when the output is passed into this layer, it is converted into probability vectors whose elements sum up to one. this final output can be considered as context predictions, i.e. which words are likely to be in the window vicinity of the input word. in training specifically error calculation and backpropagation we would be comparing this prediction of probability vectors with its true one hot encoded targets. the error function that we use with softmax is cross entropy, defined as i like to think of this as a dot product of the target vector and the log of the prediction, because that is essentially what the summation is doing. in this alternate formulation, the cross entropy formula can be rewritten as because a one hot encoded vector in this case, all the elements in whose entry is zero will have no effect on the final outcome. indeed, we simply end up taking the negative log of the prediction. notice that the closer the value of the prediction is to 1, the smaller the cross entropy, and vice versa. this aligns with the behavior we want, since we want the predicted probability to be as close to 1 as possible. so let's summarize the entire process a little bit. first, embeddings are simply the rows of the first weight matrix, denoted as . through training and backpropgation, we adjust the weights of , along with the weight matrix in the second layer, denoted as , using cross entropy loss. overall, our model takes on the following structure: where is the matrix contains the prediction probability vectors. with this in mind, let's actual start building and train our model. let's start implement this model in code. the implementation we took here is extremely similar to the approach we took in this post. for an in depth review of backpropagation derivation with matrix calculus, i highly recommend that you check out the linked post. the representation we will use for the model is a python dictionary, whose values are the weight matrices and keys, the name with which we will refer to the weight matrices. in accordance with the nomenclature established earlier, we stick with and to refer to these weights. let's specify our model to create ten dimensional embeddings. in other words, each token will be represented as vectors living in ten dimensional space. note that actual models tend to use much higher dimensions, most commonly 300, but for our purposes this is not necessary. let's begin with forward propagation. coding the forward propagation process simply amounts to transcribing the three matrix multiplication equations in into numpy code. for backpropagation, we will need all the intermediate variables, so we hold them in a dictionary called . however, if we simply want the final prediction vectors only, not the cache, we set to . this is just a little auxiliary feature to make things slightly easier later. we also have to implement the function we used above. note that this function receives a matrix as input, not a vector, so we will need to slightly tune things up a bit using a simple loop. at this point, we are done with implementing the forward pass. however, before we move on, it's always a good idea to check the dimensionality of the matrices, as this will provide us with some useful intuition while coding backward propagation later on. the dimensionality of the matrix after passing the first layer, or the embedding layer, is as follows: this is expected, since we want all the 330 tokens in the text to be converted into ten dimensional vectors. next, let's check the dimensionality after passing through the second layer. this time, it is a 330 by 60 matrix. this also makes sense, since we want the output to be sixty dimensional, back to the original dimensions following one hot encoding. this result can then be passed onto the softmax layer, the result of which will be a bunch probability vectors. implementing backward propagation is slightly more difficult than forward propagation. however, the good news is that we have already derived the equation for backpropagation given a softmax layer with cross entropy loss in this post, where we built a neural network from scratch. the conclusion of the lengthy derivation was ultimately that given our model since we know the error, we can now backpropagate it throughout the entire network, recalling basic principles of matrix calculus. if backprop is still confusing to you due to all the tranposes going on, one pro tip is to think in terms of dimensions. after all, the dimension of the gradient must equal to the dimension of the original matrix. with that in mind, let's implement the backpropagation function. to keep a log of the value of the error throughout the backpropagation process, i decided to make the final return value of to be the cross entropy loss between the prediction and the target labels. the cross entropy loss function can easily be implemented as follows. now we're ready to train and test the model! as we only have a small number of training data coupled with the fact that the backpropagation algorithm is simple batch gradient descent let's just iterate for 50 epochs. while training, we will be caching the value of the cross entropy error function in a list. we can then plot this result to get a better sense of whether the training worked properly. and indeed it seems like we did well! we can thus say with some degree of confidence that the embedding layer has been trained as well. an obvious sanity check we can perform is to see which token our model predicts given the word "learning." if the model was trained properly, the most likely word should understandably be "machine." and indeed, when that is the result we get: notice that "machine" is at the top of the list of tokens, sorted by degree of affinity with "learning." machine intelligence the is so build are computer perform it learning conventional a improve subset automatically model algorithms do based artificial through that known experience vision wide programmed data tasks infeasible develop applications used seen on explicitly of study predictions such filtering where needed decisions mathematical email variety or order training and being without in sample to make difficult as building and training was fun and all, but our end goal was not to build a neural network; we wanted to get word embeddings. as stated earlier in this post, the key behind word embeddings is that the rows of the first weight matrix is effectively a dense representation of one hot encoded vectors each corresponding to various tokens in the text dataset. in our example, therefore, the embedding can simply be obtained by array but of course, this is not a user friendly way of displaying the embeddings. in particular, what we want is to be able to input a word through a function and receive as output the embedding vector for that given word. below is a function that implements this feature. when we test out the word "machine," we get a dense ten dimensional vector as expected. array and of course, this vector is not a collection of some randomly initialized numbers, but a result of training with context data generated through the sliding window algorithm described above. in other words, these vectors encode meaningful semantic information that tells us which words tend to go along with each other. while this is a relatively simple, basic implementation of word2vec, the underlying principle remains the same nonetheless. the idea is that, we can train a neural network to generate word embeddings in the form of a weight matrix. this is why embedding layers can be trained to generate custom embeddings in popular neural network libraries like tensorflow or pytorch. if you end up training word embeddings on large datasets like wikipedia, you end up with things like word2vec and glove, another extremely popular alternative to word2vec. in general, it's fascinating to think that, with enough data, we can encode enough semantics into these embedding vectors to see relationships such as "king man + woman = queen." i hope you've enjoyed reading this post. see you in the next one.