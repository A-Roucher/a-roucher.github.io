in today's post, we will explore ways to build machine learning pipelines with scikit learn. a pipeline might sound like a big word, but it's just a way of chaining different operations together in a convenient object, almost like a wrapper. this abstracts out a lot of individual operations that may otherwise appear fragmented across the script. i also personally think that scikit learn's ml pipeline is very well designed. so here is a brief introduction to ml pipelines is scikit learn. for the purposes of this tutorial, we will be using the classic titanic dataset, otherwise known as the course material for kaggle 101. i'm still trying to get my feet into kaggle, so it is my hope that this tutorial will also help those trying to break into data science competitions. first, let's import the modules and datasets needed for this tutorial. scikit learn is the go to library for machine learning in python. it contains not only data loading utilities, but also imputers, encoders, pipelines, transformers, and search tools we will need to find the optimum model for the task. let's load the dataset using . let's observe the data by calling . by default, this shows us the first five rows and as many columns as it can fit within the notebook. let's take a look at the data in more depth to build a foundation for our analysis. this step typically involves the following steps: checking for null entries identifying covariance feature engineering let's proceed in order. before proceeding with any data analysis, it's always a good idea to pay attention to missing values how many of them there are, where they occur, et cetera. let's take a look. pclass false name false sex false age true sibsp false parch false ticket false fare true cabin true embarked false dtype: bool the is useful, but is doesn't really show us how many values are missing for each column. to probe into this issue in more detail, we need to use instead. pclass 0 name 0 sex 0 age 209 sibsp 0 parch 0 ticket 0 fare 1 cabin 822 embarked 0 dtype: int64 i recently realized that there is also a very cool data visualization library called for observing missing data. this visualization gives us a more intuitive sense of where the values are missing. in this case, the missing values seem to be distributed somewhat evenly or randomly. however, we can also imagine cases were missing values might have something to do with an inherent attribute in the dataset . in such cases, using this library to visualize where missing values occur is a good idea, as this is an additional dimension of information that calling wouldn't be able to reveal. now that we have a rough sense of where missing values occur, we need to decide from one of few choices: drop entries with missing values drop columns with too many missing value use imputation to fill missing values with alternate values indeed, in this case, we will go ahead and drop the attribute. this choice becomes more obvious when we compute the percentage of null values. pclass 0.000000 name 0.000000 sex 0.000000 age 19.961796 sibsp 0.000000 parch 0.000000 ticket 0.000000 fare 0.095511 cabin 78.510029 embarked 0.000000 dtype: float64 this shows us that 77 percent of the rows have missing attribute values. given this information, it's probably a bad idea to try and impute these values. we opt to drop it instead. correlation is a metric that we always care about, since ultimately the goal of ml engineering is to use a set of input features to generate a prediction. given this context, we don't want to feed our model useless information that lacks value; instead, we only want to feed into the model highly correlated, relevant, and informative features. if certain features in the raw data are deemed useless, we need to either drop it or engage in some sort of feature engineering to produce a new set of more correlated features. from this preliminary analysis, it seems like there are some very weekly correlated features, namely and . the week correlation suggests that perhaps we need to engage in some feature engineering to extract more meaningful information out of the dataset. let's use the findings from the previous section to engineer some more informative features. one popular approach is to make use of names to derive a feature. intuitively, this makes sense: mr. and mrs, cpt. and dr. might be of interest for our model. another popular approach is to combine the less important features and into something like . implementing these should fairly be simple, so let's try it here. note that in an actual setting, there will be no answer to reference; we will have to rely on our own domain knowledge and more extensive eda to figure out which features matter, and what new features we will need. sometimes, weakly correlated features can be combined together to form a new feature, which might exhibit higher correlation with respect to the target. we can combine and into a new feature, called . strictly speaking, we would have to add 1, but adding all values by one corresponds to shifting everything by a constant value, which will not affect modeling since such constant adjustments will be taken care of by the preprocessing step anyway. note that feature engineering is also applied to both the training and test set simultaneously. we have created two new features, namely and . let's go ahead and perform feature engineering on the column as well to squeeze out more information. now we have some data that seems a lot more workable. however, we still have a problem with the column: it seems like there are many titles, so we should probably perform some binning or grouping. for men, the most common title is ; for women, and . let's see if there is a difference in the survival rate between the two most common title for females it seems like the the difference is insignificant, so we will simply group them together in one. mr false miss false mrs false master false dr true rev true col true mlle true ms true major true mme true the countess true don true dona true jonkheer true sir true lady true capt true name: title, dtype: bool imputation refers to a technique used to replace missing values. there are many techniques we can use for imputation. from the analysis above, we know that the columns that require imputation are as follows: age fare embarked let's first take a look at the data types for each column. pclass float64 sex category age float64 fare float64 embarked category family_size float64 is_alone int64 title object dtype: object checking data types is necessary both for imputation and general data preprocessing. specifically, we need to pay attention as to whether a given column encodes categorical or numerical variables. for example, we can't use the mean to impute categorical variables; instead, something like the mode would make much more sense. the best way to determine whether a variable is categorical or not is simply to use domain knowledge and actually observe the data. of course, one might use hacky methods like the one below: {'embarked', 'sex', 'title'} although you might think that this is a working hack, this approach is in fact highly dangerous, even in this toy example. for example, consider , which is supposedly a numerical variable of type . however, earlier with , we saw that is in fact a ordinal variable taking discrete values, one of 1.0, 2.0, and 3.0. so hacky methods must not be used in isolation; at the very least, they need to be complemented with some form of human input. let's try to use a simple pipeline to deal with missing values in some categorical variables. this is going to be our first sneak peak at how pipelines are declared and used. here, we have declared a three step pipeline: an imputer, one hot encoder, and principal component analysis. how this works is fairly simple: the imputer looks for missing values and fills them according to the strategy specified. there are many strategies to choose from, such as most constant or most frequent. then, we one hot encode the categorical variables since most machine learning models cannot accept non numerical values as input. the last pca step might seem extraneous. however, as discussed in this stack overflow thread, the judicious combination of one hot plus pca can seldom be beat by other encoding schemes. pca finds the linear overlap, so will naturally tend to group similar features into the same feature. i don't have enough experience to attest to the veracity of this claim, but mathematically or statistically speaking, this proposition seems valid. the idea is that one hot encoding all categorical variables may very well lead to an unmanageable number of columns, thus causing one to flounder in the curse of dimensionality. a quick fix, then, is to apply pca or some other dimensionality reduction technique onto the results of one hot encoding. back to the implementation, note that we can look inside the individual components of by simply treating it as an iterable, much like a list or tuple. for example, simpleimputer next, we need to do something similar for numerical variables. only this time, we wouldn't be one hot encoding the data; instead, what we want to do is to apply some scaling, such as normalization or standardization. recently in one of andreas mueller's lectures on youtube, i learned about the , which uses median and iqr instead of mean and standard deviation as does the . this makes the a superior choice in the presence of outliers. let's try using it here. now that we have the two pipelines for numeric and categorical columns, now it's time to put them together into one nice package, then apply the process over the entire dataframe. this packaging can nicely be abstracted via the , which is the magic glue to put all the pieces together. we simply have to tell which transformer applies to which column, along with the name for each process. is the complete package that we will use to transform our data. note that allows us to specify which pipeline will be applied to which column. this is useful, since by default, imputers or transformers apply to the entire dataset. more often or not, this is not what we want; instead, we want to be able to micro manage categorical and numerical columns. the combination of and is thus a very powerful one. now all that is left is to build a final pipeline that includes the classifier model. let's see how well our model performs on a stratified 5 fold cross validation. note that this is without any hyperparameter tuning. 0.7630849851902483 and just like that, we can evaluate the performance of our model. when it comes to general fitting and testing, a useful tip i found on kaggle is the following rule of thumb: if a pipeline ends with a transformer, call then . if a pipeline ends with a model, call then . calling will cause all steps prior to the model to undergo , and the final step the model will run . if you think about it for a second, this configurations makes a lot of sense: if the pipeline contains a model, it means that it is the full package. all the steps prior to the model would involve wrangling the data; the last step would have the model use the data to make a prediction. therefore, calling should apply only to the last model after is called on all the preprocessing steps. if the pipeline itself is just a bundle of preprocessors, on the other hand, we should only be able to call . scikit learn's models are great, but in a sense they are too great. this is because there are a lot of hypterparameters to tune. fortunately for us, we can somewhat resort to a quasi brute force approach to deal with this: train models on a number of different combinations of hyperparameters and find the one that performs best! well, this is what does. is not quite as bad in that it doesn't create and test all possible models that distinct combinations of hyperparameters can yield: instead, it relies on a randomized algorithm to perform a search of the hyperparameter space. this is why is a lot quicker than , with marginal sacrifices in model performance. let's see how we might be able to perform hyperparameter search given a pipeline like the one we have built above. the parameter space we are searching for here is by no means exhaustive, but it covers a fair amount of ground. of course, we can go crazy with randomized search, basically shoving scikit learn with every possible configuration and even running a grid search instead. however, that would take an extreme amount of time and computing resources. therefore, it is important to consider which features are potentially the most important and zoom into these deciding parameters for hypterparameter optimization. randomizedsearchcv the search took a good five to ten minutes, which is a fair amount of time. let's take a look at its results. 0.8022875370243792 we can also take a look at the best parameters that were found. it's worth noting that the algorithm decided that the is superior to , which in my opinion is no surprise. however, it is interesting to see our intuition being vindicated in this fashion nonetheless. {'classifier__bootstrap': true, 'classifier__max_depth': 7, 'classifier__n_estimators': 158, 'preprocessor__cat__imputer__add_indicator': false, 'preprocessor__cat__imputer__strategy': 'constant', 'preprocessor__cat__pca__n_components': 8, 'preprocessor__num__imputer__add_indicator': true, 'preprocessor__num__imputer__n_neighbors': 5} now it's time for us to evaluate the model. while there are many different metrics we can use, in binary classification, we can look at things like accuracy, precision, recall, and the f1 score. let's take a look. array the pipeline seems to be working correctly as expected, preprocessing and imputing the data as it was fit on the training data, then generating predictions using the model with optimized parameters. let's see how well our model is doing. one useful function in is the function, which, as the name implies, gives us a comprehensive report of many widely used metrics, such as precision, recall, and the f1 score. precision recall f1 score support 0 0.85 0.91 0.88 162 1 0.84 0.73 0.78 100 accuracy 0.84 262 macro avg 0.84 0.82 0.83 262 weighted avg 0.84 0.84 0.84 262 the report suggests that the accuracy of our model on the test dataset is about 84 percent. we can manually verify this claim by calculating the accuracy ourselves using boolean indexing. 0.8435114503816794 let's top this discussion off with a look at the confusion matrix, which is another way of compactly encoding various pieces of information for model evaluation, namely true positives, true negatives, false positives, and false negatives. note that precision and recall are all metrics that are computed using tp, tn, fp and fn as parameters. the confusion matrix shows that our model performs well at determining the death and survival of those passengers who actually died, but performs rather poorly on those who lived. analyses like these cannot be obtained simply by looking at accuracy, which is why plotting the confusion matrix is always a good idea to get a sense of the model's performance. although the titanic dataset is considered trite, much like mnist is in the context of dl, i still think there is a lot to be learned. even simple ml projects like these have infinite spaces and options for exploration and experimentation. i hope to go through these classic datasets and competitions to glean insight from excellent public kernels, just like this kaggle kernel which i referenced extensively to write this tutorial. in its inception, this post was conceived of as a simple introduction to 's pipelines, but it eventually ballooned up into a somewhat comprehensive rundown of a little kaggle project. i hope to do a lot more of these in the coming days, just because i think there is immense value in mastering ml, although dl sounds a lot cooler. i hope you enjoyed reading this post. catch you up in the next one!