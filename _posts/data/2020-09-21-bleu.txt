recently, i joined the language, information, and learning at yale lab, led by professor dragomir radev. although i'm still in what i would consider to be the incipient stages of ml/dl/nlp studies meaning it will take time for me to be able to actively participate in an contribute to research and publications i think it will be a great learning experience from which i can glean valuable insight into what research at yale looks like. one of the first projects i was introduced to at the lab is domain independent table summarization. as the name implies, the goal is to train a model such that it can extract some meaningful insight from the table and produce a human readable summary. members are the lab seem to be making great progress in this project, and i'm excited to see where it will go. in the meantime, i decided to write a short post on bleu, a metric that i came across while reading some of the survey papers related to this topic. let's dive into it. before going into code and equations, a high level overview of what bleu is might be helpful here. bleu, which stands for bilingual evaluation understudy, is an metric that was introduced to quantitatively evaluate the quality of machine translations. the motivation is clear: as humans, we are able to get an intuitive sense of whether or not a given translation is accurate and of high quality; however, it is difficult to translate this arbitrary linguistic intuition to train nlp models to produce better translations. this is where bleu comes to the rescue. the way bleu works is simple. given some candidate translation of a sentence and a group of reference sentences, we use a bag of word approach to see how many occurences of bows co occur in both the translation and reference sentences. bow is a simple yet highly effective way of ensuring that the machine translation contains key phrases or words that reference translations also contain. in other words, bleu compares candidate translations with human produced, annotated reference translations and compares how many hits there are in the candidate sentence. the more bow hits there are, the better the translation. of course, there are many more details that go beyond this. for instance, bleu is able to account for situations in which meaningless words are repeated throughout the machine translation to simply increase bow hits. it can also penalize translations that are too short. by combining this bow precision based approach with some penalization terms, bleu provides a robust means of evaluating machine translations. with this high level overview in mind, let's start implementing bleu from scratch. first, let's begin by defining some simple preprocessing and helper functions that we will be using throughout this tutorial. the first on the list is , which converts a given sentence into lowercase and splits it into tokens, which are, in this case, english words. we could make this more robust using regular expressions to remove punctuations, but for the purposes of this demonstration, let's make this simpler. i decided to use anonymous functions for the sake of simplicity and code readability. next, let's write a function that creates n grams from a given sentence. this involves tokenizing the given sentence using , then looping through the tokens to create a bag of words. and here is a quick sanity check of what we've done so far. 'this is', 'is an', 'an example', 'example sentence' the bleu score is based on a familar concept in machine learning: precision. formally, precision is defined as where and stand for true and false positives, respectively. in the context of machine translations, we can consider positives as roughly corresponding to the notion of hits or matches. in other words, the positives are the bag of word n grams we can construct from a given candidate translation. true positives are n grams that appear in both the candidate and some reference translation; false positives are those that only appear in the candidate translation. let's use this intuition to build a simple precision based metric. first, we need to create some n grams from the candidate translation. then, we iterate through the n grams to see if they exist in any of the n grams generated from reference translations. we count the total number of such hits, or true positives, and divide that quantity by the total number of n grams produced from the candidate translation. below are some candidate sentences and reference translations that we will be using as an example throughout this tutorial. comparing with , it is pretty clear that the former is the better translation. let's see if the simple precision metric is able to capture this intuition. 0.9444444444444444 0.5714285714285714 and indeed that seems to be the case! however, the simple precision based metric has some huge problems. as an extreme example, consider the following candidate translation. 1.0 obviously, is a horrible translation, but the simple precision metric fails to flag it. this is because precision simply involves checking whether a hit occurs or not: it does not check for repeated bag of words. hence, the original authors of bleu introduces modified precision as a solution, which uses clipped counts. the gist of it is that, if some n gram is repeated many times, we clip its count through the following formula: here, refers to the number of hits we assign to a certain n gram. we sum this value over all distinct n grams in the candidate sentence. note that the distinction requirement effectively weeds out repetitive translations such as we looked at earlier. refers to the number of occurrences of a n gram in the candidate sentence. for example, in , the unigram appears 13 times, and so . this value, however, is clipped by , which is the maximum number of occurrence of that n gram in any one of the reference sentences. in other words, for each reference, we count the number of occurrence of that n gram and take the maximum value among them. this can seem very confusing, but hopefully it's clearer once you read the code. here is my implementation using . notice that we use a in order to remove redundancies. corresponds to ; corresponds to . using this modified metric, we can see that the is now penalized quite a lot through the clipping mechanism. 0.07692307692307693 but there are still problems that modified precision doesn't take into account. consider the following example translation. 0.9444444444444444 to us, it's pretty obvious that is a bad translation. although some of the key words might be there, the order in which they are arranged violates english syntax. this is the limitation of using unigrams for precision analysis. to make sure that sentences are coherent and read fluently, we now have to introduce the notion of n grams, where is larger than 1. this way, we can preserve some of the sequential encoding in reference sentences and make better comparison. the fact that unigrams are a poor way of evaluating translations becomes immediately clear once we plot the in n grams against modified precision. as you can see, precision score decreases as gets higher. this makes sense: a larger simply means that the window of comparison is larger. unless whole phrases co occur in the translation and reference sentences which is highly unlikely precision will be low. people have generally found that a suitable value lies somewhere around 1 and 4. as we will see later, packages like use what is known as cumulative 4 gram bleu score, or bleu 4. the good news is that our current implementation is already able to account for different values. this is because we wrote a handy little function, . by passing in different values to , we can deal with different n grams. now we're almost done. the last example to consider is the following translation: this is obviously a bad translation. however, due to the way modified precision is currently being calculated, this sentence will likely earn a high score. to prevent this from happening, we need to apply what is known as brevity penalty. as the name implies, this penalizes short candidate translations, thus ensuring that only sufficiently long machine translations are ascribed a high score. although this might seem confusing, the underlying mechanism is quite simple. the goal is to find the length of the reference sentence whose length is closest to that of the candidate translation in question. if the length of that reference sentence is larger than the candidate sentence, we apply some penalty; if the candidate sentence is longer, than we do not apply any penalization. the specific formula for penalization looks as follows: the brevity penalty term is multiplied to the n gram modified precision. therefore, a value of 1 means that no penalization is applied. let's perform a quick sanity check to see whether the brevity penalty function works as expected. 0.11080315836233387 1 finally, it's time to put all the pieces together. the formula for bleu can be written as follows: first, some notation clarifications. specifies the size of the bag of word, or the n gram. denotes the weight we will ascribe to the modified precision produced under that gram configuration. in other words, we calculate the weighted average of log precision, exponentiate that sum, and apply some brevity penalty. although this can sound like a lot, really it's just putting all the pieces we have discussed so far together. let's take a look at the code implementation. the weighting happens in the part within the generator expression within the statement. in this case, we apply weighting across that goes from to . now we're done! let's test out our final implementation with for from 1 to 4, all weighted equally. 0.5045666840058485 the package offers functions for bleu calculation by default. for convenience purposes, let's create a wrapper functions. this wrapping isn't really necessary, but it abstracts out many of the preprocessing steps, such as applying . this is because the bleu calculation function expects tokenized input, whereas and are untokenized sentences. and we see that the result matches that derived from our own implementation! 0.5045666840058485 in this post, we took a look at bleu, a very common way of evaluating the fluency of machine translations. studying the implementation of this metric was a meaningful and interesting process, not only because bleu itself is widely used, but also because the motivation and intuition behind its construction was easily understandable and came very naturally to me. each component of bleu addresses some problem with simpler metrics, such as precision or modified precision. it also takes into account things like abnormally short or repetitive translations. one area of interest for me these days is seq2seq models. although rnn models have largely given way to transformers, i still think it's a very interesting architecture worth diving into. i've also recently ran into a combined lstm cnn approach for processing series data. i might write about these topics in a future post. i hope you've enjoyed reading this post. catch you up later!