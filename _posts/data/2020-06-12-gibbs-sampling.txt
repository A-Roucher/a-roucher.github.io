it's been a while since i have posted anything about math or statistics related, and i'll admit that i've been taking a brief break from these domains, instead working on some personal projects and uping my python coding skills. this post is going to be a fun, exciting mesh of some python and math. without further ado, let's get started. i remember struggling to understand metropolis hastings a while back. gibbs sampling, on the other hand, came somewhat very naturally and intuitively to me. this is not because i've suddenly grown intelligent over the past couple of months, but because gibbs sampling is conceptually simpler, at least in my humble opinion. all that is necessary to understand gibbs sampling is the notion of conditional probability distributions. we know the classic context in which mcmc comes into play in a bayesian setting: there is some intractable distribution that we wish to sample from. metropolis hastings was one simple way to go about this, and gibbs sampling provides another method. a feature that makes gibbs sampling unique is its restrictive context. in order to use gibbs sampling, we need to have access to information regarding the conditional probabilities of the distribution we seek to sample from. in other words, say we want to sample from some joint probability distribution number of random variables. let's denote this distribution as follows: turns out that the gibbs sampler is a more specific version of the metropolis hastings algorithm. we can only use the gibbs sampler in a restricted context: namely, that we have access to conditional probability distributions. you quickly see why the gibbs sampler can only be used in limited contexts. nonetheless, when these set of information are available, it is a powerful algorithm with which we can sample from intractable distributions. let's see how this works. the gist of the gibbs sampler is simple: sample from known conditional distributions, and use that resulting value to sample the next random variable from the following conditional probability distribution, ad infinitum. but this is just a lot of words and some needless latin for fun and flair, so let's hash out what the sentence really means. continuing on from our generic example, let's say we sampled a value from the first conditional probability distribution. we will use a superscript and subscript notation to each denote the iteration and the sequence of random variable. assume that we start from some random dimensional vector to start with. following our notation, this vector would be the superscripts are all 0 since this is the first "sample" we will start off with. theoretically, it doesn't matter what these random numbers are asymptotically speaking, we should still be able to approximate the final distribution, especially if given the fact that we take burn in into account. on the first iteration, we will begin by sampling from the first probability distribution. note that we simply used the initial random values for through to sample the first value from a conditional probability distribution. now, we do the same to sample . only this time, we can use the result from earlier, namely . we can see how this might help us yield a slightly more convincing result than simply using the random data. we still have to use random values for through since we haven't sampled from their relevant conditional distributions just yet. however, as we go through all the random variables in order, it becomes obvious that we will no longer be using randomly initialized values at one point. specifically, on the th iteration, we would expect something like this to happen: can be any number between 1 and , since it is used to represent the th random variable. as we repeat more iterations of sampling, we will eventually end up with a plausible representation of dimensional vectors, which is what we sought to sample from the intractable distribution! in this section, we will take a look at a very simple example, namely sampling from a bivariate gaussian distribution. although we have dealt with dimensional examples in the algorithm analysis above, for the sake of demonstration, let's work on a simple example that we can also easily visualize and intuit. for this reason, the bivariate gaussian distribution is a sensible choice. for this post, i'll be using , which is a data visualization library built on top of . i'll simply be using to display a bivariate gaussian. for reproducibility's sake, we will also set a random seed. the code for the gibbs sampler is simple, partially because the distribution we are dealing with is a bivariate gaussian, not some high dimensional intractable distribution. this point notwithstanding, the function shows the gist of how gibbs sampling works. here, we pass in parameters for the conditional distribution, and start sampling given an initial value corresponding to . as stated earlier, this random value can be chosen arbitrarily. of course, if we start from a value that is way off, it will take much longer for the algorithm to converge, i.e. we will have to discard a large portion of initially sampled values. this is known as burn in. in this case, however, we will apply a quick hack and start from a plausible value to begin with, reducing the need for burn in. we then take turns sampling from the conditional probability distributions using the sampled values, and append to a list to accumulate the result. note that the two functions are symmetrical, which is expected given that this is a bivariate distribution. these functions simulate a conditional distribution, where given a value of one random variable, we can sample the value of the other. this is the core mechanism by which we will be sampling from the joint probability distribution using the gibbs sampling algorithm. let's initialize the parameters for the distribution and test the sampler. 5.094375689072134, 5.058667902942396, 5.175861934887288, 5.447651414116084, 5.358397131507042, 5.278071396535993, 5.550314691828798, 5.641095821184971, 5.487786105738833, 5.542093903446283 great! this works as expected. for the purposes of demonstrating the implications of burn in, let's define discard the first 100 values that were sampled. below is the plot of the final resulting distribution based on sampled values using the gibbs sampler. the result is what we would expect: a bivariate gaussian. and this is what we end up with if we sample directly from the bivariate gaussian instead of using the gibbs sampler. note that we can do this only because we chose a deliberately simple example; in many other contexts, this would certainly not be the case . notice the similarity between the result achieved by sampling from the gibbs sampler and the result produced from direct sampling as shown below. so at this point, we have now empirically checked that gibbs sampling indeed works: even if we can't directly sample from the distribution, if we have access to conditional distributions, we can still achieve an asymptotically similar result. now comes the mathematics portion of deriving the conditional distribution of a multivariate gaussian, as promised earlier. in this section, we will derive an expression for the conditional distribution of the multivariate gaussian. this isn't really relevant to the gibbs sampling algorithm itself, since the sampler can be used in non gaussian contexts as long as we have access to conditional distributions. nonetheless, deriving this is a good mental exercise that merits some discussion. just for the sake of quick review, let's briefly revisit the familiar definition of a conditional probability: in the context of random vectors, we can rewrite this as of course, if and are scalars, we go back to the familiar bivariate context of our example. in short, deriving the expression for the conditional distribution simply amounts to simplifying the fraction whose denominator is the marginal distribution and the numerator is the joint distribution. let's clarify the setup and notation first. we define a dimensional random vector that follows a multivariate gaussian distribution, namely . this vector, denoted as , can be split into a dimensional vector and dimensional vector in the following fashion: it is apparent that . similarly, we can split up the covariance matrix in the following fashion where , , . also, given the symmetric property of the covariance matrix, . the goal is to derive is the conditional distribution, . this derivation was heavily adapted from this source and this thread on stack exchange. it is certainly a somewhat lengthy derivation, but there is nothing too conceptually difficult involved it's just a lot of algebra and simplifications. we begin from the formula for the multivariate gaussian: for convenience purposes, let then, let note that this is not a one to one correspondence, i.e. . the blocks are only one to one insofar as being dimensionally equivalent. then, using block matrix multiplication, notice that the final result should be a single scalar given the dimensions of each matrix. therefore, we can further simply the expression above using the fact that . specifically, the second and third terms are transposes of each other. although we simply resorted a convenient substitution in , we still need to derive an expression for the inverse of the covariance matrix. note that the inverse of the covariance matrix can intuitively be understood as the precision matrix. we won't derive the block matrix inversion formula here. the derivation is just a matter of simply plugging in and substituting one expression for another. for a detailed full derivation, checkout this link or this journal article. to cut to the chase, we end up with plugging these results back into , and with some elided simplification steps, we end up with note that we can more conveniently express the result in the following fashion: we're now almost done. heuristically, we know that the addition in will become a multiplication when plugged back into the original formula for the multivariate gaussian as shown in , using . therefore, if we divide the entire expression by , we will only end up with the term produced by . using this heuristic, we conclude that notice that this result is exactly what we have in the function which we used to sample from the conditional distribution. the gibbs sampler is another very interesting algorithm we can use to sample from complicated, intractable distributions. although the use case of the gibbs sampler is somewhat limited due to the fact that we need to be able to access the conditional distribution, it is a powerful algorithm nonetheless. we also discussed the notion of conditional distributions of the multivariate gaussian in this post. the derivation was not the simplest, and granted we omitted a lot of algebra along the way, but it was a good mental exercise nonetheless. if you are interested in a simpler proof, i highly recommend that you check out the stack exchange post i linked above. i hope you enjoyed reading this post. catch you up in the next one!