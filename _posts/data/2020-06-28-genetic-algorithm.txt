the traveling salesman problem is a famous problem in computer science. the problem might be summarized as follows: imagine you are a salesperson who needs to visit some number of cities. because you want to minimize costs spent on traveling , you want to find out the most efficient route, one that will require the least amount of traveling. you are given a coordinate of the cities to visit on a map. how can you find the optimal route? the most obvious solution would be the brute force method, where you consider all the different possibilities, calculate the estimated distance for each, and choose the one that is the shortest path. while this is a definite way to solve tsp, the issue with this approach is that it requires a lot of compute the runtime of this brute force algorithm would be , which is just utterly terrible. in this post, we will consider a more interesting way to approach tsp: genetic algorithms. as the name implies, genetic algorithms somewhat simulate an evolutionary process, in which the principle of the survival of the fittest ensures that only the best genes will have survived after some iteration of evolutionary cycles across a number of generations. genetic algorithms can be considered as a sort of randomized algorithm where we use random sampling to ensure that we probe the entire search space while trying to find the optimal solution. while genetic algorithms are not the most efficient or guaranteed method of solving tsp, i thought it was a fascinating approach nonetheless, so here goes the post on tsp and genetic algorithms. before we dive into the solution, we need to first consider how we might represent this problem in code. let's take a look at the modules we will be using and the mode of representation we will adopt in approaching tsp. the original, popular tsp requires that the salesperson return to the original starting point destination as well. in other words, if the salesman starts at city a, he has to visit all the rest of the cities until returning back to city a. for the sake of simplicity, however, we don't enforce this returning requirement in our modified version of tsp. below are the modules we will be using for this post. we will be using , more specifically a lot of functions from for things like sampling, choosing, or permuting. arrays are also generally faster than using normal python lists since they support vectorization, which will certainly be beneficial when building our model. for reproducibility, let's set the random seed to 42. now we need to consider the question of how we might represent tsp in code. obviously, we will need some cities and some information on the distance between these cities. one solution is to consider adjacency matrices, somewhat similar to the adjacency list we took a look at on the post on breadth first and depth first search algorithms. the simple idea is that we can construct some matrix that represent distances between cities and such that represents the distance between those two cities. when , therefore, it is obvious that will be zero, since the distance from city to itself is trivially zero. here is an example of some adjacency matrix. for convenience purposes, we will represent cities by their indices. now it's time for us to understand how genetic algorithms work. don't worry, you don't have to be a biology major to understand this; simple intuition will do. the idea is that, we can use some sort of randomized approach to generate an initial population, and motivate an evolutionary cycle such that only superior genes will survive successive iterations. you might be wondering what genes are in this context. most typically, genes can be thought of as some representation of the solution we are trying to find. in this case, an encoding of the optimal path would be the gene we are looking for. evolution is a process that finds an optimal solution for survival through competition and mutation. basically, the genes that have superior traits will survive, leaving offspring into the next generation. those that are inferior will be unable to find a mate and perish, as sad as it sounds. then how do these superior or inferior traits occur in the first place? the answer lies in random mutations. the children of one parent will not all have identical genes: due to mutation, which occurs by chance, some will acquire even more superior features that puts them far ahead of their peers. needless to say, such beneficiaries of positive mutation will survive and leave offspring, carrying onto the next generation. those who experience adversarial mutation, on the other hand, will not be able to survive. in genetic algorithm engineering, we want to be able to simulate this process over an extended period of time without hard coding our solution, such that the end result after hundred or thousands of generations will contain the optimal solution. of course, we can't let the computer do everything: we still have to implement mutational procedures that define an evolutionary process. but more on that later. first, let's begin with the simple task of building a way of modeling a population. first, let's define a class to represent the population. i decided to go with a class based implementation to attach pieces of information about a specific generation of population to that class object. specifically, we can have things like to represent the full population, to represent th chosen, selected superior few, to store the score of the best chromosome in the population, to store the best chromosome itself, and , the adjacency matrix that we will be using to calculate the distance in the context of tsp. here is a little snippet of code that we will be using to randomly generate the first generation of population. let's see if this everything works as expected by generating a dummy population. array now we need some function that will determine the fitness of a chromosome. in the context of tsp, fitness is defined in very simple terms: the shorter the total distance, the fitter and more superior the chromosome. recall that all the distance information we need is nicely stored in . we can calculate the sum of all the distances between two adjacent cities in the chromosome sequence. next, we evaluate the population. simply put, evaluation amounts to calculating the fitness of each chromosome in the total population, determining who is best, storing the score information, and returning some probability vector whose each element represents the probability that the th element in the population bag is chosen as a parent. we apply some basic preprocessing to ensure that the worst performing chromosome has absolutely no chance of being selected. when we call , we get a probability vector as expected. from the result, it appears that the last element is the best chromosome; the second chromosome in the population bag is the worst. array when we call , notice that we get the last element in the population, as previously anticipated. array we can also access the score of the best chromosome. in this case, the distance is said to be 86.25. note that the lower the score, the better, since these scores represent the total distance a salesman has to travel to visit all the cities. 86.25 now, we will select number of parents to be the basis of the next generation. here, we use a simple roulette model, where we compare the value of the probability vector and a random number sampled from a uniform distribution. if the value of the probability vector is higher, the corresponding chromosome is added to . we repeat this process until we have parents. as expected, we get 4 parents after selecting the parents through . array now is the crucial part: mutation. there are different types of mutation schemes we can use for our model. here, we use a simple swap and crossover mutation. as the name implies, swap simply involves swapping two elements of a chromosome. for instance, if we have , we might swap the first two elements to end up with . the problem with swap mutation, however, is the fact that swapping is a very disruptive process in the context of tsp. because each chromosome encodes the order in which a salesman has to visit each city, swapping two cities may greatly impact the final fitness score of that mutated chromosome. therefore, we also use another form of mutation, known as crossovers. in crossover mutation, we grab two parents. then, we slice a portion of the chromosome of one parent, and fill the rest of the slots with that of the other parent. when filling the rest of the slots, we need to make sure that there are no duplicates in the chromosome. let's take a look at an example. imagine one parent has and the other has . let's also say that slicing a random portion of the first parent gave us . then, we fill up the rest of the empty indices with the other parent, paying attention to the order in which elements occur. in this case, we would end up with . let's see how this works. now, we wrap the swap and crossover mutation into one nice function to call so that we perform each mutation according to some specified threshold. let's test it on . when we call , we end up with the population bag for the next generation, as expected. 3, 1, 2, 0, 4, 0, 1, 2, 3, 4, 3, 1, 4, 0, 2, 3, 1, 2, 0, 4, 3, 1, 2, 0, 4 now it's finally time to put it all together. for convenience, i've added some additional parameters such as or , but for the most part, a lot of what is being done here should be familiar and straightforward. the gist of it is that we run a simulation of population selection and mutation over generations. the key part is and . basically, we obtain the children from the mutation and pass it over as the population bag of the next generation in the constructor. now let's test it on our tsp example over 20 generations. as generations pass, the fitness score seems to improve, but not by a lot. generation 0: 105.04 generation 1: 105.04 generation 2: 104.13 generation 3: 104.13 generation 4: 104.13 generation 5: 104.13 generation 6: 104.13 generation 7: 104.13 generation 8: 104.13 generation 9: 104.13 generation 10: 104.13 generation 11: 104.13 generation 12: 104.13 generation 13: 104.13 generation 14: 104.13 generation 15: 104.13 generation 16: 104.13 generation 17: 104.13 generation 18: 104.13 generation 19: 104.13 3, 0, 2, 1, 4 let's try running this over an extended period of time, namely 100 generations. for clarity, let's also plot the progress of our genetic algorithm by setting to . generation 0: 117.11000000000001 generation 20: 99.06 generation 40: 86.25 generation 60: 86.25 generation 80: 86.25 4, 1, 3, 2, 0 after something like 30 iterations, it seems like algorithm has converged to the minimum, sitting at around 86.25. apparently, the best way to travel the cities is to go in the order of . but this was more of a contrived example. we want to see if this algorithm can scale. so let's write some functions to generate city coordinates and corresponding adjacency matrices. generates number of random city coordinates in the form of a numpy array. now, we need some functions that will create an adjacency matrix based on the city coordinates. let's perform a quick sanity check to see if works as expected. here, give vertices of a unit square as input to the function. while we're at it, let's also make sure that indeed does create city coordinates as expected. array array now, we're finally ready to use these functions to randomly generate city coordinates and use the genetic algorithm to find the optimal path using with the appropriate parameters. let's run the algorithm for a few iterations and plot its history. generation 0: 50574.20948201705 generation 100: 37016.74080507189 generation 200: 29461.92301346868 generation 300: 27377.57152367764 generation 400: 25832.95851026539 generation 500: 24982.527095698166 generation 600: 23996.17871767765 generation 700: 23747.520578015312 generation 800: 22876.05723826875 generation 900: 22585.508967184436 91, 82, 25, 69, 52, 78, 2, 57, 75, 29, 27, 81, 35, 92, 18, 68, 34, 79, 58, 55, 0, 54, 74, 13, 37, 23, 67, 19, 61, 97, 64, 86, 93, 65, 17, 1, 3, 8, 59, 7, 98, 66, 49, 22, 5, 62, 41, 96, 12, 95, 36, 44, 77, 48, 31, 16, 39, 99, 53, 6, 43, 42, 83, 73, 60, 71, 76, 14, 33, 89, 38, 47, 28, 9, 85, 11, 72, 21, 88, 51, 63, 4, 15, 70, 56, 24, 94, 87, 90, 80, 50, 26, 30, 10, 40, 84, 46, 32, 20, 45 we can see that the genetic algorithm does seems to be optimizing the path as we expect, since the distance metric seems to be decreasing throughout the iteration. now, let's actually try plotting the path along with the corresponding city coordinates. here's a helper function to print the optimal path. and calling this function, we obtain the following: at a glance, it's really difficult to see if this is indeed the optimal path, especially because the city coordinates were generated at random. i therefore decided to create a much more contrived example, but with many coordinates, so that we can easily verify whether the path decided on by the algorithm is indeed the optimal path. namely, we will be arranging city coordinates to lie on a semi circle, using the very familiar equation let's create 100 such fake cities and run the genetic algorithm to optimize the path. if the algorithm does successfully find an optimal path, it will be a single curve from one end of the semi circle fully connected all the way up to its other end. generation 0: 2964.489767366144 generation 500: 871.7211766732178 generation 1000: 688.4158889546616 generation 1500: 600.4773168672432 generation 2000: 531.3472866075754 generation 2500: 502.99936690851723 generation 3000: 418.2191546248274 generation 3500: 414.9874637897337 generation 4000: 411.94420801395387 generation 4500: 411.87289018133225 generation 5000: 407.6196328244909 generation 5500: 407.54195052584106 0, 1, 2, 3, 4, 36, 37, 38, 39, 40, 41, 42, 43, 47, 48, 49, 50, 51, 52, 53, 99, 98, 97, 96, 95, 94, 93, 92, 91, 90, 89, 88, 87, 86, 85, 84, 83, 82, 81, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 54, 55, 56, 80, 79, 78, 77, 76, 75, 74, 73, 72, 71, 70, 69, 68, 67, 66, 65, 64, 63, 62, 61, 60, 59, 58, 57, 46, 45, 44, 11, 10, 9, 8, 7, 6, 5 the algorithm seems to have converged, but the returned does not seem to be the optimal path, as it is not a sorted array from 0 to 99 as we expect. plotting this result, the fact that the algorithm hasn't quite found the most optimal solution becomes clearer. this point notwithstanding, it is still worth noting that the algorithm has found what might be referred to as optimal segments: notice that there are some segments of the path that contain consecutive numbers, which is what we would expect to see in the optimal path. an optimal path would look as follows. comparing the two, we see that the optimal path returned by the genetic algorithm does contain some wasted traveling routes, namely the the chords between certain non adjacent cities. nonetheless, a lot of the adjacent cities are connected . considering the fact that there are a total of possibilities, the fact that the algorithm was able to narrow it down to a plausible route that beats the baseline is still very interesting. genetic algorithms belong to a larger group of algorithms known as randomized algorithms. prior to learning about genetic algorithms, the word "randomized algorithms" seemed more like a mysterious black box. after all, how can an algorithm find an answer to a problem using pseudo random number generators, for instance? this post was a great opportunity to think more about this naive question through a concrete example. moreover, it was also interesting to think about the traveling salesman problem, which is a problem that appears so simple and easy, belying the true level of difficulty under the surface. there are many other ways to approach tsp, and genetic algorithms are just one of the many approaches we can take. it is also not the most effective way, as iterating over generations and generations can often take a lot of time. the contrived semi circle example, for instance, took somewhere around five to ten minutes to fully run on my 13 inch macbook pro. nonetheless, i think it is an interesting way well worth the time and effort spent on implementation. i hope you've enjoyed reading this post. catch you up in the next one!