
ğŸ§ Â CLEAR: first multimodal benchmark to make models forget what we want them to forget

With privacy concerns rising, we sometimes need our models to "forget" specific information - like a person's data - while keeping everything else intact. Researchers just released CLEAR, the first benchmark to test how well this works with both text and images.

âŒÂ Bad news: Current methods either fail to truly forget or end up forgetting way too much. It's like trying to remove a single ingredient from a baked cake!

âœ¨Â But there's hope: Adding simple mathematical constraints (L1 regularization) during the forgetting process significantly improves results.

ğŸ¯Â Key insights:

âœ…Â The benchmark tests forgetting on 200 fictional personas

â€£ 3,770 visual Q&A pairs

â€£ 4,000 textual Q&A pairs

â€£ Additional real-world tests

ğŸ›‘Â Most current forgetting methods don't work well with both text and images

â€£ They either remember what they should forget

â€£ Or they forget too much unrelated information

âœ¨Â Simple mathematical constraints work surprisingly well

â€£ L1 regularization prevents excessive forgetting

â€£ Works especially well with the LLMU method

ğŸ‘‰Â Read the full paper here: [https://huggingface.co/papers/2410.18057](https://huggingface.co/papers/2410.18057)

![image.png](attachments/Posts/CLEAR/image.png)