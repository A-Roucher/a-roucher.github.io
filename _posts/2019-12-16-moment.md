---
title: Moments in Statistics
mathjax: true
date: 2019-12-16
categories:
  - study
tags:
  - math
---

The word “moment” has many meanings. Most commonly, it means a slice of time. In physics, moment refers to the rotational tendency of some object, similar to how torque measures the change in an object’s angular momentum. What we are interested in, however, is what moment means in math and statistics. In this post, we will attempt to shed new light on the topic of probability distributions through moment generating functions, or MGF for short. 

# Introduction to Moments
It’s actually quite simple. 

* First Moment: $$\mathbf{E}(X)$$
* Second Moment: $$\mathbf{E}(X^2)$$
* Third Moment: $$\mathbf{E}(X^3)$$
* Fourth Moment: $$\mathbf{E}(X^4)$$

And of course, we can imagine how the list would continue: the $$n$$th moment of a random variable would be $$\mathbf{E}(X^n)$$. It is worth noting that the first moment corresponds to the mean of the distribution, $$\mu$$. The second moment is related to variance, as $$\sigma^2 = \mathbf{E}(X^2) - \mathbf{E}(X)\mathbf{E}(X)$$. The third moment relates to the symmetry of the distribution, or the lack thereof, a quality which goes by the name of skewness. The fourth moment relates to [kurtosis], which is a measure of how heavy the tail of a distribution is. Higher kurtosis corresponds to many outliers, while the converse would signify that the distribution contains little deviations. As you can see, the common theme is that the moment contains information about the defining features of a distribution, which is why it is such a convenient way to present information about a distribution. 

# Moment Generating Function
As the name suggests, MGF is a function that generates moments of a distribution. Put more specifically, we can calculate the $$n$$th moment of a distribution simply by taking its $$n$$th derivative. Why is this important? Well, given that moments convey defining properties of a distribution, a moment generating function is basically an all-in-one package that contains every single bit of information about the distribution in question. 

Let’s get more specific by taking a look at the mathematical formula for an MGF. 

$$M_X(t) = \mathbf{E}(e^{tX}) = \sum e^{tx} f(x) \, dx \tag{1}$$

If $$X$$ is a continuous random variable, we would take an integral instead.

$$M_X(t) = \mathbf{E}(e^{tX}) = \int e^{tx} f(x) \, dx \tag{2}$$

Now, you might be wondering how taking the $$n$$th derivative of $$\mathbf{E}(e^{tX})$$ gives us the $$n$$th moment of the distribution for the random variable $$X$$. To convince ourselves of this statement, let’s start by taking a look at the taylor polynomial for the exponential. 

$$e^x = 1 + x + \frac{x^2}{2!} + \frac{x^3}{3!} + \cdots \tag{3}$$

It’s not difficult to see the coherency of this expression by taking the derivative---the derivative of the polynomial is equal to itself, as we would expect for $$e^x$$. 







[kurtosis]: https://en.wikipedia.org/wiki/Kurtosis
