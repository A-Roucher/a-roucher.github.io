---
title: Building Neural Network From Scratch
mathjax: true
toc: true
categories:
  - study
tags:
  - from_scratch
  - machine_learning
  - deep_learning
  - linear_algebra
---

Welcome back to another episode of "From Scratch" series on this blog, where we explore various machine learning algorithms by hand-coding them from scratch. So far , we have looked at various machine learning models, such as kNN, logistic regression, and naive Bayes. Now is time for an exciting addition to this mix: neural networks. 

Around last year December, I bought my first book on deep learning, titled [Deep Learning from Scratch](https://www.amazon.com/Deep-running-starting-bottom-Korean/dp/8968484635/ref=sr_1_1?qid=1579967314&refinements=p_27%3ASaito+Goki&s=books&sr=1-1&text=Saito+Goki),  by Saito Goki. It was a Korean translation of a book originally published in Japanese by O'Reilly Japan. Many bloggers recommended the book as the go-to introductory textbook on deep learning, some even going as far as to say that it is a must-have. After reading a few pages in, I could see why: as the title claimed, the author used only `numpy` to essentially recreate deep learning models, ranging from simple vanilla neural networks to convolutional neural networks. As someone who had just started to learn Python, following the book was a lot harder than expected, but it was a worthwhile read indeed.

Inspired by that book, and in part in an attempt to test the knowledge I gained from having read that bok, I decided to implement my own rendition of a simple neural network supported by minibatch gradient descent. Let's jump right into it.

# Preparing Data

The default setup of my Jupyter Notebook, as always:


```python
%matplotlib inline
%config InlineBackend.figure_format = 'svg'
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_moons
from sklearn.model_selection import train_test_split
plt.style.use('seaborn')
```

## Data Generation

Before we start building our model, we should first prepare some data. Instead of using hand-made dummy data as I had done in some previous posts, I decided to use the `sklearn` library to generate random data points. This approach makes a lot more sense given that neural networks require a lot more input data than do machine learning models. In this particular instance, we will use the `make_moons` function to accomplish this task. 


```python
X, y = make_moons(n_samples=5000, random_state=42, noise=0.1)
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)
```

Let's take a look at what our data looks like.


```python
X[0:5]
```


    array([[-0.1196884 ,  1.03684845],
           [ 0.03370055,  0.2493631 ],
           [ 0.03864294,  0.33033539],
           [ 0.22222051,  1.03355193],
           [ 0.74448612,  0.69288687]])

As expected, the dataset contains the $x$ and $y$ coordinates of the points generated by the `make_moons` function. If you haven't heard about this function before, you might be wondering what all the moons deal is about. Well, if we plot the data points, it will become a lot more obvious.


```python
def plot(data, label):
    fig = plt.figure()
    ax = fig.add_subplot(111, xlabel="x", ylabel="y")
    for i, point in enumerate(data):
        if label[i] == 0:
            ax.scatter(point[0], point[1], color='skyblue', edgecolors='white')
        else:
            ax.scatter(point[0], point[1], color='gold', edgecolors='white')
    plt.show()
```


```python
plot(X_train, y_train)
```


<img src="/assets/images/2020-02-05-neural-net_files/2020-02-05-neural-net_5_0.svg">

As you can see, the generated points belong to either one of two classes, and together, each class of points seem to form some sort of moon-like shape. Our goal will be to build a neural network that is capable of determining whether a given point belongs to class 0 or class 1. In other words, this is a classic example of a binary classification problem. 

## One-Hot Encoding

It is standard practice in any classification problem to convert class labels into one-hot encodeded vectors. The reason why this preprocessing is necessary is that the class number is merely a label that does not carry any meaning. Assume a simple classification problem with 3 labels: 0, 1, and 2. In that context, a class label of 2 is not at all related to adding two data points belonging to class 1, or any arithmatic operation of that kind. To prevent our model from making such arbitrary, unhelpful connections, we convert class labels to one-hot encoded vectors. We could use external libraries such as `keras` to invoke the `to_categorical` function, but instead let's just build a function ourselves since this is a relatively simple task.

```python
def one_hot_encode(labels):
    result = []
    for label in labels:
        if label:
            result.append([0, 1])
        else:
            result.append([1, 0])
    return np.array(result)
```

Let's test the `one_hot_encode` function on the training data. We don't need the entire data to see that it works, so let's slice the `y_train` array to see its first five elements.


```python
y_train[:5]
```


    array([0, 1, 0, 1, 0])

When we apply `one_hot_encode` to the data, we see that the returned result is a two-dimensional array containing one-hot encoded vectors, as intended.


```python
one_hot_encode(y_train[:5])
```


    array([[1, 0],
           [0, 1],
           [1, 0],
           [0, 1],
           [1, 0]])

That's all the data and the preprocessing we will need for now.



# Neural Network Model



# Activation Functions

Activation functions are important aspects of neural networks. In fact, it is what allows neural networks to model nonlinearities in data. As we saw in the preceding section, a neural network is essentially composed of layers and weights that can be expressed as matrix multiplications. No matter how complex a matrix may be, matrix multiplication is a linear operation, which means that is impossible to model nonlinearities with matrix multiplication. This is why we need activation functions in neural networks. 





## Batch Softmax


```python
def softmax(x):
    result = []
    for instance in x:
        exp = np.exp(instance - np.max(instance))
        result.append(exp / exp.sum())
    return np.array(result)
```


```python
softmax([[10, 10], [1, 4]])
```




    array([[0.5       , 0.5       ],
           [0.04742587, 0.95257413]])



# ReLU


```python
def relu(x):
    x[x < 0] = 0
    return x
```


```python
x = np.linspace(-10, 10, 100)
y = relu(np.linspace(-10, 10, 100))
fig = plt.figure()
ax = fig.add_subplot(111, xlabel="x", ylabel="y", title='ReLU')
ax.plot(x, y, color='skyblue')
plt.show()
```


<img src="/assets/images/2020-02-05-neural-net_files/2020-02-05-neural-net_12_0.svg">



# Building the Model

```python
def init_network(n_features=2, n_class=2, n_hidden=64):
    model = {
        'W1': np.random.randn(n_features, n_hidden),
        'b1': np.random.randn(n_hidden),
        'W2': np.random.randn(n_hidden, n_class),
        'b2': np.random.randn(n_class)
    }
    return model
```



## Forward Propagation




```python
def forward(model, input_data):
    W1, W2 = model['W1'], model['W2']
    b1, b2 = model['b1'], model['b2']
    
    a1 = input_data @ W1 + b1
    z1 = relu(a1)
    a2 = z1 @ W2 + b2
    z2 = softmax(a2)

    return z1, z2
```



## Minibatch Processing




```python
def fit(model, input_data, label, batch_size, iter_num):
    for epoch in range(iter_num):
        p = np.random.permutation(len(label))
        input_data, label = input_data[p], label[p]
        for i in range(0, len(label), batch_size):
            batch_data, batch_label = input_data[i:i + batch_size], label[i:i + batch_size]
            model = sgd(model, batch_data, batch_label)
    return model
```


```python
def sgd(model, data, label, alpha=1e-4):
    grad = backward(model, data, label)
    for layer in grad.keys():
        model[layer] += alpha * grad[layer]
    return model
```

## Back Propagation


```python
def backward(model, data, label):
    z1, z2 = forward(model, data)
    label = one_hot_encode(label)
    db2_temp = label - z2
    db2 = np.sum(db2_temp, axis=0)
    dW2 = z1.T @ db2_temp
    db1_temp = db2_temp @ model['W2'].T
    db1_temp[z1 <= 0] = 0
    db1 = np.sum(db1_temp, axis=0)
    dW1 = data.T @ db1_temp
    return {'W1': dW1, 'b1': db1, 'W2': dW2, 'b2': db2}
```



# Testing the Model




```python
def test(train_data, train_label, test_data, test_label, batch_size, iter_num, n_experiment):
    acc_lst = []
    for k in range(n_experiment):
        model = init_network()
        model = fit(model, train_data, train_label, batch_size=batch_size, iter_num=iter_num)
        _, pred_label = forward(model, test_data)
        pred_label = np.array([np.argmax(pred) for pred in pred_label])
        acc_lst.append((pred_label == test_label).sum() / test_label.size)
    acc_lst = np.array(acc_lst)
    print('Mean Accuracy: {0:.5g}, Standard Deviation: {1:.5g}'.format(acc_lst.mean(), acc_lst.std()))
```


```python
test(X_train, y_train, X_test, y_test, 10, 10, 100)
```

    Mean Accuracy: 0.94541, Standard Deviation: 0.019558



## Class-based Implementation



```python
class NeuralNet:
    
    def __init__(self, n_hidden, n_features=2, n_class=2):
        self.params = {
            'W1': np.random.randn(n_features, n_hidden),
            'b1': np.random.randn(n_hidden),
            'W2': np.random.randn(n_hidden, n_class),
            'b2': np.random.randn(n_class)
        }
    
    def forward(self, input_data):
        W1, W2 = self.params['W1'], self.params['W2']
        b1, b2 = self.params['b1'], self.params['b2']
        
        a1 = input_data @ W1 + b1
        self.params['z1'] = relu(a1)
        a2 = self.params['z1'] @ W2 + b2
        self.params['z2'] = softmax(a2)
        
        return self.params['z1'], self.params['z2']
    
    def fit(self, input_data, label, batch_size, iter_num):
        for epoch in range(iter_num):
            p = np.random.permutation(len(label))
            input_data, label = input_data[p], label[p]
            for i in range(0, len(label), batch_size):
                batch_data, batch_label = input_data[i:i + batch_size], label[i:i + batch_size]
                self.sgd(batch_data, batch_label)
    
    def sgd(self, data, label, alpha=1e-4):
        grad = self.backward(data, label)
        for layer in grad.keys():
            self.params[layer] += alpha * grad[layer]
    
    def backward(self, data, label):
        W1, W2 = self.params['W1'], self.params['W2']
        b1, b2 = self.params['b1'], self.params['b2']
        z1, z2 = self.forward(data)
        
        label = one_hot_encode(label)
        db2_temp = label - z2
        db2 = np.sum(db2_temp, axis=0)
        dW2 = z1.T @ db2_temp
        db1_temp = db2_temp @ W2.T
        db1_temp[z1 <= 0] = 0
        db1 = np.sum(db1_temp, axis=0)
        dW1 = data.T @ db1_temp
        
        return {'W1': dW1, 'b1': db1, 'W2': dW2, 'b2': db2}
    
    def test(self, train_data, train_label, test_data, test_label, batch_size, iter_num):
        self.fit(train_data, train_label, batch_size=batch_size, iter_num=iter_num)
        _, pred_label = self.forward(test_data)
        pred_label = np.array([np.argmax(pred) for pred in pred_label])
        acc = (pred_label == test_label).sum() / test_label.size
        return acc
```


```python
net = NeuralNet(99)
net.test(X_train, y_train, X_test, y_test, 20, 10)
```




    0.9496




```python
def test_class(min_neuron, max_neuron, n_trial):
    acc_lst = []
    domain = np.arange(min_neuron, max_neuron)
    for n_neuron in domain:
        acc_ = []
        for _ in range(n_trial):
            net = NeuralNet(n_neuron)
            acc = net.test(X_train, y_train, X_test, y_test, 10, 100)
            acc_.append(acc)
        acc_score = sum(acc_) / len(acc_)
        acc_lst.append(acc_score)
    fig = plt.figure()
    ax = fig.add_subplot(111, xlabel="Number of Neurons", ylabel="Accuracy")
    ax.plot(domain, acc_lst, color='skyblue')
    plt.show()
```


```python
test_class(3, 40, 3)
```

<img src="/assets/images/2020-02-05-neural-net_files/2020-02-05-neural-net_23_0.svg">


$$

$$


$$A_1 = XW_1 + b_1$$
$$Z_1 = \text{max}(0, A_1)$$
$$A_2 = Z_1 W_2 + b_2$$
$$Z_2 = \sigma(A_2)$$
$$L = - \sum_{i = 1}^n y_i \log(z_i), \ z_i \in Z_2, y_i \in y$$
$$z_i = \sigma(a_i) = \frac{e^{a_i}}{\sum_{k = 1}^n e^{a_k}}, \ a_i \in A_2$$

$$\begin{align}
\frac{\partial z_i}{\partial a_i} &= \frac{\partial}{\partial a_i} \bigg[\frac{e^{a_i}}{\sum_{k = 1}^n e^{a_k}}\bigg] \\
&= \frac{e^{a_i}}{\sum_{k = 1}^n e^{a_k}} - e^{a_i} e^{a_i} \left(\sum_{k = 1}^n e^{a_k}\right)^{-2} \\
&= z_i - z_i^2 \\
&= z_i(1 - z_i)
\end{align}$$

$$\begin{align}
\frac{\partial z_i}{\partial a_{j \neq i}} &= \frac{\partial}{\partial a_j} \bigg[\frac{e^{a_i}}{\sum_{k = 1}^n e^{a_k}}\bigg] \\
&= - e^{a_i} e^{a_j} \left(\sum_{k = 1}^n e^{a_k}\right)^{-2} \\
&= - z_i z_j
\end{align}$$

$$\begin{align}
\frac{\partial L}{\partial a_i} &= \frac{\partial}{\partial a_i} \bigg[ - \sum_{k = 1}^m y_k \log(z_k) \bigg] \\ 
&= - \sum_{k = 1}^m y_k \frac{\partial \log(z_k)}{\partial a_i} \\
&= - \sum_{k = 1}^m y_k \frac{\partial \log(z_k)}{\partial z_k} \frac{\partial z_k}{\partial a_i} \\
&= - \sum_{k = 1}^m \frac{y_k}{z_k} \frac{\partial z_k}{\partial a_i} \\
&= - \bigg(\frac{y_i}{z_i}\frac{\partial z_i}{\partial a_i} + \sum_{k = 1, k \neq i}^m \frac{y_k}{z_k} \frac{\partial z_k}{\partial a_i} \bigg) \\
&= - \frac{y_i}{z_i} \cdot z_i(1 - z_i) + \sum_{k = 1, k \neq i}^m \frac{y_k}{z_k} \cdot z_k z_i \\
&= - y_i + y_i z_i + \sum_{k = 1, k \neq i}^m y_k z_i \\
&= z_i \left(y_i + \sum_{k = 1, k \neq i}^m y_k \right) - y_i \\
&= z_i \left(\sum_{k = 1}^m y_k \right) - y_i \\
&= z_i - y_i
\end{align}$$

$$ \frac{\partial L}{\partial A_2} = Z_2 - y$$

$$\frac{\partial L}{\partial W_2} = Z_1^T \frac{\partial L}{\partial A_2} = Z_1^T (Z_2 - y)$$

$$\frac{\partial L}{\partial b_2} = Z_2 - y$$

$$\frac{\partial L}{\partial W_1} = X^T I[(Z_2 - y)W_2^T]$$

$$I(\forall x \in X) = 
\begin{cases}
1 & x > 0 \\
0 & x \leq 0
\end{cases}$$
