---
title: Principal Component Analysis
mathjax: true
toc: true
categories:
  - study
tags:
  - statistics
  - linear_algebra

---

Principal component analysis is one of those techniques that I've always heard about somewhere, but didn't have a chance to really dive into. PCA would come up in papers on GANs, tutorials on unsupervised machine learning, and of course, math textbooks, whether it be on statistics or linear algebra. I decided that it's about time that I devote a post to this topic, especially since I promised one after writing about [singular value decomposition] on this blog some time ago. So here it goes.

# Why PCA?

What do we need principal component analysis for? Or more importantly, what *is* a principal component to begin with? 

Well, to cut to the chase, PCA is a way of implementing dimensionality reduction, often referred to as lossy compression. This simply means that we want to transform some data living in high dimensional space into lower dimensions. Imagine having a data with hundreds of thousands of feature columns. It would take a lot of computing power to apply a machine learning model to fit the data and generate predictions. This is when PCA comes in: with PCA, we can figure out which dimensions are the most important and apply a transformation to compress that data into lower dimensions, making it a lot more tractable and easier to work with.

And in case you're still wondering, principal components refer to those new extracted dimensions used to newly represent data!

# Derivation with Residuals

Let's derive PCA with some good old linear algebra tricks. Note that I used Ian Goodfellow's [Deep Learning](https://www.deeplearningbook.org) as reference for this post. 

## Setup

The setup of a classic PCA problem might be summarized as follows. Suppose we have a dataset of $m$ points, each living in $d$-dimensional space. In other words,


$$
D = \{x_1, x_2, \cdots , x_m\}
$$
where


$$
x_i = (x_i^{(1)}, x_i^{(2)} \cdots , x_i^{(d)})^\top
$$


Our goal is to find a way to compress the data into lower dimensional space $\mathbb{R}^l$ where $l < d$. We might imagine this as a transformation, *i.e.* the objective is to find a transformation 


$$
f: \mathbb{R}^d \mapsto \mathbb{R}^l
$$


So that applying $f(x_i)$ will yield a new vector $c$ living in lower dimensional space. We can also imagine there being a reverse transformation or a decoding function $g$ that achieves


$$
g(f(x_i)) = c_i \approx x_i \tag{1}
$$


Because PCA is in essence a linear transformation, it is most natural to express and understand it as a matrix. Let's define this transformation as $T$, and the matrix corresponding to the decoding $D$. In other words,


$$
c = f(x) = Tx \\x \approx g(c) = Dc \tag{2}
$$


PCA makes a number of assumptions to simplify this problem. The most important assumption is that each column of $D$ is orthogonal to each other. As we will see later in an alternate derivation with statistics, this has to do with the notion of covariance. Another restriction is that the columns of $D$ must have a Euclidean norm of one. This constraint is necessary for us to find a unique matrix $D$ that achieves compression---otherwise, we could have any multiples, leading to an infinite number of such matrices. 

We make one more convenient assumption about the given data points, $x$. That is, $x$ is assumed to have a mean of zero, *i.e.* $\mathbb{E}[x] = 0$. If this is not the case, we can easily perform standardization by subtracting the mean from the data. 

With this setup in mind, let's finally start the derivation.

## Minimizing Data Loss

As said earlier, the goal of PCA is to compress data (and be able to uncompress it) with as little loss of information as possible. We don't want to compress data in a haphazard fashion; instead, we want the compression scheme to be able to preserve the structure of the data as much as possible in its lower dimensional representation. From this, we can come up with the following equation:


$$
c^* = \mathop{\rm arg\,min}\limits_{c} \lVert x - g(c) \rVert_2 \tag{3}
$$


In other words, the goal is to find $c^*$ that which minimizes the difference between the original data and the reconstructed data. Note that finding this optimal $c^*$ amounts to finding $f$ that most effectively compresses given data.

Instead of the L2 norm, let's consider the squared L2 norm for convenience purposes. Note that minimizing the L2 norm is equal to minimizing the squared L2 norm, so there is no semantic difference. By definition of vector transpose, we can now express the squared L2 norm versions of (3) as follows:


$$
\begin{align}
c^* &= \mathop{\rm arg\,min}\limits_{c} (x - g(c))^\top(x - g(c)) \\ &= \mathop{\rm arg\,min}\limits_{c} x^\top x - x^\top g(c) - g(c)^\top x + g(c)^\top g(c) \\ &= \mathop{\rm arg\,min}\limits_{c} x^\top x - 2x^\top g(c) + g(c)^\top g(c) \\ &= \mathop{\rm arg\,min}\limits_{c} g(c)^\top g(c) - 2x^\top g(c)
\end{align} \tag{4}
$$


where the second to last equality is due to the fact that $x^\top g(c)$ and $g(c)^\top x$ are both constants that denote the same value. Also,  the argument of the minimum is with respect to $c$, we can omit the first term, which is purely in terms of $x$.

It's time to take derivatives. But in order to do so, we need to unpack $g(c)$ , since we have no idea how to take its derivative. Using (2), we can reorganize (4) as follows:


$$
\begin{align}
c^* &= \mathop{\rm arg\,min}\limits_{c} (Dc)^\top Dc - 2x^\top Dc \\ &= \mathop{\rm arg\,min}\limits_{c} c^\top D^\top Dc - 2x^\top Dc \\ &= \mathop{\rm arg\,min}\limits_{c} c^\top c - 2x^\top Dc
\end{align} \tag{5}
$$


The last equality is due to the fact that we constrained the columns of $D$ to be unit vectors that are orthogonal to each other. 

## Taking the Gradient

Now we can take a derivative of the argument with respect to $c$ and set it equal to zero to find the minimum.


$$
\nabla_c (c^\top c - 2 x^\top Dc) = 2c - 2 D^\top x \implies c = D^\top x \tag{6}
$$


This tells us that the optimal way of compressing $x$ is simply by multiplying it by the transpose of the decoding matrix. In other words, we have found the transformation $T$ in (2).

For those of you who are confused about how gradients and matrix calculus work, here is a very short explanation. First, notice that $c^\top c$ is just a scalar, since $c$ is a column vector. Taking a gradient with respect to this quantity would mean that we get another column vector of equal dimensions with $c$ with the following elements:


$$
\nabla_c c^\top c = (\frac{d c_1^2}{d c_1}, \frac{d c_2^2}{d c_2} \cdots ,\frac{d c_n^2}{d c_n})^\top \tag{7}
$$


And we know how to go from there. The same line of thinking can be applied to think about the second term, $2x^\top Dc$. We know that $2x\top D$ is a row vector its dot product with $c$ should be possible dimensionally speaking. Then, we know that the gradient with respect to $c$ should give each of the elements of $2x\top D$, but in column vector format---hence the need for a transpose. 

In general, the rule of thumb is that the gradient of a scalar with respect to a vector or a matrix should return a vector or matrix of the same dimension.

## Reconstructing Original Data

Recall from (1) that reconstruction can be achieved by applying compression followed by a decoding operation:


$$
g(f(x_i)) = c_i \approx x_i \tag{1}
$$


Since we know that $f$ is just $D^\top$ and $g$ is $D$ by definition, we can express (1) in a different way.


$$
DD^\top x \approx x \tag{8}
$$


In retrospect, this is somewhat intuitive since $D$ can roughly be thought of as a pseudo-orthonormal matrix---pseudo since there is no guarantee that it is a square matrix. 

Now, all that is left is to find the matrix $D$. The way to go about this is to reconsider (3), the notion of minimizing data loss, given our findings in (6). In other words, 


$$
D^* = \mathop{\rm arg\,min}\limits_{D} \lVert X - X DD^\top \rVert_F^2 \tag{9}
$$


Instead of considering a single observation, here we consider the design matrix in its entirety. Note that $X$ is a design matrix whose rows correspond to a single observation. And because we are dealing with matrices, the Euclidean norm was replaced with its matrix equivalent, the Frobenius norm. Observe that the first term $X$ can safely be removed from the argument since it is a constant with respect to $D$; let's also change the argument of the minimum to the maximum given the negative sign.


$$
D^* = \mathop{\rm arg\,max}\limits_{D} \lVert X DD^\top \rVert_F^2 \tag{10}
$$


The Frobenius norm of a real matrix can be calculated as


$$
\lVert A \rVert = \sqrt{\text{Tr}(A A^\top)} \tag{11}
$$


Therefore, 


$$
\begin{align}
D^* &= \mathop{\rm arg\,max}\limits_{D} \text{Tr}(X D D^\top D D^\top X^\top) \\ &= \mathop{\rm arg\,max}\limits_{D} \text{Tr}(X D D^\top X^\top) \\ &= \mathop{\rm arg\,max}\limits_{D} \text{Tr}(D^\top X^\top XD)
\end{align} \tag{12}
$$


The last equality is due to a useful property of trace, which is that we can cycle the order of matrices without changing its value. 

Let's consider a single column in $D$, denoted as $d$. You might also imagine this as a situation where $l$ is one-dimensional, meaning we want to compress data into a single scalar value. It is not difficult to see that the trace of $d^\top X^\top X d$, which is a scalar in the one-dimensional case, is maximized when $d$ is an eigenvector of $X^\top X$ with the largest eigenvalue.


$$
\begin{align}
\text{Tr}(d^\top X^\top X d) &= d^\top X^\top X d \\ &= d^\top \lambda d \\ &= \lambda \lVert d \rVert_2^2 \\ &= \lambda
\end{align} \tag{13}
$$


Generalizing this result back to $D$, we see that $D$ is a matrix whose columns correspond to the eigenvectors of $X^\top X$ in descending order. 

# Derivation with Covariance

If you had prior exposure to PCA, you might know that the standard way of obtaining principal components is by calculating the covariance matrix of the data and finding its eigenvectors. Here, I attempt to present an explanation of how and why the procedure outlined in the preceding section is essentially achieving the same tasks, albeit through a different frame of thought.

The unbiased sample covariance matrix is given by 


$$
\Sigma = \frac{1}{m - 1}X^\top X \tag{14}
$$


Of course, this is operating under the assumption that $X$ has already been standardized such that the mean of the data is zero.

You might be thinking that the formulation in (14) looks different from the one introduced previously on [this post](https://jaketae.github.io/study/gaussian-distribution/#covariance) on SVD. In that particular post, I stated that covariance could be calculated as


$$
\Sigma = \mathbb{E}[(X - \mathbb{E}[X])(X - \mathbb{E}[X])^\top] \tag{15}
$$


(14) and (15) certainly look different. However, under the hood, they express the same quantity.

- In (14), we assumed from the beginning that our data has a mean of zero. In (15), we make this assumption explicit by subtracting the mean from the data.
- In (14), we assumed a row-based design matrix, where each data point is stored as a row vector. In (15), we assumed that each data points are stored as columns of the design matrix; hence the difference in the order of transpose.
- In (14), we are dealing with the unbiased sample covariance matrix, which is why we divide by a fraction of $1/(m - 1)$. In (15), we simply express this division as an expectation encapsulating the entire expression.

So in a nutshell, the conclusion we arrived at in the preceding section with the minimization of residual sums ultimately amounts to finding the covariance matrix and its eigenvectors. I found this to be the more dominant interpretation of PCA, since indeed it is highly intuitive: the goal of PCA is to find the axes---or the principal components---that which maximize the variance seen in the data. 

[setosa.io](https://setosa.io/ev/principal-component-analysis/) has some excellent visualizations on the notion of covariance and how it relates to PCA, so I highly recommend that you go check it out.

If were to derive PCA from the gecko with the covariance approach, we would be iterating through the following process.
$$
\(\require{algorithm}
\begin{algorithm}
\caption{\small Minibatch stochastic gradient descent training of generative adversarial nets.
The number of steps to apply to the discriminator, $k$, is a hyperparameter. We used $k=1$, the
least expensive option, in our experiments.
}
\begin{algorithmic}
\label{alg:AGF}
\FOR{number of training iterations}
  \FOR{$k$ steps}
    \STATE{$\bullet$ Sample minibatch of $m$ noise samples $\{ \bm{z}^{(1)}, \dots, \bm{z}^{(m)} \}$ from noise prior $p_g(\bm{z})$.}
    \STATE{$\bullet$ Sample minibatch of $m$ examples $\{ \bm{x}^{(1)}, \dots, \bm{x}^{(m)} \}$ from data generating distribution $p_\text{data}(\bm{x})$.}
    \STATE{$\bullet$ Update the discriminator by ascending its stochastic gradient:
        \[
            \nabla_{\theta_d} \frac{1}{m} \sum_{i=1}^m \left[
            \log D\left(\bm{x}^{(i)}\right)
            + \log \left(1-D\left(G\left(\bm{z}^{(i)}\right)\right)\right)
            \right].
        \]}
    %parameters $\theta_d$ of discriminator $D$
   %in the direction of the stochastic gradient of the binomial cross-entropy
   %for $D$ predicting whether its argument comes from $p_\text{data}(\bm{x})$ (target = 1, input = $\bm{x}$) or
   %$P_g$ (target = 0, input = $G(\bm{z})$), i.e., towards minimizing
   % \mbox{$-\log D(\bm{x}) - \log(1 - D(G(\bm{z})))$}.}
   \ENDFOR
  \STATE{$\bullet$ Sample minibatch of $m$ noise samples $\{ \bm{z}^{(1)}, \dots, \bm{z}^{(m)} \}$ from noise prior $p_g(\bm{z})$.}
    \STATE{$\bullet$ Update the generator by descending its stochastic gradient:
        \[
            \nabla_{\theta_g} \frac{1}{m} \sum_{i=1}^m
            \log \left(1-D\left(G\left(\bm{z}^{(i)}\right)\right)\right)
            .
        \]}
  \ENDFOR
  \\The gradient-based updates can use any standard gradient-based learning rule. We used momentum in our experiments.
\end{algorithmic}
\end{algorithm}
\)
$$


# Relevance with SVD

In a previous post, 



