---
title: Hunyuan-Large
toc: true
categories: 
* AI news
* 
* 
tags:
  - AI
  - LLM
---

ğŸš€ Hunyuan-Large just released by Tencent: Largest ever open MoE LLM, only 52B active parameters but beats LLaMA 3.1-405B on most academic benchmarks

Key insights:

âš¡ Mixture of Experts (MoE) architecture: 389 B parameters in total, but only 52B are activated for any input

ğŸ§ª Trained on 7T tokens, including 1.5T tokens of synthetic data

ğŸ—ï¸ Architecture : Novel "recycle routing" prevents token dropping when experts are overrloaded

ğŸ“Š Great benchmark results: Surpasses Llama-3-405B-Instruct in most benchmarks although it has 8x fewer active parameters

â€£ Impressive perf on MATH: 77.4

ğŸ‹Â Large context length: up to 256K tokens

ğŸ”’ License:

â€£ Commercial use allowed, except if your products have >100M monthly active users

â€£ No access in the EU

ğŸ¤—Â Model weights available on HF!

Read the full paper here ğŸ‘‰Â [https://huggingface.co/papers/2411.02265](https://huggingface.co/papers/2411.02265)



![Capture dâ€™eÌcran 2024-11-05 aÌ€ 10.29.37.png](Capture_decran_2024-11-05_a_10.29.37.png)