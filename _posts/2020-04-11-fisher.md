---
title: Fisher Score and Information
mathjax: true
toc: true
categories:
  - study
tags:
  - statistics
---

Fisher's information is an interesting concept that connects many of the dots that we have explored so far: maximum likelihood estimation, gradient, Jacobian, and the Hessian, to name just a few. When I first came across Fisher's matrix a few months ago, I lacked the mathematical foundation to fully comprehend what it was. I'm still far from reaching that level of knowledge, but I thought I'd take a jab at it nonetheless. After all, I realized that sitting down to write a blog post about some concept forces me to study more, so it is a positive, self-reinforcing cycle. Let's begin.

# Fisher's Score

Fisher's score function is deeply related to maximum likelihood estimation. In fact, it's something that we already know--we just haven't defined it explicitly as Fisher's score before. 

## Maximum Likelihood Estimation

First, we begin with the definition of the likelihood function. Assume some dataset $X$ where each observation is identically and independently distributed according to a true underlying distribution parametrized by $\theta$. Given this probability density function $f_\theta(x)$, we can write the likelihood function as follows:


$$
\mathcal{L}(\theta \vert X) = \prod_{i=1}^n p(x_i \vert \theta)
$$


Then, we know that the maximum likelihood estimate of the distribution's parameter is given by


$$
\begin{align}
\theta_{MLE} 
&= \mathop{\rm arg\,max}\limits_{\theta} \mathcal{L}(\theta \vert X) \\
&= \mathop{\rm arg\,max}\limits_{\theta} \log \mathcal{L}(\theta \vert X) \\
&= \mathop{\rm arg\,max}\limits_{\theta} \sum_{i=1}^n \log p(x_i \vert \theta)
\end{align}
$$


This is the standard drill we already know. The next step, as we all know, is to take the derivative of the term in the argument maxima, set it equal to zero, and voila! We have found the maximum likelihood estimate of the parameter.

## Fisher's Score

Now here comes the definition of Fisher's score function, which really is nothing more than what we've done above. 
$$
u(\theta) = \nabla_\theta \log \mathcal{L}(\theta \vert X)
$$


In other words, we have already been implicitly using Fisher's score to find the maximum of the likelihood function all along, just without explicitly using the term. Fisher's score is simply the gradient or the derivative of the log likelihood function, which means that setting the score equal to zero gives us the maximum likelihood estimate of the parameter. 

## Expectation of Fisher's Score

An important characteristic to note about Fisher's score is the fact that the score evaluated the true value of the parameter equals zero. Concretely, this means that given a true parameter $\theta_0$, 


$$
\mathbb{E}_{\theta_0}[s(\theta)] = 0
$$


This might seem deceptively obvious: after all, the whole point of Fisher's score and maximum likelihood estimation is to find a parameter value that would set the gradient equal to zero. This is exactly what I had thought, but there are subtle intricacies taking place here that deserves our attention. So let's hash out exactly why the expectation of the score with respect to the true underlying distribution is zero.

To begin, let's write out the full expression of the expectation in integral form.


$$
\begin{align}
\mathbb{E}_{\theta_0}[s(\theta)] 
&= \int_{- \infty}^\infty \nabla_\theta \log p(x \vert \theta) \cdot p(x \vert \theta_0) \, dx \\
&= \int_{- \infty}^\infty \frac{\nabla_\theta p(x \vert \theta)}{p(x \vert \theta)} \cdot p(x \vert \theta_0) \, dx \\
\end{align}
$$


If we evaluate this integral at the true parameter, *i.e.* when $\theta = \theta_0$, 


$$
\begin{align}
\int_{- \infty}^\infty \frac{\nabla_\theta p(x \vert \theta_0)}{p(x \vert \theta_0)} \cdot p(x \vert \theta_0) \, dx 
&= \int_{- \infty}^\infty \nabla_\theta p(x \vert \theta_0) \, dx \\
&= \nabla_\theta \int_{- \infty}^\infty p(x \vert \theta_0) \, dx \\
&= 0
\end{align}
$$


The key part of this derivation is the use of the Leibniz rule, or sometimes known as Feynman's technique or differentiation under the integral sign. I am most definitely going to write a post detailing in intuitive explanation behind why this operation makes sense in the future, but to prevent unnecessary divergence, for now it suffices to use that rule to show that the expected value of Fisher's score is zero at the true parameter.

# Fisher's Information Matrix

Things start to get a little more interesting (and more complicated) as we move onto the discussion of Fisher's Information Matrix.  There are two sides of the coin that we will consider in this discussion: Fisher's information as understood as the covariance matrix of the score function, and Fisher's information as understood as a Hessian of the negative log likelihood. The gist of it is that there are two different ways of understanding the same concept, and that they provide intriguing complementary views on the information matrix. 

## Covariance

Before jumping into anything else, perhaps it's instructive to review core concepts like variance, covariance, and the covariance matrix. Here is a little cheat sheet to help you out (and my future self, who will most likely be reviewing this later).

An intuitive way to think about variance is to consider it as a measure of how far samples are from the mean. We square that quantity to prevent negative values from canceling out positive ones. 


$$
\begin{align}
\text{Var}(X) 
&= \mathbb{E}[(X - \mu)^2] \\
&= \mathbb{E}[(X - \mathbb{E}[X])^2]\\
&= \mathbb{E}[X^2] - \mathbb{E}[X]^2
\end{align}
$$


Covariance is just an extension of this concept applied to a comparison of two random variables instead of one. Here, we consider how two variables move in tandem.


$$
\begin{align}
\text{Cov}[X, Y] 
&= \mathbb{E}[(X - \mu_x)(Y - \mu_y)] \\
&= \mathbb{E}[(X - \mathbb{E}[X])(Y - \mathbb{E}[Y])] \\
&= \mathbb{E}[XY] - \mathbb{E}[X]\mathbb{E}[Y]
\end{align}
$$


And the variance-covariance matrix is simply a matrix that contains information on the covariance of multiple random variables in a neat, compact matrix form. 


$$
K = 
\begin{pmatrix}
\text{Cov}[X_1, X_1] & \text{Cov}[X_1, X_2]& \cdots & \text{Cov}[X_1, X_n] \\
\text{Cov}[X_2, X_1] & \text{Cov}[X_2, X_2]& \cdots & \text{Cov}[X_2, X_n] \\
\vdots & \vdots & \ddots & \vdots \\
\text{Cov}[X_n, X_1] & \text{Cov}[X_n, X_2]& \cdots & \text{Cov}[X_n, X_n] \\
\end{pmatrix}
$$


A closed-form expression for the covariance matrix, which follows straight from aforementioned definitions and some linear algebra, looks as follows:


$$
K = \mathbb{E}[(X - \mathbb{E}[X])(X - \mathbb{E}[X])^\top]
$$


Enough of the prologue and review, now we're ready to start talking about Fisher.

## Fisher's Information

The information matrix is defined as the covariance matrix as the score function as a random vector. Concretely,


$$
I(\theta) = \mathbb{E}[(s(\theta) - 0)(s(\theta) - 0)^\top] = \mathbb{E}[s(\theta)s(\theta)^\top]
$$


Recall that the score function tells us the derivative of the likelihood function with respect to the parameters of a distribution. Covariance would then tell us whether these derivatives are moving in the same direction or not. 

(Writing in progress)

## 













