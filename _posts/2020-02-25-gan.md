---
title: My First GAN
mathjax: true
toc: true
categories:
  - study
tags:
  - deep_learning
  - tensorflow
---

In this post, we will take a look at generative adversarial networks, or GANs for short.



```python
import numpy as np
import tensorflow.keras.datasets as tfds
from tensorflow.keras import layers
from tensorflow.keras import optimizers
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.utils import plot_model
from tensorflow.keras.preprocessing import image
%matplotlib inline
%config InlineBackend.figure_format = 'svg'
import matplotlib.pyplot as plt
plt.style.use('seaborn')
```



```python
latent_dim = 128
height = 32
width = 32
channels = 3
```


```python
def build_generator(latent_dim, height, width, channels):
  inputs = layers.Input(shape=(latent_dim,))
  x = layers.Dense(128 * 16 * 16)(inputs)
  x = layers.LeakyReLU()(x)
  x = layers.Reshape((16, 16, 128))(x)
  x = layers.Conv2D(256, 5, padding='same')(x)
  x = layers.LeakyReLU()(x)
  x = layers.Conv2DTranspose(256, 4, strides=2, padding='same')(x) # Upsample to 32x32
  x = layers.LeakyReLU()(x)
  x = layers.Conv2D(256, 5, padding='same')(x)
  x = layers.LeakyReLU()(x)
  x = layers.Conv2D(256, 5, padding='same')(x)
  x = layers.LeakyReLU()(x)
  output = layers.Conv2D(channels, 7, activation='tanh', padding='same')(x) # Produce a 32x32 1-channel feature map
  model = Model(inputs, output)
  print(model.summary())
  return model
```


```python
generator = build_generator(latent_dim, height, width, channels)
plot_model(generator, show_shapes=True, show_layer_names=True)
```

    WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
    Instructions for updating:
    If using Keras pass *_constraint arguments to layers.
    Model: "model"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    input_1 (InputLayer)         [(None, 128)]             0         
    _________________________________________________________________
    dense (Dense)                (None, 32768)             4227072   
    _________________________________________________________________
    leaky_re_lu (LeakyReLU)      (None, 32768)             0         
    _________________________________________________________________
    reshape (Reshape)            (None, 16, 16, 128)       0         
    _________________________________________________________________
    conv2d (Conv2D)              (None, 16, 16, 256)       819456    
    _________________________________________________________________
    leaky_re_lu_1 (LeakyReLU)    (None, 16, 16, 256)       0         
    _________________________________________________________________
    conv2d_transpose (Conv2DTran (None, 32, 32, 256)       1048832   
    _________________________________________________________________
    leaky_re_lu_2 (LeakyReLU)    (None, 32, 32, 256)       0         
    _________________________________________________________________
    conv2d_1 (Conv2D)            (None, 32, 32, 256)       1638656   
    _________________________________________________________________
    leaky_re_lu_3 (LeakyReLU)    (None, 32, 32, 256)       0         
    _________________________________________________________________
    conv2d_2 (Conv2D)            (None, 32, 32, 256)       1638656   
    _________________________________________________________________
    leaky_re_lu_4 (LeakyReLU)    (None, 32, 32, 256)       0         
    _________________________________________________________________
    conv2d_3 (Conv2D)            (None, 32, 32, 3)         37635     
    =================================================================
    Total params: 9,410,307
    Trainable params: 9,410,307
    Non-trainable params: 0
    _________________________________________________________________
    None





<img src="/assets/images/2020-02-25-gan_files/2020-02-25-gan_3_1.png">




```python
def build_discriminator(height, width, channels):
  inputs = layers.Input(shape=(height, width, channels))
  x = layers.Conv2D(128, 3)(inputs)
  x = layers.LeakyReLU()(x)
  x = layers.Conv2D(128, 4, strides=2)(x)
  x = layers.LeakyReLU()(x)
  x = layers.Conv2D(128, 4, strides=2)(x)
  x = layers.LeakyReLU()(x)
  x = layers.Conv2D(128, 4, strides=2)(x)
  x = layers.LeakyReLU()(x)
  x = layers.Flatten()(x)
  x = layers.Dropout(0.4)(x) # One dropout layer - important trick!
  output = layers.Dense(1, activation='sigmoid')(x)
  model = Model(inputs, output)
  optimizer = optimizers.RMSprop(lr=0.0008, clipvalue=1.0, decay=1e-8)
  model.compile(optimizer=optimizer, loss='binary_crossentropy')
  print(model.summary())
  return model
```


```python
discriminator = build_discriminator(height, width, channels)
plot_model(discriminator, show_shapes=True, show_layer_names=True)
```

    WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
    Instructions for updating:
    Use tf.where in 2.0, which has the same broadcast rule as np.where
    Model: "model_1"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    input_2 (InputLayer)         [(None, 32, 32, 3)]       0         
    _________________________________________________________________
    conv2d_4 (Conv2D)            (None, 30, 30, 128)       3584      
    _________________________________________________________________
    leaky_re_lu_5 (LeakyReLU)    (None, 30, 30, 128)       0         
    _________________________________________________________________
    conv2d_5 (Conv2D)            (None, 14, 14, 128)       262272    
    _________________________________________________________________
    leaky_re_lu_6 (LeakyReLU)    (None, 14, 14, 128)       0         
    _________________________________________________________________
    conv2d_6 (Conv2D)            (None, 6, 6, 128)         262272    
    _________________________________________________________________
    leaky_re_lu_7 (LeakyReLU)    (None, 6, 6, 128)         0         
    _________________________________________________________________
    conv2d_7 (Conv2D)            (None, 2, 2, 128)         262272    
    _________________________________________________________________
    leaky_re_lu_8 (LeakyReLU)    (None, 2, 2, 128)         0         
    _________________________________________________________________
    flatten (Flatten)            (None, 512)               0         
    _________________________________________________________________
    dropout (Dropout)            (None, 512)               0         
    _________________________________________________________________
    dense_1 (Dense)              (None, 1)                 513       
    =================================================================
    Total params: 790,913
    Trainable params: 790,913
    Non-trainable params: 0
    _________________________________________________________________
    None





<img src="/assets/images/2020-02-25-gan_files/2020-02-25-gan_5_1.png">




```python
discriminator.trainable = False
gan_input = layers.Input(shape=(latent_dim,))
gan_output = discriminator(generator(gan_input))
gan = Model(gan_input, gan_output)
optimizer = optimizers.RMSprop(lr=0.0004, clipvalue=1.0, decay=1e-8)
gan.compile(optimizer=optimizer, loss='binary_crossentropy')
print(gan.summary())
plot_model(gan, show_shapes=True, show_layer_names=True)
```

    Model: "model_2"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    input_3 (InputLayer)         [(None, 128)]             0         
    _________________________________________________________________
    model (Model)                (None, 32, 32, 3)         9410307   
    _________________________________________________________________
    model_1 (Model)              (None, 1)                 790913    
    =================================================================
    Total params: 10,201,220
    Trainable params: 9,410,307
    Non-trainable params: 790,913
    _________________________________________________________________
    None





<img src="/assets/images/2020-02-25-gan_files/2020-02-25-gan_6_1.png">




```python
def load_dataset(class_label):
  (X_train, y_train), (X_test, y_test) = tfds.cifar10.load_data()
  X_train = X_train[y_train.flatten() == class_label]
  X_test = X_test[y_test.flatten() == class_label]
  X_train = X_train.astype('float64') / 255.0
  X_test = X_test.astype('float64') / 255.0
  combined_data = np.concatenate([X_train, X_test])
  
  return combined_data
```


```python
train_image = load_dataset(8)
```


```python
train_image.shape
```




    (6000, 32, 32, 3)




```python
def train_gan(batch_size, iter_num, latent_dim, data, gan_model, generator_model, discriminator_model):
  start = 0
  d_loss_lst, gan_loss_lst = [], []
  for step in range(iter_num):
    random_latent_vectors = np.random.normal(size=(batch_size, latent_dim))
    generated_images = generator_model.predict(random_latent_vectors)
    real_images = data[start: start + batch_size]
    combined_images = np.concatenate([generated_images, real_images])
    labels = np.concatenate([np.zeros((batch_size, 1)),
                             np.ones((batch_size, 1))])
    labels += 0.05 * np.random.random(labels.shape)
    d_loss = discriminator_model.train_on_batch(combined_images, labels)
    d_loss_lst.append(d_loss)

    random_latent_vectors = np.random.normal(size=(batch_size, latent_dim))
    misleading_labels = np.ones((batch_size, 1))
    gan_loss = gan_model.train_on_batch(random_latent_vectors, misleading_labels)
    gan_loss_lst.append(gan_loss)

    if step % 200 == 0:
      print("Iteration {0}/{1}".format(step, iter_num))
      print("[==============================] d-loss: {0:.3f}, gan-loss: {1:.3f}".format(d_loss_lst[-1], gan_loss_lst[-1]))
    
    start += batch_size
    if start > len(data) - batch_size:
      start = 0

  return gan_model, generator_model, discriminator_model, d_loss_lst, gan_loss_lst
```


```python
batch_size = 32
iter_num = 10000

gan, generator, discriminator, d_history, gan_history = train_gan(batch_size, iter_num, latent_dim, train_image, gan, generator, discriminator)
```

    WARNING:tensorflow:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?
    Iteration 0/10000
    [==============================] d-loss: 0.679, gan-loss: 0.736
    Iteration 200/10000
    [==============================] d-loss: 0.560, gan-loss: 2.285
    Iteration 400/10000
    [==============================] d-loss: 0.678, gan-loss: 0.801
    Iteration 600/10000
    [==============================] d-loss: 0.556, gan-loss: 2.400
    Iteration 800/10000
    [==============================] d-loss: 0.695, gan-loss: 0.705
    Iteration 1000/10000
    [==============================] d-loss: 0.699, gan-loss: 0.652
    Iteration 1200/10000
    [==============================] d-loss: 0.718, gan-loss: 0.606
    Iteration 1400/10000
    [==============================] d-loss: 0.706, gan-loss: 0.679
    Iteration 1600/10000
    [==============================] d-loss: 0.675, gan-loss: 0.702
    Iteration 1800/10000
    [==============================] d-loss: 0.651, gan-loss: 0.668
    Iteration 2000/10000
    [==============================] d-loss: 0.748, gan-loss: 0.805
    Iteration 2200/10000
    [==============================] d-loss: 0.682, gan-loss: 0.729
    Iteration 2400/10000
    [==============================] d-loss: 0.402, gan-loss: 3.102
    Iteration 2600/10000
    [==============================] d-loss: 0.672, gan-loss: 0.665
    Iteration 2800/10000
    [==============================] d-loss: 0.659, gan-loss: 0.534
    Iteration 3000/10000
    [==============================] d-loss: 0.686, gan-loss: 0.679
    Iteration 3200/10000
    [==============================] d-loss: 0.645, gan-loss: 0.679
    Iteration 3400/10000
    [==============================] d-loss: 0.681, gan-loss: 0.728
    Iteration 3600/10000
    [==============================] d-loss: 0.792, gan-loss: 1.180
    Iteration 3800/10000
    [==============================] d-loss: 0.687, gan-loss: 0.897
    Iteration 4000/10000
    [==============================] d-loss: 0.791, gan-loss: 1.159
    Iteration 4200/10000
    [==============================] d-loss: 0.695, gan-loss: 0.680
    Iteration 4400/10000
    [==============================] d-loss: 0.671, gan-loss: 0.706
    Iteration 4600/10000
    [==============================] d-loss: 0.702, gan-loss: 0.811
    Iteration 4800/10000
    [==============================] d-loss: 0.697, gan-loss: 0.634
    Iteration 5000/10000
    [==============================] d-loss: 0.759, gan-loss: 0.802
    Iteration 5200/10000
    [==============================] d-loss: 0.677, gan-loss: 0.740
    Iteration 5400/10000
    [==============================] d-loss: 0.701, gan-loss: 0.663
    Iteration 5600/10000
    [==============================] d-loss: 0.670, gan-loss: 0.598
    Iteration 5800/10000
    [==============================] d-loss: 0.615, gan-loss: 0.756
    Iteration 6000/10000
    [==============================] d-loss: 0.677, gan-loss: 0.626
    Iteration 6200/10000
    [==============================] d-loss: 0.669, gan-loss: 0.767
    Iteration 6400/10000
    [==============================] d-loss: 0.682, gan-loss: 0.644
    Iteration 6600/10000
    [==============================] d-loss: 0.742, gan-loss: 0.955
    Iteration 6800/10000
    [==============================] d-loss: 0.701, gan-loss: 0.680
    Iteration 7000/10000
    [==============================] d-loss: 0.303, gan-loss: 7.814
    Iteration 7200/10000
    [==============================] d-loss: 0.596, gan-loss: 0.847
    Iteration 7400/10000
    [==============================] d-loss: 0.717, gan-loss: 0.770
    Iteration 7600/10000
    [==============================] d-loss: 0.707, gan-loss: 0.742
    Iteration 7800/10000
    [==============================] d-loss: 0.697, gan-loss: 0.795
    Iteration 8000/10000
    [==============================] d-loss: 0.647, gan-loss: 0.672
    Iteration 8200/10000
    [==============================] d-loss: 0.676, gan-loss: 0.725
    Iteration 8400/10000
    [==============================] d-loss: 0.608, gan-loss: 1.050
    Iteration 8600/10000
    [==============================] d-loss: 0.757, gan-loss: 0.824
    Iteration 8800/10000
    [==============================] d-loss: 0.614, gan-loss: 0.758
    Iteration 9000/10000
    [==============================] d-loss: 0.660, gan-loss: 0.647
    Iteration 9200/10000
    [==============================] d-loss: 0.651, gan-loss: 1.122
    Iteration 9400/10000
    [==============================] d-loss: 0.710, gan-loss: 0.991
    Iteration 9600/10000
    [==============================] d-loss: 0.734, gan-loss: 0.901
    Iteration 9800/10000
    [==============================] d-loss: 0.681, gan-loss: 0.899



```python
def show_generated_image(generator_model, latent_dim, row_num=4):
  num_image = row_num**2
  random_latent_vectors = np.random.normal(size=(num_image, latent_dim))
  generated_images = generator_model.predict(random_latent_vectors)
  plt.figure(figsize=(10,10))
  for i in range(num_image):
    img = image.array_to_img(generated_images[i] * 255., scale=False)
    plt.subplot(row_num,row_num,i+1)
    plt.grid(False)
    plt.xticks([]); plt.yticks([])
    plt.imshow(img)
  plt.show()
```


```python
show_generated_image(generator, latent_dim)
```


<img src="/assets/images/2020-02-25-gan_files/2020-02-25-gan_13_0.svg">



```python
def plot_learning_curve(d_loss_lst, gan_loss_lst):
  fig = plt.figure()
  plt.plot(d_loss_lst, color='skyblue')
  plt.plot(gan_loss_lst, color='gold')
  plt.title('Model Learning Curve')
  plt.xlabel('Epochs'); plt.ylabel('Cross Entropy Loss')
  plt.show()
```


```python
plot_learning_curve(d_history, gan_history)
```


<img src="/assets/images/2020-02-25-gan_files/2020-02-25-gan_15_0.svg">

