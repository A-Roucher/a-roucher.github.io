---
title: A Step Up with Convolutional Variational Autoencoders
mathjax: true
toc: true
categories:
  - study
tags:
  - deep_learning
  - tensorflow
---

In a [previous post](), we took a look at autoencoders, a type of neural network that receives some data as input, encodes them into a latent representation, and decodes this information to restore the original input. Autoencoders are exciting in and of themselves, but things can get a lot more interesting if we apply a bit of twist. In this post, we will take a look at one of the many flavors of the autoencoder model, known as [variational autoencoders](https://arxiv.org/pdf/1606.05908.pdf), or VAE for short. Specifically, the model that we will build in this tutorial is a convolutional variational Autoencoder, since we will be using convolutional layers for better image processing.

The model architecture introduced in this tutorial was heavily inspired by the one outlined in Fran√ßois Chollet's [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python), as well as that from a separate article on the [Keras blog](https://blog.keras.io/building-autoencoders-in-keras.html). 

# Setup

Let's start by importing the modules necessary for this demonstration. 

```python
from tensorflow.keras import datasets
from tensorflow.keras import backend as K
from tensorflow.keras.layers import *
from tensorflow.keras.models import Model
from tensorflow.keras.utils import plot_model
from tensorflow.keras import callbacks
from tensorflow.keras.losses import binary_crossentropy
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
%config InlineBackend.figure_format = 'svg'
plt.style.use('seaborn')
```

The objective of today's task is to build an autoencoder model that produces MNIST hand-written digits. The hidden dimension, or the latent space of the model, is going to a random vector living in two-dimensional space. Let's specify this setup, along with some other miscellaneous configurations, before we proceed with constructing the model architecture.


```python
image_shape = (28, 28, 1)
batch_size = 32
latent_dim = 2
kernel_size = 3
filters = 16
epochs = 30
```

# The Model

It's time to build our model... or not quite now. Before we start stacking layers for the encoder and the decoder, we need to define a sampling function that will perform the meat of the variational inference involved in VAE. 

## Sampling Function

Let's start out by taking a look at the sampling function we will use to define one of the layers of the variational Autoencoder network. 


```python
def sampling(args):
    z_mean, z_log_var = args
    batch = K.shape(z_mean)[0]
    dim = K.int_shape(z_mean)[1]
    epsilon = K.random_normal(shape=(batch, dim))
    return z_mean + K.exp(0.5 * z_log_var) * epsilon
```

Simply put, the `sampling()` above below takes as arguments `z_mean` and `z_log_var` in the form of a bundled list. As you can guess from the name of the variables, these two  parameters refer to the mean and log variance of the random vector living in our predefined latent space. Note that we are assuming a diagonal Gaussian here: in other words, the covariance matrix of the multi-dimensional Gaussian is assumed to be diagonal, meaning that each elements of the vector are independent. If any of this sounds foreign to you, I recommend that you read [this post](https://jaketae.github.io/study/gaussian-distribution/) on the Gaussian distribution.

Let's continue our discussion with the sampling function. The goal here is to sample a random vector in the latent space from the distribution specified by the two parameters, mean and log variance. The sampling process can be expressed as follows:


$$
z = \mu_z + \epsilon \cdot \sigma_z \tag{1}
$$


where $\mu_z$ denotes the mean, corresponding to `z_mean`, $\epsilon$ denotes a tensor of random numbers sampled from the standard normal distribution, and $\sigma_z$ denotes the standard deviation (we will see how this is related to `z_log_var` in just a moment). Essentially, the goal here is to use a resampling technique such that we can sample from a standard normal distribution centered around mean 0 and a standard deviation of 1, but consequentially sample from a distribution of $z$ living in the latent space. 

If you are wondering how (1) translates to the return statement,

```python
z_mean + K.exp(0.5 * z_log_var) * epsilon
```

then the following equation might resolve your curiosity. This is the promised elaboration on the relationship between log variance and standard deviation:



$$
\begin{align}
\text{exp}\left(0.5 \cdot \log \sigma^2 \right) &= \text{exp}(0.5 \cdot 2 \log \sigma) \\ &= \text{exp}(\log \sigma) \\ &= \sigma
\end{align} \tag{2}
$$



Therefore, multiplying 0.5 is just a simple algebraic manipulation to morph log variance to standard deviation. The reason why we use log variance instead of just variance or standard deviation is to ensure numerical stability in computation. 

Now that this part has been cleared, let's start stacking away layers!

## The Encoder Network

Just like the autoencoder, VAEs are composed of two discrete components: the encoder and the decoder. Here, we take a look at the first piece of the puzzle, the encoder network. 

There are several things to note about this model. First, I decided to use a `for` loop to simplify the process of stacking layers. Instead of repeating the same code over multiple lines, I found this approach to be more succinct and concise. Second, we define a custom layer at the end, shown as `Lambda`, that uses the `sampling` function we defined earlier. This is the final key that enables us to build an encoder model that receives as input a 28-by-28 image, then output a two-dimensional latent vector representation of that image to pass onto the decoder network.

```python
inputs = Input(shape=image_shape)
x = inputs
for i in range(2):
    filters *= 2
    x = Conv2D(filters=filters,
               kernel_size=kernel_size,
               activation='relu',
               strides=2,
               padding='same')(x)
shape = K.int_shape(x)
x = Flatten()(x)
x = Dense(16, activation='relu')(x)
z_mean = Dense(latent_dim, name='z_mean')(x)
z_log_var = Dense(latent_dim, name='z_log_var')(x)

z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])

encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')
encoder.summary()
```

Below is the summary of what our model looks like. Note that the model outputs a total of three quantities: `z_mean`, `z_log_var`, and `z`. We need the first two parameters to later sample from the latent distribution; `z `, of course, is needed to train the decoder.

    Model: "encoder"
    __________________________________________________________________________________________________
    Layer (type)                    Output Shape         Param #     Connected to                     
    ==================================================================================================
    input_1 (InputLayer)            [(None, 28, 28, 1)]  0                                            
    __________________________________________________________________________________________________
    conv2d (Conv2D)                 (None, 14, 14, 32)   320         input_1[0][0]                    
    __________________________________________________________________________________________________
    conv2d_1 (Conv2D)               (None, 7, 7, 64)     18496       conv2d[0][0]                     
    __________________________________________________________________________________________________
    flatten (Flatten)               (None, 3136)         0           conv2d_1[0][0]                   
    __________________________________________________________________________________________________
    dense (Dense)                   (None, 16)           50192       flatten[0][0]                    
    __________________________________________________________________________________________________
    z_mean (Dense)                  (None, 2)            34          dense[0][0]                      
    __________________________________________________________________________________________________
    z_log_var (Dense)               (None, 2)            34          dense[0][0]                      
    __________________________________________________________________________________________________
    z (Lambda)                      (None, 2)            0           z_mean[0][0]                     
                                                                     z_log_var[0][0]                  
    ==================================================================================================
    Total params: 69,076
    Trainable params: 69,076
    Non-trainable params: 0
    __________________________________________________________________________________________________

## The Decoder Network

The decoder network looks similar to the the encoder, except that much of the architecture is in reverse order. Most notably, we use `Conv2DTranpose` to undo the convolution done by the encoder. This allows us to effectively scale up the input back to its original dimension, which is what we want to do with a generative model like a VAE.

One subtly worth mentioning is the fact that we use a sigmoid activation in the end. This is because we want the pixel values of the output to be between 0 and 1, just as the original input was normalized before it was fed into the encoder network via division by 255. 

```python
latent_inputs = Input(shape=(latent_dim,))
x = Dense(np.prod(shape[1:]), activation='relu')(latent_inputs)
x = Reshape(shape[1:])(x)
for i in range(2):
    x = Conv2DTranspose(filters=filters,
                        kernel_size=kernel_size,
                        activation='relu',
                        strides=2,
                        padding='same')(x)
    filters //= 2

outputs = Conv2DTranspose(filters=1,
                          kernel_size=kernel_size,
                          activation='sigmoid',
                          padding='same')(x)

decoder = Model(latent_inputs, outputs)
decoder.summary()
```

The summary of the decoder network is presented below:

    Model: "decoder"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    input_2 (InputLayer)         [(None, 2)]               0         
    _________________________________________________________________
    dense_1 (Dense)              (None, 3136)              9408      
    _________________________________________________________________
    reshape (Reshape)            (None, 7, 7, 64)          0         
    _________________________________________________________________
    conv2d_transpose (Conv2DTran (None, 14, 14, 64)        36928     
    _________________________________________________________________
    conv2d_transpose_1 (Conv2DTr (None, 28, 28, 32)        18464     
    _________________________________________________________________
    conv2d_transpose_2 (Conv2DTr (None, 28, 28, 1)         289       
    =================================================================
    Total params: 65,089
    Trainable params: 65,089
    Non-trainable params: 0
    _________________________________________________________________

## The Variational Autoencoder

Now that we have both the encoder and the decode network fully defined, it's time to wrap them together into one autoencoder model. This can simply achieved by defining the input as the input of the encoder---the normalized MNIST images---and defining the output as the output of the decoder when fed a latent vector. Concretely, this process might look as follows:

```python
outputs = decoder(encoder(inputs)[2])
vae = Model(inputs, outputs)
vae.summary()
```

Let's look a the summary of the CVAE. Note that the encoder and the decoder look like individual layers in the grand scheme of the VAE architecture.

    Model: "model_1"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    input_1 (InputLayer)         [(None, 28, 28, 1)]       0         
    _________________________________________________________________
    encoder (Model)              [(None, 2), (None, 2), (N 69076     
    _________________________________________________________________
    decoder (Model)              (None, 28, 28, 1)         65089     
    =================================================================
    Total params: 134,165
    Trainable params: 134,165
    Non-trainable params: 0
    _________________________________________________________________

# The Loss Function

We have almost everything we need, but there is one crucial step that is missing: compiling the model with an optimizer and a loss function. Normally, defining a loss function is very easy: in most cases, we  use pre-made loss functions that are available through the TensorFlow API, such as cross entropy or mean squared error. In the case of variational autoencoders, however, this is not such an easy task: how do we judge the robustness or the effectiveness of the decoder, which is essentially a generative algorithm? Of course, we could stop training once the figures it generates becomes reasonable, *i.e.* the mock MNIST digits it creates looks compelling to the human eye. However, this is a subjective metric at best, and we can't expect there to be a ML engineer peering at the screen, looking at the outputs of the decoder per each epoch. 

To tackle this challenge, we need to dive into some math. Let's take a look.

## Back to Bayes

First, let's carefully review what our goal is for this task. The motivating idea behind variational autoencoders is that we want to model a specific distribution, namely the distribution of the latent space given some input. As you recall, this latent space is a two dimensional vector modeled as a multivariate diagonal Gaussian. Using Bayes' theorem, we can express this distribution as follows:


$$
\begin{align}
p(z \vert x) &= \frac{p(x, z)}{p(x)} \\
&= \frac{p(x \vert z) p(z)}{p(x)} \\
&= \frac{p(x \vert z) p(z)}{\int p(x \vert z) \, dz}
\end{align} \tag{3}
$$


By now, it is pretty clear what the problem its: the evidence sitting in the denominator is intractable. Therefore, we cannot directly calculate or derive $p(z \vert x)$ in its closed form; hence the need for variational inference.

## Kullback-Leibler Divergence

The best we can do is to find a distribution $q(z \vert x)$ that best approximates $p(z \vert x)$. How do we find this distribution? Well, we know one handy concept that measures the difference or the pseudo-distance between two distributions, and that is Kullback-Leibler divergence. As we discussed in [this post](https://jaketae.github.io/study/information-entropy/) on entropy, KL divergence tells us how different two distributions are. So the goal here would be find a distribution that minimizes the following expression:


$$
D_{KL}[q(z \vert x) \parallel p(z \vert x)] = \mathbb{E}_{q}[\log q(z \vert x) - \log p(z \vert x)] \tag{4}
$$


Using (1), we can simplify (4) as follows:


$$
\begin{align}
D_{KL}[q(z \vert x) \parallel p(z \vert x)] &= \mathbb{E}_{q}[\log q(z \vert x) - \log p(x, z) - \log p(x)] \\ &= \mathbb{E}_{q}[\log q(z \vert x) - \log p(x, z)] - \log p(x)
\end{align} \tag{5}
$$


Our goal, then is to find $q$ that minimizes this quantity:


$$
q^*(z \vert x) = \mathop{\rm arg\,min}\limits_{q} \mathbb{E}_{q}[\log q(z \vert x) - \log p(x, z)] - \log p(x) \tag{6}
$$


Because the last term $\log p(x)$ is a constant that does not depend on any variable of interest, we can safely remove it from the operation.


$$
q^*(z \vert x) = \mathop{\rm arg\,min}\limits_{q} \mathbb{E}_{q}[\log q(z \vert x) - \log p(x, z)] \tag{7}
$$


Removing $p(x)$ is very convenient, since the evidence term is what made direct  inference impossible. Reframing the task a a KL divergence problem simplifies our job since we do not have to consider evidence---the intractable integral over the entire latent space---anymore.







```python
xent_loss = 28 * 28 * binary_crossentropy(K.flatten(inputs), 
                                     K.flatten(outputs))

kl_loss = -0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)
vae_loss = K.mean(xent_loss + kl_loss)
vae.add_loss(vae_loss)
vae.compile(optimizer='adam')
```



https://jaan.io/what-is-variational-autoencoder-vae-tutorial/

https://towardsdatascience.com/variational-inference-derivation-of-the-variational-autoencoder-vae-loss-function-a-true-story-3543a3dc67ee





# Testing the Model

It's finally time to test the model. Let's first begin with data preparation and preprocessing. 

```python
def load_data():
  (X_train, y_train), (X_test, y_test) = datasets.mnist.load_data()
  X_train = np.reshape(X_train, (len(X_train), 28, 28, 1))
  X_test = np.reshape(X_test, (len(X_test), 28, 28, 1))
  X_train, X_test = X_train.astype('float64') / 255., X_test.astype('float64') / 255.
  return X_train, y_train, X_test, y_test
```


```python
X_train, y_train, X_test, y_test = load_data()
```

Now, we should have the training and test set ready to be fed into our network. Next, let's define a simple callback application using the `EarlyStopping` monitor so that training can be stopped when no substantial improvements are being made to our model. This was included because training a VAE can take some time, and we don't want to waste computing resources seeing only submarginal increments to the model performance.


```python
early_stopping_monitor = callbacks.EarlyStopping(patience=2)

vae.fit(X_train,
        shuffle=True,
        epochs=epochs,
        batch_size=batch_size,
        validation_data=(X_test, None),
        callbacks=[early_stopping_monitor])
```

Training begins!

    Train on 60000 samples, validate on 10000 samples
    Epoch 1/30
    60000/60000 [==============================] - 13s 210us/sample - loss: 191.6765 - val_loss: 170.1529
    Epoch 2/30
    60000/60000 [==============================] - 11s 180us/sample - loss: 163.9683 - val_loss: 160.2263
    Epoch 3/30
    60000/60000 [==============================] - 11s 181us/sample - loss: 159.0007 - val_loss: 158.0777
    Epoch 4/30
    60000/60000 [==============================] - 11s 180us/sample - loss: 156.8238 - val_loss: 156.3414
    Epoch 5/30
    60000/60000 [==============================] - 11s 181us/sample - loss: 155.4041 - val_loss: 154.7498
    Epoch 6/30
    60000/60000 [==============================] - 11s 181us/sample - loss: 154.2847 - val_loss: 153.9668
    Epoch 7/30
    60000/60000 [==============================] - 11s 180us/sample - loss: 153.4675 - val_loss: 153.8024
    Epoch 8/30
    60000/60000 [==============================] - 11s 179us/sample - loss: 152.7539 - val_loss: 152.6393
    Epoch 9/30
    60000/60000 [==============================] - 11s 181us/sample - loss: 152.2562 - val_loss: 152.6557
    Epoch 10/30
    60000/60000 [==============================] - 11s 180us/sample - loss: 151.7278 - val_loss: 151.7882
    Epoch 11/30
    60000/60000 [==============================] - 11s 179us/sample - loss: 151.3973 - val_loss: 151.6642
    Epoch 12/30
    60000/60000 [==============================] - 11s 177us/sample - loss: 150.9899 - val_loss: 151.3316
    Epoch 13/30
    60000/60000 [==============================] - 11s 177us/sample - loss: 150.6191 - val_loss: 152.0779
    Epoch 14/30
    60000/60000 [==============================] - 11s 179us/sample - loss: 150.3378 - val_loss: 151.6977

After 14 epochs, training has stopped. 






```python
X_test_encoded, _, _ = encoder.predict(X_test, batch_size=batch_size)
plt.figure(figsize=(10, 10))
plt.scatter(X_test_encoded[:, 0], X_test_encoded[:, 1], c=y_test)
plt.colorbar()
plt.show()
```


<img src="/assets/images/2020-02-20-vae_files/2020-02-20-vae_11_0.svg">



```python
# display a 2D manifold of the digits
n = 15  # figure with 15x15 digits
digit_size = 28
figure = np.zeros((digit_size * n, digit_size * n))
# we will sample n points within [-5, 5] standard deviations
grid_x = np.linspace(-2, 2, n)
grid_y = np.linspace(-1, 3, n)

for i, yi in enumerate(grid_x):
    for j, xi in enumerate(grid_y):
        z_sample = np.array([[xi, yi]])
        x_decoded = decoder.predict(z_sample)
        digit = x_decoded[0].reshape(digit_size, digit_size)
        figure[i * digit_size: (i + 1) * digit_size,
               j * digit_size: (j + 1) * digit_size] = digit

plt.figure(figsize=(10, 10))
plt.imshow(figure, cmap='Greys_r')
plt.xticks([]); plt.yticks([])
plt.show()
```


<img src="/assets/images/2020-02-20-vae_files/2020-02-20-vae_12_0.svg">

