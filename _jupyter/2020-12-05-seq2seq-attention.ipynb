{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention took the NLP community by storm a few years ago when it was first announced. I've personally heard about attention many times, but never had the chance to fully dive into what it was. In this post, we will attempt to bake in a simple attention mechanism into a seq2seq model. \n",
    "\n",
    "Before anything, I highly recommend that you check out Ben Trevett's [sequence modeling tutorials](https://github.com/bentrevett/pytorch-seq2seq). In particular, this post is heavily based off of his [NMT notebook](https://github.com/bentrevett/pytorch-seq2seq/blob/master/3%20-%20Neural%20Machine%20Translation%20by%20Jointly%20Learning%20to%20Align%20and%20Translate.ipynb). Now let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "For this tutorial, we will need to import a number of dependencies, mainly from `torch` and `torchtext`. `torchtext` is a library that provides a nice interface to dealing with text-based data in PyTorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torchtext\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext.data import BucketIterator, Field\n",
    "from torchtext.datasets import Multi30k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In particular, `torchtext` includes a `Field` class, which essentially allows us to define some preprocessing steps to be applied on the data. We will be using the `Multi30k` dataset, which contains translations of short texts from many languages. In this tutorial, we will be using German and English, so we define preprocessing steps for each language. The preprocessing, as defined below, tells `torchtext` to:\n",
    "\n",
    "* Tokenize the dataset using `spacy`\n",
    "* Prepend each line with `\"<sos>\"` and `\"<eos>\"` tokens\n",
    "* Lowercase every word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC = Field(\n",
    "    tokenize=\"spacy\",\n",
    "    tokenizer_language=\"de\",\n",
    "    init_token=\"<sos>\",\n",
    "    eos_token=\"<eos>\",\n",
    "    lower=True,\n",
    ")\n",
    "\n",
    "TRG = Field(\n",
    "    tokenize=\"spacy\",\n",
    "    tokenizer_language=\"en\",\n",
    "    init_token=\"<sos>\",\n",
    "    eos_token=\"<eos>\",\n",
    "    lower=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now prepare the data by calling `split()` on the `Multi30k` dataset, using the fields we have defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, validation_data, test_data = Multi30k.splits(\n",
    "    root=\"data\", exts=(\".de\", \".en\"), fields=(SRC, TRG)\n",
    ")\n",
    "\n",
    "SRC.build_vocab(train_data, max_size=10000, min_freq=2)\n",
    "TRG.build_vocab(train_data, max_size=10000, min_freq=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create iterators to load the dataset to be fed into our model. These iterators are effectively data loaders in PyTorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, validation_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, validation_data, test_data), batch_size=BATCH_SIZE, device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we can see that all data have properly been batched. Notice that the length of each batch is different; of course, within each batch, all sentences have the same length. Otherwise, they wouldn't be a batch in the first place. However, it is apparent from this design that one benefit of using `torchtext` for batching data is that there is no need to worry about zero padding each sentence to make their lengths uniform across all batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([37, 128])\n",
      "torch.Size([28, 128])\n",
      "torch.Size([28, 128])\n",
      "torch.Size([37, 128])\n",
      "torch.Size([28, 128])\n",
      "torch.Size([27, 128])\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(train_iterator):\n",
    "    print(batch.src.shape)\n",
    "    if i == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "Now is the time for the fun part: modeling and implementing attention. Attention mechanisms originally arose in the context of sequence-to-sequence modeling. The underlying question is this: when some information is encoded via the encoder, then decoded by the decoder, can the decoder learn which part of the encoding to focus on while decoding? An easy real-life example of this would be machine translation. Given the input \"I love you,\" the Korean translation would be \"나는 너를 사랑해,\" or, translated word by word \"I you love.\" In this particular instance, the decoder has to know that there is some syntactic difference between Korean and English, and know which part of the original English sequence to focus on when producing a translation. \n",
    "\n",
    "Now that we have some idea of what attention is, let's start coding the encoder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        vocab_size, \n",
    "        embed_dim, \n",
    "        encoder_hidden_size, \n",
    "        decoder_hidden_size, \n",
    "        dropout\n",
    "    ):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.gru = nn.GRU(embed_dim, encoder_hidden_size, bidirectional=True)\n",
    "        self.fc = nn.Linear(encoder_hidden_size * 2, decoder_hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedding = self.dropout(self.embed(x))\n",
    "        outputs, hidden = self.gru(embedding)\n",
    "        # outputs.shape == (seq_len, batch_size, 2 * encoder_hidden_size)\n",
    "        # hidden.shape == (2, batch_size, encoder_hidden_size)\n",
    "        concat_hidden = torch.cat((hidden[-1], hidden[-2]), dim=1)\n",
    "        # concat_hidden.shape == (batch_size, encoder_hidden_size * 2)\n",
    "        hidden = torch.tanh(self.fc(concat_hidden))\n",
    "        # hidden.shape = (batch_size, decoder_hidden_size)\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        encoder_hidden_size,\n",
    "        decoder_hidden_size,\n",
    "    ):\n",
    "        super(Attention, self).__init__()\n",
    "        self.fc1 = nn.Linear(\n",
    "            encoder_hidden_size * 2 + decoder_hidden_size, \n",
    "            decoder_hidden_size\n",
    "        )\n",
    "        self.fc2 = nn.Linear(decoder_hidden_size, 1)\n",
    "    \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # hidden.size = (batch_size, decoder_hidden_size)\n",
    "        # encoder_outputs = (seq_len, batch_size, encoder_hidden_size * 2)\n",
    "        seq_len = encoder_outputs.size(0)\n",
    "        batch_size = encoder_outputs.size(1)\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, seq_len, 1)\n",
    "        # hidden.size = (batch_size, seq_len, decoder_hidden_size)\n",
    "        # encoder_outputs = (batch_size, seq_len, encoder_hidden_size * 2)\n",
    "        concat = torch.cat((hidden, encoder_outputs), dim=2)\n",
    "        # concat.shape == (batch_size, seq_len, encoder_hidden_size * 2 + decoder_hidden_size)\n",
    "        energy = torch.tanh(self.fc1(concat))\n",
    "        # energy.shape == (batch_size, seq_len, decoder_hidden_size)\n",
    "        attention = self.fc2(energy)\n",
    "        # attention.shape == (batch_size, seq_len, 1)\n",
    "        attention = attention.squeeze(2)\n",
    "        # attention.shape == (batch_size, seq_len)\n",
    "        return F.softmax(attention, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        vocab_size,\n",
    "        embed_dim,\n",
    "        decoder_hidden_size,\n",
    "        encoder_hidden_size, \n",
    "        droppout,\n",
    "    ):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dropout = nn.Dropout(droppout)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.gru = nn.GRU(\n",
    "            encoder_hidden_size * 2 + embed_dim, decoder_hidden_size\n",
    "        )\n",
    "        self.attention = Attention(encoder_hidden_size, decoder_hidden_size)\n",
    "        self.fc = nn.Linear(\n",
    "            encoder_hidden_size * 2 + decoder_hidden_size + embed_dim, \n",
    "            vocab_size\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, hidden, encoder_outputs):\n",
    "        x = x.unsqueeze(0)\n",
    "        # x.shape == (1, batch_size)\n",
    "        # hidden.shape = (batch_size, decoder_hidden_size)\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        # embedding.shape == (1, batch_size, embed_dim)\n",
    "        attention = self.attention(hidden, encoder_outputs)\n",
    "        # attention.shape == (batch_size, seq_len)\n",
    "        attention = attention.unsqueeze(1)\n",
    "        # attention.shape == (batch_size, 1, seq_len)\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        # encoder_outputs.shape == (batch_size, seq_len, encoder_hidden_size * 2)\n",
    "        weighted = torch.bmm(attention, encoder_outputs)\n",
    "        # weighted.shape == (batch_size, 1, encoder_hidden_dim * 2)\n",
    "        weighted.permute(1, 0, 2)\n",
    "        # weighted.shape == (1, batch_size, encoder_hidden_dim * 2)\n",
    "        weighted_concat = weighted.cat((embedding, weighted), dim=2)\n",
    "        # weighted_concat.shape == (1, batch_size, encoder_hidden_dim * 2 + embed_dim)\n",
    "        output, hidden = self.gru(weighted_concat, hidden)\n",
    "        # output.shape == (1, batch_size, decoder_hidden_size)\n",
    "        # hidden.shape == (1, batch_size, decoder_hidden_size)\n",
    "        embedding = embedding.squeeze(0)        \n",
    "        output = output.squeeze(0)\n",
    "        hidden = hidden.squeeze(0)\n",
    "        weighted = weighted.squeeze(0)\n",
    "        # embedding.shape == (batch_size, embed_dim)\n",
    "        # output.shape == (batch_size, decoder_hidden_size)\n",
    "        # weighted.shape == (batch_size, encoder_hidden_dim * 2)\n",
    "        fc_in = torch.cat((output, weighted, embedding), dim=1)\n",
    "        prediction = self.fc(fc_in)\n",
    "        # prediction.shape == (batch_size, vocab_size)\n",
    "        return prediction, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "    \n",
    "    def forward(self, source, target, teacher_force_ratio=0.5):\n",
    "        seq_len = target.size(0)\n",
    "        batch_size = target.size(1)    \n",
    "        outputs = torch.zeros(\n",
    "            seq_len, batch_size, self.decoder.vocab_size\n",
    "        ).to(self.device)\n",
    "        \n",
    "        encoder_outputs, hidden = self.encoder(source)\n",
    "        x = target[0]\n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            output, hidden = self.decoder(x, hidden, encoder_outputs)\n",
    "            outputs[t] = output\n",
    "            teacher_force = random.random() < teacher_force_ratio\n",
    "            if teacher_force:\n",
    "                x = predictions.argmax(1)\n",
    "            else:\n",
    "                x = target[t]\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "ENC_HID_DIM = 512\n",
    "DEC_HID_DIM = 512\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "encoder = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "decoder = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT)\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/bentrevett/pytorch-seq2seq/blob/master/3%20-%20Neural%20Machine%20Translation%20by%20Jointly%20Learning%20to%20Align%20and%20Translate.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
