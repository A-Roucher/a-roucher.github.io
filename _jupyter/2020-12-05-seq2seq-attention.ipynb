{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torchtext\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext.data import BucketIterator, Field\n",
    "from torchtext.datasets import Multi30k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC = Field(\n",
    "    tokenize=\"spacy\",\n",
    "    tokenizer_language=\"de\",\n",
    "    init_token=\"<sos>\",\n",
    "    eos_token=\"<eos>\",\n",
    "    lower=True,\n",
    ")\n",
    "\n",
    "TRG = Field(\n",
    "    tokenize=\"spacy\",\n",
    "    tokenizer_language=\"en\",\n",
    "    init_token=\"<sos>\",\n",
    "    eos_token=\"<eos>\",\n",
    "    lower=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, validation_data, test_data = Multi30k.splits(\n",
    "    root=\"data\", exts=(\".de\", \".en\"), fields=(SRC, TRG)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC.build_vocab(train_data, max_size=10000, min_freq=2)\n",
    "TRG.build_vocab(train_data, max_size=10000, min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, validation_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, validation_data, test_data), batch_size=BATCH_SIZE, device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([37, 128])\n",
      "torch.Size([28, 128])\n",
      "torch.Size([28, 128])\n",
      "torch.Size([37, 128])\n",
      "torch.Size([28, 128])\n",
      "torch.Size([27, 128])\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(train_iterator):\n",
    "    print(batch.src.shape)\n",
    "    if i == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        vocab_size, \n",
    "        embed_dim, \n",
    "        encoder_hidden_size, \n",
    "        decoder_hidden_size, \n",
    "        dropout\n",
    "    ):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.gru = nn.GRU(embed_dim, encoder_hidden_size, bidirectional=True)\n",
    "        self.fc = nn.Linear(encoder_hidden_size * 2, decoder_hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedding = self.dropout(self.embed(x))\n",
    "        outputs, hidden = self.gru(embedding)\n",
    "        # outputs.shape == (seq_len, batch_size, 2 * encoder_hidden_size)\n",
    "        # hidden.shape == (2, batch_size, encoder_hidden_size)\n",
    "        concat_hidden = torch.cat((hidden[-1], hidden[-2]), dim=1)\n",
    "        # concat_hidden.shape == (batch_size, encoder_hidden_size * 2)\n",
    "        hidden = torch.tanh(self.fc(concat_hidden))\n",
    "        # hidden.shape = (batch_size, decoder_hidden_size)\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        encoder_hidden_size,\n",
    "        decoder_hidden_size,\n",
    "    ):\n",
    "        super(Attention, self).__init__()\n",
    "        self.fc1 = nn.Linear(\n",
    "            encoder_hidden_size * 2 + decoder_hidden_size, \n",
    "            decoder_hidden_size\n",
    "        )\n",
    "        self.fc2 = nn.Linear(decoder_hidden_size, 1)\n",
    "    \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # hidden.size = (batch_size, decoder_hidden_size)\n",
    "        # encoder_outputs = (seq_len, batch_size, encoder_hidden_size * 2)\n",
    "        seq_len = encoder_outputs.size(0)\n",
    "        batch_size = encoder_outputs.size(1)\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, seq_len, 1)\n",
    "        # hidden.size = (batch_size, seq_len, decoder_hidden_size)\n",
    "        # encoder_outputs = (batch_size, seq_len, encoder_hidden_size * 2)\n",
    "        concat = torch.cat((hidden, encoder_outputs), dim=2)\n",
    "        # concat.shape == (batch_size, seq_len, encoder_hidden_size * 2 + decoder_hidden_size)\n",
    "        energy = torch.tanh(self.fc1(concat))\n",
    "        # energy.shape == (batch_size, seq_len, decoder_hidden_size)\n",
    "        attention = self.fc2(energy)\n",
    "        # attention.shape == (batch_size, seq_len, 1)\n",
    "        attention = attention.squeeze(2)\n",
    "        # attention.shape == (batch_size, seq_len)\n",
    "        return F.softmax(attention, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        vocab_size,\n",
    "        embed_dim,\n",
    "        decoder_hidden_size,\n",
    "        encoder_hidden_size, \n",
    "        droppout,\n",
    "    ):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dropout = nn.Dropout(droppout)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.gru = nn.GRU(\n",
    "            encoder_hidden_size * 2 + embed_dim, decoder_hidden_size\n",
    "        )\n",
    "        self.attention = Attention(encoder_hidden_size, decoder_hidden_size)\n",
    "        self.fc = nn.Linear(\n",
    "            encoder_hidden_size * 2 + decoder_hidden_size + embed_dim, \n",
    "            vocab_size\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, hidden, encoder_outputs):\n",
    "        x = x.unsqueeze(0)\n",
    "        # x.shape == (1, batch_size)\n",
    "        # hidden.shape = (batch_size, decoder_hidden_size)\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        # embedding.shape == (1, batch_size, embed_dim)\n",
    "        attention = self.attention(hidden, encoder_outputs)\n",
    "        # attention.shape == (batch_size, seq_len)\n",
    "        attention = attention.unsqueeze(1)\n",
    "        # attention.shape == (batch_size, 1, seq_len)\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        # encoder_outputs.shape == (batch_size, seq_len, encoder_hidden_size * 2)\n",
    "        weighted = torch.bmm(attention, encoder_outputs)\n",
    "        # weighted.shape == (batch_size, 1, encoder_hidden_dim * 2)\n",
    "        weighted.permute(1, 0, 2)\n",
    "        # weighted.shape == (1, batch_size, encoder_hidden_dim * 2)\n",
    "        weighted_concat = weighted.cat((embedding, weighted), dim=2)\n",
    "        # weighted_concat.shape == (1, batch_size, encoder_hidden_dim * 2 + embed_dim)\n",
    "        output, hidden = self.gru(weighted_concat, hidden)\n",
    "        # output.shape == (1, batch_size, decoder_hidden_size)\n",
    "        # hidden.shape == (1, batch_size, decoder_hidden_size)\n",
    "        embedding = embedding.squeeze(0)        \n",
    "        output = output.squeeze(0)\n",
    "        hidden = hidden.squeeze(0)\n",
    "        weighted = weighted.squeeze(0)\n",
    "        # embedding.shape == (batch_size, embed_dim)\n",
    "        # output.shape == (batch_size, decoder_hidden_size)\n",
    "        # weighted.shape == (batch_size, encoder_hidden_dim * 2)\n",
    "        fc_in = torch.cat((output, weighted, embedding), dim=1)\n",
    "        prediction = self.fc(fc_in)\n",
    "        # prediction.shape == (batch_size, vocab_size)\n",
    "        return prediction, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "    \n",
    "    def forward(self, source, target, teacher_force_ratio=0.5):\n",
    "        seq_len = target.size(0)\n",
    "        batch_size = target.size(1)    \n",
    "        outputs = torch.zeros(\n",
    "            seq_len, batch_size, self.decoder.vocab_size\n",
    "        ).to(self.device)\n",
    "        \n",
    "        encoder_outputs, hidden = self.encoder(source)\n",
    "        x = target[0]\n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            output, hidden = self.decoder(x, hidden, encoder_outputs)\n",
    "            outputs[t] = output\n",
    "            teacher_force = random.random() < teacher_force_ratio\n",
    "            if teacher_force:\n",
    "                x = predictions.argmax(1)\n",
    "            else:\n",
    "                x = target[t]\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "ENC_HID_DIM = 512\n",
    "DEC_HID_DIM = 512\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "encoder = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "decoder = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT)\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/bentrevett/pytorch-seq2seq/blob/master/3%20-%20Neural%20Machine%20Translation%20by%20Jointly%20Learning%20to%20Align%20and%20Translate.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
