{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a very long time, I've been fascinated by sequence-to-sequence models. Give the model a photo as input, it spits out a caption to go along with it; give it some English text, it can translate it into another language. Seq2seq models are also not only widely applicable in different contexts, but it is also the basis of other more advanced models that came after, namely attention. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torchtext\n",
    "from torch import nn\n",
    "from torchtext.data import BucketIterator, Field\n",
    "from torchtext.datasets import Multi30k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC = Field(\n",
    "    tokenize=\"spacy\",\n",
    "    tokenizer_language=\"de\",\n",
    "    init_token=\"<sos>\",\n",
    "    eos_token=\"<eos>\",\n",
    "    lower=True,\n",
    ")\n",
    "\n",
    "TRG = Field(\n",
    "    tokenize=\"spacy\",\n",
    "    tokenizer_language=\"en\",\n",
    "    init_token=\"<sos>\",\n",
    "    eos_token=\"<eos>\",\n",
    "    lower=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, validation_data, test_data = Multi30k.splits(\n",
    "    root=\"data\", exts=(\".de\", \".en\"), fields=(SRC, TRG)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29000\n",
      "1014\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "for data in (train_data, validation_data, test_data):\n",
    "    print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'src': ['zwei', 'junge', 'weiße', 'männer', 'sind', 'im', 'freien', 'in', 'der', 'nähe', 'vieler', 'büsche', '.'], 'trg': ['two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.']}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data.examples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC.build_vocab(train_data, max_size=10000, min_freq=2)\n",
    "TRG.build_vocab(train_data, max_size=10000, min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sos> <eos> <pad>\n"
     ]
    }
   ],
   "source": [
    "print(TRG.init_token, TRG.eos_token, TRG.pad_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in source (ger) vocabulary: 7854\n",
      "Unique tokens in target (eng) vocabulary: 5893\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique tokens in source (ger) vocabulary: {len(SRC.vocab)}\")\n",
    "print(f\"Unique tokens in target (eng) vocabulary: {len(TRG.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SRC.vocab.stoi[\"<sos>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, validation_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, validation_data, test_data), batch_size=BATCH_SIZE, device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2])\n",
      "torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_iterator:\n",
    "    print(batch.src[0])\n",
    "    print(batch.src[1].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_size, num_layers, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_size, num_layers, dropout=dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x.shape == (src_seq_len, batch_size) == (36, 128)\n",
    "        embedding = self.dropout(self.embed(x))\n",
    "        # embedding.shape == (36, 128, embed_dim) == (36, 128, 256)\n",
    "        outputs, (hidden, cell) = self.lstm(embedding)\n",
    "        # outputs.shape == (36, 128, hidden_size) == (36, 128, 512)\n",
    "        # hidden.shape, cell.shape == (2, 128, hidden_size) == (2, 128, 512)\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_size, num_layers, dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_size, num_layers, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, x, hidden, cell):\n",
    "        # x.shape == (128,)\n",
    "        x = x.unsqueeze(0)\n",
    "        # x.shape == (1, 128)\n",
    "        embedding = self.dropout(self.embed(x))\n",
    "        # embedding.shape = (1, 128, 256)\n",
    "        outputs, (hidden, cell) = self.lstm(embedding, (hidden, cell))\n",
    "        # outputs.shape == (1, 128, 512)\n",
    "        outputs = outputs.squeeze(0)\n",
    "        # outputs.shape == (128, 512)\n",
    "        predictions = self.fc(outputs)\n",
    "        # predictions.shape == (128, vocab_size)\n",
    "        return predictions, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "    \n",
    "    def forward(self, source, target, teacher_force_ratio=0.5):\n",
    "        seq_len = target.size(0)\n",
    "        batch_size = target.size(1)    \n",
    "        outputs = torch.zeros(\n",
    "            seq_len, batch_size, self.decoder.vocab_size\n",
    "        ).to(self.device)\n",
    "        \n",
    "        hidden, cell = self.encoder(source)\n",
    "        x = target[0]\n",
    "        # x.shape == (128,)\n",
    "        \n",
    "        for t in range(1, seq_len):\n",
    "            predictions, hidden, cell = self.decoder(x, hidden, cell)\n",
    "            outputs[t] = predictions\n",
    "            teacher_force = random.random() < teacher_force_ratio\n",
    "            if teacher_force:\n",
    "                x = predictions.argmax(1)\n",
    "            else:\n",
    "                x = target[t]\n",
    "                \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "HID_DIM = 512\n",
    "N_LAYERS = 2\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "encoder = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "decoder = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (embed): Embedding(7854, 256)\n",
       "    (lstm): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (embed): Embedding(5893, 256)\n",
       "    (lstm): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
       "    (fc): Linear(in_features=512, out_features=5893, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "\n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13898757"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        source = batch.src\n",
    "        target = batch.trg\n",
    "        # source.shape == (batch_seq_len, 128)\n",
    "        # target.shape == (batch_seq_len, 128)\n",
    "        output = model(source, target)\n",
    "        # output.shape == (batch_seq_len, batch_size, vocab_size) == (25, 128, 5893)\n",
    "        \n",
    "        output = output[1:].reshape(-1, output.size(2))\n",
    "        # output.shape == (3072, 5893)\n",
    "        target = target[1:].reshape(-1)\n",
    "        # target.shape == (3072)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    return epoch_loss / (i + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIP = 1\n",
    "N_EPOCHS = 10\n",
    "best_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    validation_loss = evaluate(model, validation_iterator, criterion)\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    \n",
    "    if validation_loss < best_loss:\n",
    "        best_loss = validation_loss\n",
    "        torch.save(model.state_dict(), './data/seq2seq/weights.pt')\n",
    "    \n",
    "    print(\n",
    "        f\"Epoch [{epoch+1}{N_EPOCHS}], Time: {total_time}s, \"\n",
    "        f\"Train Loss: {train_loss:.3f}, Val. Loss: {validation_loss:.3f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            source = batch.src\n",
    "            target = batch.trg\n",
    "            output = model(source, target, teacher_force_ratio=0)\n",
    "            target = target[1:].reshape(-1)\n",
    "            output = output[1:].reshape(-1, output.size(2))\n",
    "            loss = criterion(output, target)\n",
    "            epoch_loss += loss.item()\n",
    "    model.train()\n",
    "    return epoch_loss / (i + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/bentrevett/pytorch-seq2seq/blob/master/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/torchtext_translation_tutorial.html\n",
    "\n",
    "https://github.com/SethHWeidman/pytorch-seq2seq/blob/master/3%20-%20Neural%20Machine%20Translation%20by%20Jointly%20Learning%20to%20Align%20and%20Translate.ipynb\n",
    "\n",
    "https://www.youtube.com/watch?v=EoGUlvhRYpk&list=PLhhyoLH6IjfxeoooqP9rhU3HJIAVAJ3Vz&index=33"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
