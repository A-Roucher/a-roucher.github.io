{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other day, my friend and I were talking about our mutual friend Jeremy. \"He's an oddball,\" my friend Sean remarked, to which I agreed. Out of nowhere, Jeremy had just told us that he would not be coming back to Korea for the next three years. \"He is just about the most random person I know.\" And both of us, being aspiring statistics majors, began wondering: is there a systematic way of measuirng randomness? It is from here that we went down the rabbit hole of Google and Wikipedia search. I ended up landing on entropy land, which is going to be the topic for today's post. It's a random post on the topic of randomness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy in Science\n",
    "\n",
    "To begin our discussion of randomness, let's take a look at how scientists measure the randomness involved in natural phenomena, such as particle movement. If you are familiar with chemistry or molecular physics, you might be familiar with the concept of entropy, one of the core elements of thermodynamics and a topic that recurs throughout many subfields of natural sciences. Although I'm no science major, one of the few things that I recall from high school chemistry class is the equation for calculating the spontaneity of some reaction using Gibbs Free energy, which went as follows:\n",
    "\n",
    "$$\\Delta G = \\Delta H - T \\Delta S$$\n",
    "\n",
    "where the term for entropy, denoted as $\\Delta S$ satisfies the condition that\n",
    "\n",
    "$$\\Delta S = k_B \\ln \\Omega$$\n",
    "\n",
    "We won't get into the details of these equations, but an intuition that we can glean from the concept of entropy is that the randomness of a particle is determined by the number of potential states that are possible for the given particle. In other words, a gas particle that freely moves across space at ATP is going to be more random than a neaar-static water particle composing an ice cub. Then, it intuitively makes sense for us to say that the gas particle carries a larger amount of information than does the particle in a solid---after all, the gas particle would have a very wide probability distribution that encompasses various possible states and positionns, whereas the probability distribution for a particle composing a solid would have lesser variance, most likely concentrated at a point give its limited number of orientations. \n",
    "\n",
    "Entropy in science, denoted above as $\\Delta S$, provides us with a valuable intuition on the topic of randomness and information. In fact, it is no coincidence that the notion of randomness in information theory, a subfield of math that we are going to be dipping our toes in, borrowed the term \"entropy\" to express randomness exhibited in data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy in Information Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...links...\n",
    "https://medium.com/activating-robotic-minds/demystifying-entropy-f2c3221e2550\n",
    "https://medium.com/udacity/shannon-entropy-information-gain-and-picking-balls-from-buckets-5810d35d54b4\n",
    "https://towardsdatascience.com/information-entropy-c037a90de58f\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
