{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today, we are finally going to take a look at transformers, the mother of most, if not all current state-of-the-art NLP models. Back in the day, RNNs used to be king. The classic setup for NLP tasks was to use a bidirectional LSTM with word embeddings such as word2vec or GloVe. Now, the world has changed, and transformer models like BERT, GPT, and T5 have now become the new SOTA. \n",
    "\n",
    "Before we begin, I highly recommend that you check out the following resources:\n",
    "\n",
    "* [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)\n",
    "* [Illustrated Guide to Transformers Neural Network](https://www.youtube.com/watch?v=4Bdc55j80l8&t=360s)\n",
    "* [Attention and Transformer Networks](https://www.youtube.com/watch?v=OyFJWRnt_AY&t=2316s)\n",
    "\n",
    "These resources were of enormous help for me in gaining a solid conceptual understanding of this topic. For implementation details, I referred to\n",
    "\n",
    "* [Ben Trevett's seq2seq tutorial](https://github.com/bentrevett/pytorch-seq2seq/blob/master/6%20-%20Attention%20is%20All%20You%20Need.ipynb)\n",
    "* [Transformers from Scratch](https://www.youtube.com/watch?v=U0s0f995w14&t=1172s)\n",
    "\n",
    "And of course, it is my hope that this post also turns out to be helpful for those trying to break into the world of transformers. Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "While LSTMs and other RNNs are effective at solving NLP tasks, they are only effective to a degree: due to the sequential nature with which data is processed, RNNs cannot handle long-range dependencies very well. This means that, when sentences get long, some data might be lost. This is an inherent limitation of RNNs, as they rely on hidden states that are passed throughout the unrolling sequence to store information about the input data. When it is unrolled for too many time steps, the hidden state can no longer accurately capture information from earlier on in the sequence. Below is an illustration taken from Chris Olah's blog."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-longtermdependencies.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformers, unlike RNNs, are not recursive by nature. Instead, transformer models are able to digest sequential input data all at once in parallel. Therefore, they do not suffer from the long-range dependency problem. They are also generally quicker to train than LSTM networks, although this statement is somewhat undercut by the fact that recent transformer SOTA models are extremely massive to the extent that no individual developer can train them on a personal GPU from scratch. However, the massiveness of a network is not an inherent characteristic of the transformer architecture; it is better understood as a general trend in the bleeding-edge research community. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a very high level, the transformer architecture is composed of two parts: an encoder and a decoder. Below is an illustration taken from the original paper that started it all, [Attention is All You Need](https://arxiv.org/abs/1706.03762)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://devopedia.org/images/article/235/8482.1573652874.png\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the transformer is definitely not the simplest of all models. However, upon closer examination, you might also realize that many of the components of the model are repeated and reused throughout the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The source mask is used to prevent the model from needlessly computing self-attention using padding tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        embed_dim,\n",
    "        max_len,\n",
    "        num_layers,\n",
    "        num_heads,\n",
    "        ff_hid_dim,\n",
    "        dropout=0.5,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.tok_embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_embed = nn.Embedding(max_len, embed_dim)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                EncoderLayer(embed_dim, num_heads, ff_hid_dim, dropout)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        batch_size = src.size(0)\n",
    "        seq_len = src.size(1)\n",
    "        device = next(self.parameters()).device\n",
    "        pos = (\n",
    "            torch.arange(0, seq_len)\n",
    "            .unsqueeze(0)\n",
    "            .repeat(batch_size, 1)\n",
    "            .to(device)\n",
    "        )\n",
    "        src = self.dropout(self.pos_embed(pos) + self.tok_embed(src))\n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self, embed_dim, num_heads, ff_hid_dim, dropout\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.ff_ln = nn.LayerNorm(embed_dim)\n",
    "        self.attention_ln = nn.LayerNorm(embed_dim)\n",
    "        self.ff = FeedForward(embed_dim, ff_hid_dim, dropout)\n",
    "        self.attention = MultiHeadAttention(embed_dim, num_heads)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        attention_out = self.attention(src, src, src, src_mask)\n",
    "        attetion_ln_out = self.dropout(self.attention_ln(src + attention_out))\n",
    "        ff_out = self.ff(attetion_ln_out)\n",
    "        ff_ln_out = self.dropout(self.ff_ln(attetion_ln_out + ff_out))\n",
    "        return ff_ln_out        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equation for attention can be written as follows:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}(\\frac{QK^\\top}{\\sqrt{d}})V\n",
    "$$\n",
    "\n",
    "<img src=\"https://paperswithcode.com/media/methods/multi-head-attention_l1A3G7a.png\" width=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hid_dim, num_heads):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert hid_dim % num_heads == 0, \"`hidden_dim` must be a multiple of `num_heads`\"\n",
    "        \n",
    "        self.hid_dim = hid_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hid_dim // num_heads\n",
    "        \n",
    "        self.fc_v = nn.Linear(hid_dim, hid_dim, bias=False)\n",
    "        self.fc_k = nn.Linear(hid_dim, hid_dim, bias=False)\n",
    "        self.fc_q = nn.Linear(hid_dim, hid_dim, bias=False)\n",
    "        self.fc = nn.Linear(hid_dim, hid_dim)\n",
    "    \n",
    "    def forward(self, value, key, query, mask=None):\n",
    "        # keys.shape = [batch_size, seq_len, embed_dim]\n",
    "        batch_size = query.size(0)\n",
    "        \n",
    "        V = self.fc_v(value)\n",
    "        K = self.fc_k(key)\n",
    "        Q = self.fc_q(query)\n",
    "        # shape = [batch_size, seq_len, hid_dim]\n",
    "        \n",
    "        V = V.reshape(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K_t = K.reshape(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 3, 1)\n",
    "        Q = Q.reshape(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        # V.shape = [batch_size, num_heads, value_len, head_dim]\n",
    "        # K_t.shape = [batch_size, num_heads, head_dim, key_len]\n",
    "        \n",
    "        energy = torch.matmul(Q, K_t) / (self.hid_dim ** 1/2)\n",
    "        # energy.shape = [batch_size, num_heads, query_len, key_len]\n",
    "        \n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-inf\"))\n",
    "        \n",
    "        attention = F.softmax(energy, dim=-1)\n",
    "        weighted = torch.matmul(attention, V)\n",
    "        # weighted.shape = [batch_size, num_heads, seq_len, head_dim]\n",
    "        weighted = weighted.permute(0, 2, 1, 3)\n",
    "        # weighted.shape = [batch_size, seq_len, num_heads, head_dim]\n",
    "        weighted = weighted.reshape(batch_size, -1, self.hid_dim)\n",
    "        # weighted.shape = [batch_size, seq_len, hid_dim]\n",
    "        out = self.fc(weighted)\n",
    "        # out.shape = [batch_size, seq_len, hid_dim]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, hid_dim, dropout):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(embed_dim, hid_dim)\n",
    "        self.fc2 = nn.Linear(hid_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc2(F.relu(self.dropout(self.fc1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        embed_dim,\n",
    "        max_len,\n",
    "        num_layers,\n",
    "        num_heads,\n",
    "        ff_hid_dim,\n",
    "        dropout=0.5,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.tok_embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_embed = nn.Embedding(max_len, embed_dim)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                DecoderLayer(embed_dim, num_heads, ff_hid_dim, dropout)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, trg, trg_mask, enc_src, src_mask):\n",
    "        batch_size = trg.size(0)\n",
    "        seq_len = trg.size(1)\n",
    "        device = next(model.parameters()).device\n",
    "        pos = (\n",
    "            torch.arange(0, seq_len)\n",
    "            .unsqueeze(0)\n",
    "            .repeat(batch_size, 1)\n",
    "            .to(device)\n",
    "        )\n",
    "        trg = self.dropout(self.pos_embed(pos) + self.tok_embed(trg))\n",
    "        for layer in self.layers:\n",
    "            trg = layer(trg, trg_mask, enc_src, src_mask)\n",
    "        out = self.fc(trg)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_hid_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.ff_ln = nn.LayerNorm(embed_dim)\n",
    "        self.dec_attn_ln = nn.LayerNorm(embed_dim)\n",
    "        self.enc_attn_ln = nn.LayerNorm(embed_dim)\n",
    "        self.ff = FeedForward(embed_dim, ff_hid_dim, dropout)\n",
    "        self.dec_attn = MultiHeadAttention(embed_dim, num_heads)\n",
    "        self.enc_attn = MultiHeadAttention(embed_dim, num_heads)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, trg, trg_mask, enc_src, src_mask):\n",
    "        dec_attn_out = self.dropout(self.dec_attn(trg, trg, trg, trg_mask))\n",
    "        dec_attn_ln_out = self.dec_attn_ln(trg + dec_attn_out)\n",
    "        enc_attn_out = self.dropout(\n",
    "            self.enc_attn(enc_src, enc_src, dec_attn_ln_out, src_mask)\n",
    "        )\n",
    "        enc_attn_ln_out = self.enc_attn_ln(dec_attn_ln_out + enc_attn_out)\n",
    "        ff_out = self.dropout(self.ff(enc_attn_ln_out))\n",
    "        ff_ln_out = self.ff_ln(ff_out + enc_attn_ln_out)\n",
    "        return ff_ln_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        src_vocab_size, \n",
    "        trg_vocab_size,\n",
    "        src_pad_idx, \n",
    "        trg_pad_idx,\n",
    "        embed_dim=512,\n",
    "        max_len=100,\n",
    "        num_layers=6,\n",
    "        num_heads=8,\n",
    "        ff_hid_dim=2048,\n",
    "        dropout=0.5,\n",
    "\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(\n",
    "            src_vocab_size,\n",
    "            embed_dim,\n",
    "            max_len,\n",
    "            num_layers,\n",
    "            num_heads,\n",
    "            ff_hid_dim,\n",
    "            dropout,\n",
    "        )\n",
    "        self.decoder = Decoder(\n",
    "            trg_vocab_size,\n",
    "            embed_dim,\n",
    "            max_len,\n",
    "            num_layers,\n",
    "            num_heads,\n",
    "            ff_hid_dim,\n",
    "            dropout,\n",
    "        )\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "    \n",
    "    def make_src_mask(self, src):\n",
    "        # src.shape = [batch_size, src_len]\n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        # src.shape = [batch_size, 1, 1, src_len]\n",
    "        return src_mask\n",
    "    \n",
    "    def make_trg_mask(self, trg):\n",
    "        batch_size = trg.size(0)\n",
    "        seq_len = trg.size(1)\n",
    "        pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        seq_mask = torch.tril(torch.ones(seq_len, seq_len))\n",
    "        trg_mask = (pad_mask * seq_len).expand(batch_size, 1, seq_len, seq_len)\n",
    "        return trg_mask\n",
    "    \n",
    "    def forward(self, src, trg):\n",
    "        device = next(model.parameters()).device\n",
    "        src_mask = self.make_src_mask(src).to(device)\n",
    "        trg_mask = self.make_trg_mask(trg).to(device)\n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        decoder_out = self.decoder(trg, trg_mask, enc_src, src_mask)\n",
    "        return decoder_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "src_pad_idx = 0\n",
    "trg_pad_idx = 0\n",
    "src_vocab_size = 10\n",
    "trg_vocab_size = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "src = torch.tensor(\n",
    "    [[1, 5, 6, 4, 3, 9, 5, 2, 0], [1, 8, 7, 3, 4, 5, 6, 7, 2]]\n",
    ").to(device)\n",
    "trg = torch.tensor([[1, 7, 4, 3, 5, 9, 2, 0], [1, 5, 6, 2, 4, 7, 6, 2]]).to(\n",
    "    device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(\n",
    "    src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(src, trg[:, :-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 7, 10])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/bentrevett/pytorch-seq2seq/blob/master/6%20-%20Attention%20is%20All%20You%20Need.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
