{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be implementing a custom seq2seq model with better attention mechanism in today's post. Ever since it was first introduced in encoder-decoder architectures, attention mechanisms have become increasingly important. It is also at the core of more advanced transformers moodels like BERT and GPT. Attention is a natural way of forcing the model to learn which portion of the input and output data correspond to each other. Gradually, with gradient descent, the model learns to see patterns in these sequential data. It is also worth noting that there are variations of the classic transformer attention architecture. Allen AI's Longformer, for instance, allows BERT-style tranformer models to process long sequences by distinguishing between local and global attention, combined with a sliding window approach. Ultimately, this allows the attention calculation to run in linear time, as opposed to quadratic run time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equation for attention can be written as follows:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}(\\frac{QK^\\top}{\\sqrt{d}})V\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, heads):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_dim // heads\n",
    "        \n",
    "        if self.head_dim * heads != embed_dim:\n",
    "            raise ValueError(`embed_dim` must be a multiple of `heads`)\n",
    "        \n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc = nn.Linear(embed_dim, embed_dim)\n",
    "    \n",
    "    def forward(self, values, keys, queries):\n",
    "        N = query.size(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
