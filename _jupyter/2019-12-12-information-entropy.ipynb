{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other day, my friend and I were talking about our mutual friend Jeremy. \"He's an oddball,\" my friend Sean remarked, to which I agreed. Out of nowhere, Jeremy had just told us that he would not be coming back to Korea for the next three years. \"He is just about the most random person I know.\" And both of us, being aspiring statistics majors, began wondering: is there a systematic way of measuirng randomness? It is from here that we went down the rabbit hole of Google and Wikipedia search. I ended up landing on entropy land, which is going to be the topic for today's post. It's a random post on the topic of randomness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy in Science\n",
    "\n",
    "To begin our discussion of randomness, let's take a look at how scientists measure the randomness involved in natural phenomena, such as particle movement. If you are familiar with chemistry or molecular physics, you might be familiar with the concept of entropy, one of the core elements of thermodynamics and a topic that recurs throughout many subfields of natural sciences. Although I'm no science major, one of the few things that I recall from high school chemistry class is the equation for calculating the spontaneity of some reaction using Gibbs Free energy, which went as follows:\n",
    "\n",
    "$$\\Delta G = \\Delta H - T \\Delta S$$\n",
    "\n",
    "where the term for entropy, denoted as $\\Delta S$ satisfies the condition that\n",
    "\n",
    "$$\\Delta S = k_B \\ln \\Omega$$\n",
    "\n",
    "We won't get into the details of these equations, but an intuition that we can glean from the concept of entropy is that the randomness of a particle is determined by the number of potential states that are possible for the given particle. In other words, a gas particle that freely moves across space at ATP is going to be more random than a neaar-static water particle composing an ice cub. Then, it intuitively makes sense for us to say that the gas particle carries a larger amount of information than does the particle in a solid---after all, the gas particle would have a very wide probability distribution that encompasses various possible states and positionns, whereas the probability distribution for a particle composing a solid would have lesser variance, most likely concentrated at a point give its limited number of orientations. \n",
    "\n",
    "Entropy in science, denoted above as $\\Delta S$, provides us with a valuable intuition on the topic of randomness and information. In fact, it is no coincidence that the notion of randomness in information theory, a subfield of math that we are going to be dipping our toes in, borrowed the term \"entropy\" to express randomness exhibited in data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy in Information Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like entropy was used to quantify randomness in the scientific phenomena, the notion of entropy is used in information theory to denote randomness in data. The origin of information entropy can be traced back to Claude Shannon's paper published in 1948, titled \"A Mathematical Theory of Communication.\" While working at Bell Labs, Shannon was experimenting with methods to most efficiently transmit data without loss of information. In other words, he was researching on ways to encode original information using the smallest possible data structure, such that the receiver of information could quickly decode transmitted data for interpretation. It is in this context that Shannon proposed the notion of entropy, which he roughly defined as the smallest possible size of losslless encoding of a message for transmission. \n",
    "\n",
    "Of course, there is a corresponding mathematical definition. But before we jump straight into entropy, let's try to  develop some preliminary intuition on the concept of information, specifically in the context of information theory and randomness. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is information? Warren Weaver, who popularized Shannon's works and together developed the field of information theory, pointed out that information is not related to what is said, but what could be said. This element of uncertainty involved in one's degree of freedom is what makes the notion of information inseparable from probabiilty and randomness. As Ian Goodfellow put it in [Deep Learning], \n",
    "\n",
    "> *The basic intuition behind information theory is that learning that an unlikely event has occurred is more informative than learning that a likely event has occurred.*\n",
    "\n",
    "In other words, a low probability event expresses a lot of information, while a high probability event expresses low information as its occurrence provides little information of value to the informed. Put differently, rare events require more information to represent than common ones. Consider, for example, how we might represent the amount of information involved in a fair coin toss. We know for a fact that \n",
    "\n",
    "$$P(H) = P(T) = 0.5$$\n",
    "\n",
    "where $H$ and $T$ denote the event that the coin lands on heads and tails, respectively. How might we be able to express information involved in the event that the coin lands on tails? How about heads?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to approach this problem, but an easy way would be to use binary numbers. For example, we might ascribe meaning to 0 and 1 such that 0 represents heads and 1 represents tails. Therefore, all we need to express the result of a fair coin toss is a single digit, which can take binary numbers 0 and 1. Typically, we use [bits] to denote the number of digits required to express information in binary numbers. Therefore, in this case, the information involved in $P(H)$ is equal to 1 bit; of course, by symmetry, the same goes for $P(T)$.\n",
    "\n",
    "If bits sounds similar to bytes or gigabytes we use for storage, you're exactly on the right path. In fact, the relationship between bit and byte is established directly by the fact that\n",
    "\n",
    "$$8 b = 1 B$$\n",
    "\n",
    "where $b$ denotes bits and $B$ denots bytes. This is why we use bytes to represent the amount of disk storage in computers, for instance: by using bytes as a unit of measuring storage space, we are implying how much information the disk can store. It is also worth mentioning that the alternative name for bits is Shannons, named eponymously after the mathematician who pioneered the field of information theory, as mentioned above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have some idea of what information is and how we can quantify it using binary numbers in bits, it's time to get into the math. Information can be calculated through the formula \n",
    "\n",
    "$$h(x) = - \\log_2(p(x))$$\n",
    "\n",
    "where $h(x)$ is the information need to express the random event $X = x$, and $p(x)$ is the probability that event $x$ occurs, *i.e.* $P(X = x)$. \n",
    "\n",
    "There are several observationns to make about this formula. First, why might we use \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...links...\n",
    "https://medium.com/activating-robotic-minds/demystifying-entropy-f2c3221e2550\n",
    "https://medium.com/udacity/shannon-entropy-information-gain-and-picking-balls-from-buckets-5810d35d54b4\n",
    "https://machinelearningmastery.com/what-is-information-entropy/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...ref...\n",
    "[Deep Learning]: http://www.deeplearningbook.org\n",
    "[bits]: https://en.wikipedia.org/wiki/Bit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
